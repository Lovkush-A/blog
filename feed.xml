<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://lovkush-a.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lovkush-a.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-06-25T15:18:46-05:00</updated><id>https://lovkush-a.github.io/blog/feed.xml</id><title type="html">Lovkush Agarwal</title><subtitle>A blog for my data science learning and projects</subtitle><entry><title type="html">Credit Card Fraud, Part VI, Summary and Lessons from Kaggle</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/06/25/creditcard6.html" rel="alternate" type="text/html" title="Credit Card Fraud, Part VI, Summary and Lessons from Kaggle" /><published>2020-06-25T00:00:00-05:00</published><updated>2020-06-25T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/06/25/creditcard6</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/06/25/creditcard6.html">&lt;h2 id=&quot;summaries&quot;&gt;Summaries&lt;/h2&gt;
&lt;h3 id=&quot;part-i&quot;&gt;Part I&lt;/h3&gt;
&lt;p&gt;In Part I, I described the framework and created the first set of models using default settings. I tried logistic regression, decision tree, random forest and xgboost models, and they respectively achieved an AUPRC of 0.616, 0.746, 0.842 and 0.856. Since then, I have learnt about more models and if I were to do this project again, I would also have included a support vector machine model and a k-nearest-neighbour model.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_1_logistic.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_tree.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_forest.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_xgb.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;part-ii&quot;&gt;Part II&lt;/h3&gt;
&lt;p&gt;In Part I, it look some time to fit the models, the forest model in particular, and I wanted to do some hyper-parameter optimisations. I wanted to find out if I could reduce the time taken to fit by removing non-fraudulent claims. The results of the experimentation showed that the time to fit was proportional to the size of the training data set, but the AUPRC did not take a massive hit.  This is good because it means I can do more hyper-parameter optimisations than before.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_2_forest_aucs.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_2_forest_times.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;part-iii&quot;&gt;Part III&lt;/h3&gt;
&lt;p&gt;Though the models were able to identify fraudulent transactions, I had gained no understanding. I tried creating a simple model: for each feature, determine whether the value is closer to the fraudulent mean or the non-fraudulent mean. This achieved an AUPRC of 0.682 and was able to identify about 70% of the frauduluent claims. This was satisfying, and better lets me appreciate what is gained by using more sophisticatd models.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_3_2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;part-iv&quot;&gt;Part IV&lt;/h3&gt;
&lt;p&gt;I started doing some hyper-parameter optimisations on the forest model, and noticed the AUPRC varied a lot between the different folds. I decided to investigate how the AUPRC can vary, to better appreciate what is gained by choosing one hyper-parameter over another. After doing this, I could confidently say that choosing 50 estimators is better than the default of 100 estimators.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est50_hist.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est50_scatter.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;part-v&quot;&gt;Part V&lt;/h3&gt;
&lt;p&gt;Here I actually carry out the hyper-parameter optimisations, and train the final models. The random forest’s AUPRC increased from 0.842 to 0.852, and the xgboost’s AUPRC increased from 0.856 to 0.872. Modest gains, and from the few articles I have read, this is to be expected.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_5_forest.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_5_xgb2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;lessons-learnt-from-kaggle&quot;&gt;Lessons learnt from Kaggle&lt;/h2&gt;
&lt;p&gt;I had a skim through the several most up-voted kernels on Kaggle. Below are the the things I found out by doing so. There is a lot for me to learn!&lt;/p&gt;

&lt;h3 id=&quot;auroc-versus-auprc&quot;&gt;AUROC versus AUPRC&lt;/h3&gt;
&lt;p&gt;Many of the examples (including the most upvoted &lt;a href=&quot;https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets/notebook&quot;&gt;example&lt;/a&gt;!) use AUROC instead of AUPRC. The main reason this surprised me is that the description of the dataset recommended using AUPRC; I suppose there was an advantage to not knowing much before hand! The second reason this surprised me is that AUPRC is a more informative measure than AUROC for unbalanced data. I try to explain why.&lt;/p&gt;

&lt;p&gt;The PRC and ROC are quite similar. They are both plots that visualise false positives against false negatives.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;False negatives are measured in the same way in both plots, namely, using recall/true positive rate. Recall tells you what percentage of truly fraudulent transactions the model successfully labels as fraudulent. (And so 1 - Recall measures how many false negatives we have, as a percentage of truly fraudulent claims.)&lt;/li&gt;
  &lt;li&gt;False positive are recorded differently in the two plots.
    &lt;ul&gt;
      &lt;li&gt;In PRC, precision is used. This is the percentage of transactions labelled as fraudulent that actually are fraudulent. Equivalently, 1-PRC is the number of false positives expressed as a percentage of &lt;em&gt;claims labelled as fraudulent&lt;/em&gt;.&lt;/li&gt;
      &lt;li&gt;In ROC, the false-positive rate is used. This is the number of false positives expressed as a percentage of &lt;em&gt;truly non-fraudulent transactions&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To make this more concrete, lets put some numbers to this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Imagine there are 100100 transactions altogether, 100 which are fraudulent and 100000 which are not.&lt;/li&gt;
  &lt;li&gt;Suppose a model predicts there are 200 fraudulent claims, and further suppose 50 of these were correct and 150 of these were incorrect.&lt;/li&gt;
  &lt;li&gt;For both PRC and ROC, the true positive measurement would be 50%: 50% of the fraudulent claims were found.&lt;/li&gt;
  &lt;li&gt;For PRC, the false positive measurement is 75%: 75% of the claims labelled as fraudulent were incorrectly labelled.&lt;/li&gt;
  &lt;li&gt;For ROC, the false positive measurement is 0.15%: only 0.15% of the non-fraudulent claims were incorrectly labelled as fraudulent.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In short, ROC is much more forgiving of false positives than PRC, when we have highly unbalanced data.&lt;/p&gt;

&lt;p&gt;I have also decided to plot PRC and ROC for a couple of the models in this series of posts, so you can visually see the difference. (Note that I have rotated the ROC curve to match up the variables with the PRC curve, so the comparison is easier.)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PRC and ROC for the final XGBoost model&lt;/strong&gt;
&lt;img src=&quot;/blog/images/creditcard_5_xgb2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_6_xgb_roc2.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;ROC makes the model look much better than PRC does. And it is deceiving: one might look at that second chart and say we can identify 90% of fraudulent claims without many false positives.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PRC and ROC for the handmade model&lt;/strong&gt;
&lt;img src=&quot;/blog/images/creditcard_3_2.png&quot; alt=&quot;&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_6_handmade.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Here, the effect is far more dramatic and very clearly shows how unfit AUROC is for unbalanced ata.&lt;/p&gt;

&lt;h3 id=&quot;under--and-over-sampling&quot;&gt;Under- and over-sampling&lt;/h3&gt;
&lt;p&gt;It turns out my idea from Part III, to remove non-fraudulent data, has a name: under-sampling. However, it sounds like there is an expectation that under-sampling could actually improve the performance of the models. This is surprising to me; unless you are systematially removing unrepresenative data, how can the model improve with less information?! A quick skim of the wikipedia article suggests I have not completely missed the point: ‘the reasons to use undersampling are mainly practical and related to resource costs’.&lt;/p&gt;

&lt;p&gt;Over-sampling looks like an interesting idea, in which you create new artificial data to pad out the under-represented class. Some people on Kaggle used SMOTE, where you take two nearby points, and introduce new points directly in between these two points. Something to keep in mind for future!&lt;/p&gt;

&lt;h3 id=&quot;removing-anomalous-data&quot;&gt;Removing anomalous data&lt;/h3&gt;
&lt;p&gt;A simple idea: try to find entries in the training data that are not representative and remove them to avoid skewing the models / to avoid over-fitting. Based on my limited understanding, I think tree-based models are not sensitive to extreme data (in the same way the median is not sensitive to extreme data), so this particular idea is unlikely to have helped me improve the models for this example. However, this is another tool I will keep in mind for future projects.&lt;/p&gt;

&lt;h3 id=&quot;dimensionality-reduction-and-clustering&quot;&gt;Dimensionality reduction and clustering&lt;/h3&gt;
&lt;p&gt;An interesting idea: try to find a mapping of the data into a smaller dimension that preserves the clusters. The algorithm somebody used was t-SNE which is explained in this &lt;a href=&quot;https://www.youtube.com/watch?v=NEaUSP4YerM&quot;&gt;YouTube video&lt;/a&gt;. A couple of other algorithms used were PCA and truncated SVD.  I do not yet understand how I could use this to improve the models (in the example, this was done to give a visual indication of whether frauduluent and non-frauduluent data could be distinguished).&lt;/p&gt;

&lt;h3 id=&quot;normalising-data&quot;&gt;Normalising data&lt;/h3&gt;
&lt;p&gt;Useful idea I should always keep in mind! Again, I don’t think this matters for tree-based models, but something I should keep in mind.&lt;/p&gt;

&lt;h3 id=&quot;outlier-detection-algorithms&quot;&gt;Outlier detection algorithms&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/pavansanagapati/anomaly-detection-credit-card-fraud-analysis/notebook&quot;&gt;One person&lt;/a&gt; used a bunch of (unsupervised?) learning algorithms: isolation forests, local outlier factor algorithm, SVM-based algorithms. More things for me to learn about!&lt;/p&gt;

&lt;h3 id=&quot;auto-encoders-and-latent-representation&quot;&gt;Auto-encoders and latent representation&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/shivamb/semi-supervised-classification-using-autoencoders&quot;&gt;This person&lt;/a&gt; used ‘semi-supervised learning’ via auto-encoders. This was particularly interesting, especially because they had a visual showing how their auto-encoder was better at separating fraudulent and non-fraudulent data than t-SNE. This is definitely something for me to delve deeper into some time, especially because of how visually striking it is.&lt;/p&gt;

&lt;h3 id=&quot;visualising-the-features&quot;&gt;Visualising the features&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/currie32/predicting-fraud-with-tensorflow/notebook&quot;&gt;Here&lt;/a&gt; and &lt;a href=&quot;https://www.kaggle.com/shelars1985/anomaly-detection-using-gaussian-distribution/notebook&quot;&gt;here&lt;/a&gt; are examples of a nice way of visualising the range of values of each feature for frauduluent and non-frauduluent data. The key thing is that they normalised the histograms, but I am not sure how they did that. Something for me to learn!&lt;/p&gt;

&lt;h3 id=&quot;gbm-vs-xgboost-vs-lightgbm&quot;&gt;GBM vs xgboost vs lightGBM&lt;/h3&gt;
&lt;p&gt;&lt;a href=&quot;https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm/notebook&quot;&gt;This kernel&lt;/a&gt; compared three algorithms. I quite liked this because it felt historical, and helps me appreciate how the community learns. The person compared the accuracy and time taken for each of the algorithms, and also describes some new settings and options they recently discovered.&lt;/p&gt;</content><author><name></name></author><summary type="html">Summaries Part I In Part I, I described the framework and created the first set of models using default settings. I tried logistic regression, decision tree, random forest and xgboost models, and they respectively achieved an AUPRC of 0.616, 0.746, 0.842 and 0.856. Since then, I have learnt about more models and if I were to do this project again, I would also have included a support vector machine model and a k-nearest-neighbour model.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lovkush-a.github.io/blog/images/creditcard_6_xgb_roc2.png" /><media:content medium="image" url="https://lovkush-a.github.io/blog/images/creditcard_6_xgb_roc2.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stop and Search, Part III, Data Analysis</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/06/22/sas3.html" rel="alternate" type="text/html" title="Stop and Search, Part III, Data Analysis" /><published>2020-06-22T00:00:00-05:00</published><updated>2020-06-22T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/06/22/sas3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/06/22/sas3.html">&lt;h2 id=&quot;total-number-of-stops-and-searches&quot;&gt;Total number of stops and searches&lt;/h2&gt;

&lt;h3 id=&quot;grouped-by-ethnicity&quot;&gt;Grouped by ethnicity&lt;/h3&gt;
&lt;p&gt;I start by plotting the total number of stops and searches (since May 2017 because that is the earliest data of the dataset), grouped by ethnicity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_sas_eth.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From this chart, a simplistic conlusion would be that white people are searched significantly more than other ethnicities, so there is no racism in the system. This is clearly bad reasoning, as we need to account for the underlying population.&lt;/p&gt;

&lt;h3 id=&quot;including-population&quot;&gt;Including population&lt;/h3&gt;
&lt;p&gt;Population data is taken from &lt;a href=&quot;https://www.ethnicity-facts-figures.service.gov.uk/uk-population-by-ethnicity/national-and-regional-populations/population-of-england-and-wales/latest#:~:text=the%20total%20population%20of%20England%20and%20Wales%20was%2056.1%20million,White%20ethnic%20group%20(4.4%25)&quot;&gt;here&lt;/a&gt;. I use this data to produce the following chart. Note that I grouped the various numbers together in the same way I grouped ethnicities together in producing the ethnicities column.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_pop_eth.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Now things look bad. There is clearly a discrepancy between the population and the number of stop and searches.&lt;/p&gt;

&lt;p&gt;To visualise this discrepancy more clearly, I decided to create a &lt;a href=&quot;http://www.sankey-diagrams.com/&quot;&gt;Sankey diagram&lt;/a&gt; using &lt;a href=&quot;https://plotly.com/python/sankey-diagram/&quot;&gt;Plotly&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_sankey_eth.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The diagram makes the discrepancy quite plain to see. Black people are stopped disproportionately more than other ethnic groups. There is evidently a big problem here.&lt;/p&gt;

&lt;p&gt;However, and unfortunately, this diagram does not tell us where exactly the problem is. Is the problem with the police or is there a deeper problem? Are the police racist for stopping black people more often, or, is this a reflection of crime rates and the underlying social issues?&lt;/p&gt;

&lt;p&gt;Some people would look at the above diagram and wonder how this is not conclusive evidence of police racism. To illustrate the idea, consider the following two charts:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_sankey_gen.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_sankey_age.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The majority of people would not look at these charts and conclude that the police are sexist or ageist, so one should not use the chart above for ethnicity to automatically conclude the police are racist.&lt;/p&gt;

&lt;p&gt;To try to shed some light on the question of racism, I will take into account the outcome of the stop-and-search.&lt;/p&gt;

&lt;h3 id=&quot;including-outcomes&quot;&gt;Including outcomes&lt;/h3&gt;
&lt;p&gt;The following stacked barchart shows the breakdown of outcomes for each ethnicity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas3_outcome.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is not at all what I was expecting. I was expecting to find that black people would have more false stop and searches than white people. It is shocking how consistent the ratio is across ethnicities - almost suspiciously so.  There is some discrepancy if you look closely, but dramatically less than what the Sankey diagram above suggested.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;My main goal for this was to gain some better understanding of crime data, and the process of cleaning and summarising data.  To my surprise, it seems from this simple analysis that police stop-and-search is not inherently racist, but there is a high chance I have not accounted for something or that my process is over-simplistic.  Of course, you should refer to more authoritative sources for conclusions on these complex issues, and not base your opinions on an amateur blog.&lt;/p&gt;

&lt;p&gt;Some key lessons I learnt:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I had to make some key decisions about how to group the data, namely, how to deal with discrepancy between officer and self defined ethnicity. In particular, it is not clear how one ought to group people of mixed race. Given how even the ratios were in the final chart, I don’t think this decision made a major difference, but it is something that I now know to consider when reading research in this area.&lt;/li&gt;
  &lt;li&gt;Dramatically different stories can be told depending on how the data is presented. This is something I already knew, but this is the first time I have experienced creating the charts for myself. With great power, comes great responsibility.&lt;/li&gt;
  &lt;li&gt;The quality of this analysis totally depends on the quality of the underlying data.
    &lt;ul&gt;
      &lt;li&gt;I did not mention this before, but there are gaps in the data: there are some police forces who do not provide the data for every month. This does not affect my simplistic analysis, but it would matter for more nuanced analyses.&lt;/li&gt;
      &lt;li&gt;The population data is from 2011, so there will be significant errors introduced by this mis-match between the datasets.&lt;/li&gt;
      &lt;li&gt;I have to, and do, trust that the data provided is accurate. It is scary to think how easily a government could skew the data, or simply withhold it. Going through this experience lets me better understand the dystopia in &lt;em&gt;1984&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;
&lt;p&gt;Here I provide sample of the code used to produce the charts.&lt;/p&gt;

&lt;p&gt;Below is the code to produce the first bar chart.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;colours_255&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;66&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;133&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;244&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;234&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;67&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;53&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;251&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;188&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;52&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;168&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;83&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;colours&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;tuple&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;255&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colour&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colour&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colours_255&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;figure&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;barplot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_ethnicity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_ethnicity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'White'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Asian'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Other'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;palette&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;colours&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;grid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'y'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Stop and Searches since May 2017, by Ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Number of Stop and Searches'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sas3_sas_eth.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the code to produce Sankey diagrams.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# create function that plots Sankey diagram given appropriate dataframe
&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;create_sankey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;go&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Figure&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;go&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Sankey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;node&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;pad&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;thickness&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;line&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;black&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;width&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;label&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Proportion of Population'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Proportion of Stop and Searches'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;color&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;blue&quot;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;),&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;dict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;source&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)),&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;target&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;list&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
      &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;))])&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;update_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title_text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;font_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;15&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fig&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;show&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# create dataframe containing population and stop and search data by ethnicity
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'population'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;population&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'sas'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_ethnicity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_ethnicity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'White'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Asian'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Other'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;


&lt;span class=&quot;c1&quot;&gt;# create sankey diagram
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;create_sankey&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_pop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Stop and Searches by Ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Here is the code to produce the stacked barcharts at the end:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# group data by ethnicity and outcome. 
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'outcome'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;outcome&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;count&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'outcome'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'frequency'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;},&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# convert frequencies into percentages
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frequency&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sum&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'total'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ethnicity&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;lambda&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_total&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;eth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'percentage'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;frequency&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;/&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;total&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;100&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# pivot table, and re-order the rows
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_new&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pivot_table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_eth_out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'percentage'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'outcome'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;sas_new&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sas_new&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'White'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Black'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Asian'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Other'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt;

&lt;span class=&quot;c1&quot;&gt;# plot the graph
&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sas_new&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;plot&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bar&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stacked&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;xlabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Ethnicity'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ylabel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Percent of Stop and Searches'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'Breakdown of Outcomes of Stop and Searches'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;legend&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;labels&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'False / no further action'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;'Minor further action'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                    &lt;span class=&quot;s&quot;&gt;'Major further action'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'center left'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;bbox_to_anchor&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mf&quot;&gt;0.5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tight_layout&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;savefig&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'sas3_outcome.png'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Total number of stops and searches</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://lovkush-a.github.io/blog/images/sas3_outcome.png" /><media:content medium="image" url="https://lovkush-a.github.io/blog/images/sas3_outcome.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Stop and Search, Part II, Data Cleaning</title><link href="https://lovkush-a.github.io/blog/2020/06/17/sas2.html" rel="alternate" type="text/html" title="Stop and Search, Part II, Data Cleaning" /><published>2020-06-17T00:00:00-05:00</published><updated>2020-06-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/06/17/sas2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/06/17/sas2.html">&lt;p&gt;In this post, I describe the cleaning I did on the data.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/python/data%20science/2020/06/22/sas3.html&quot;&gt;Stop and Search, Part III, Data Analysis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/06/15/sas1.html&quot;&gt;Stop and Search, Part I, Data Collection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-cleaning&quot;&gt;The cleaning&lt;/h2&gt;
&lt;p&gt;I cleaned each column, one by one. Note I call the original frame &lt;code class=&quot;highlighter-rouge&quot;&gt;sas&lt;/code&gt; and created a copy &lt;code class=&quot;highlighter-rouge&quot;&gt;sas_clean&lt;/code&gt; in which I would do the cleaning.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;To see a list of all the columns, I ran the code &lt;code class=&quot;highlighter-rouge&quot;&gt;sas.columns&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;To investigate the distribution of values in a column (before and after cleaning), I would use the code &lt;code class=&quot;highlighter-rouge&quot;&gt;sas_clean.column.value_counts(dropna = False)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Some columns required no cleaning:
    &lt;ul&gt;
      &lt;li&gt;age_range&lt;/li&gt;
      &lt;li&gt;gender&lt;/li&gt;
      &lt;li&gt;location.latitude and location.longitude (except I renamed thse columns)&lt;/li&gt;
      &lt;li&gt;force&lt;/li&gt;
      &lt;li&gt;month&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The other columns did require some cleaning:
    &lt;ul&gt;
      &lt;li&gt;self_defined_ethnicity and officer_defined_ethnicity&lt;/li&gt;
      &lt;li&gt;type&lt;/li&gt;
      &lt;li&gt;outcome&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;ethnicity&quot;&gt;Ethnicity&lt;/h2&gt;
&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;officer_defined_ethnicity&lt;/code&gt; was mostly clean. The distribution of values were:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;White    608092
Black    259504
Asian    139531
NaN       91601
Other     31845
Mixed      2563
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The only change I made was to combine mixed with other.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;self_defined_ethnicity&lt;/code&gt; was less clean, and the distribution of values were:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;White - English/Welsh/Scottish/Northern Irish/British                                   475167
Other ethnic group - Not stated                                                         154802
White - Any other White background                                                       85322
Black/African/Caribbean/Black British - Any other Black/African/Caribbean background     78340
Black/African/Caribbean/Black British - African                                          66072
Black/African/Caribbean/Black British - Caribbean                                        49736
Asian/Asian British - Any other Asian background                                         44517
NaN                                                                                      41149
Asian/Asian British - Pakistani                                                          33907
Asian/Asian British - Bangladeshi                                                        24128
Other ethnic group - Any other ethnic group                                              16449
Mixed/Multiple ethnic groups - Any other Mixed/Multiple ethnic background                15560
Asian/Asian British - Indian                                                             14929
Mixed/Multiple ethnic groups - White and Black Caribbean                                 14063
White - Irish                                                                             7843
Mixed/Multiple ethnic groups - White and Black African                                    4246
Mixed/Multiple ethnic groups - White and Asian                                            3598
White - Gypsy or Irish Traveller                                                          1689
Asian/Asian British - Chinese                                                             1476
Other ethnic group - Arab                                                                  143
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I decided to group these up according to the same categories used in &lt;code class=&quot;highlighter-rouge&quot;&gt;officer_defined_ethnicity&lt;/code&gt;. This was done using &lt;code class=&quot;highlighter-rouge&quot;&gt;.replace&lt;/code&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def simplify_eth(ethnicity):
    if isinstance(ethnicity, float) or 'Not stated' in ethnicity:
        return np.nan
    elif 'Other' in ethnicity or 'Mixed' in ethnicity:
        return 'Other'
    elif 'Asian' in ethnicity:
        return 'Asian'
    elif 'Black' in ethnicity:
        return 'Black'
    elif 'White' in ethnicity:
        return 'White'

ethnicities = {eth: simplify_eth(eth) for eth in sas.self_defined_ethnicity.unique()}
sas_clean = sas_clean.replace(to_replace = ethnicities)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Finally, I wanted to create a column &lt;code class=&quot;highlighter-rouge&quot;&gt;ethnicity&lt;/code&gt; that combines these two columns. I started by renaming the other two columns, creating the new column, and filling it in with values where there is no disagreement between the officer defined and self defined ethnicity.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sas_clean.rename(columns = {'self_defined_ethnicity': 'self',
                            'officer_defined_ethnicity': 'officer'}, inplace = True)
sas_clean['ethnicity'] = np.nan

# if officer and self agree, set ethnicity to either.
indices = (sas_clean.officer == sas_clean.self)
sas_clean.loc[indices, 'ethnicity'] = sas_clean.officer[indices]

# if officer is null, set ethnicity to self, and vice versa
indices = (sas_clean.officer.isnull())
sas_clean.loc[indices, 'ethnicity'] = sas_clean.self[indices]

indices = (sas_clean.self.isnull())
sas_clean.loc[indices, 'ethnicity'] = sas_clean.officer[indices]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
I created a column &lt;code class=&quot;highlighter-rouge&quot;&gt;conflicted&lt;/code&gt; to list all the cases where the stated ethnicity differs:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sas_clean['conflicted'] = np.nan
indices = (sas_clean.officer != sas_clean.self) &amp;amp; (sas_clean.officer.notna()) &amp;amp; (sas_clean.self.notna())
sas_clean.loc[indices, 'conflicted'] = sas_clean.officer[indices] + '_' + sas_clean.self[indices]
sas_clean.conflicted.value_counts()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The output was:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Black_Other      18774
White_Other      12423
Asian_Other       6240
Other_Asian       5319
Other_White       4243
Black_White       2924
Asian_White       2394
White_Asian       2027
Black_Asian       1990
White_Black       1935
Other_Black       1764
Asian_Black       1577
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
To decide how to deal with this, I went back into the original &lt;code class=&quot;highlighter-rouge&quot;&gt;self_defined_ethnicity&lt;/code&gt; to determine what the appropriate label ought to be.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i in sas_clean.conflicted.unique():
    print(i)
    indices = (sas_clean.conflicted == i)
    print(sas.loc[indices, 'self_defined_ethnicity'].value_counts())
    print()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A sample of the output is:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Black_Other
Mixed/Multiple ethnic groups - White and Black Caribbean                     9158
Mixed/Multiple ethnic groups - Any other Mixed/Multiple ethnic background    4891
Mixed/Multiple ethnic groups - White and Black African                       2689
Other ethnic group - Any other ethnic group                                  1835
Mixed/Multiple ethnic groups - White and Asian                                194

White_Black
Black/African/Caribbean/Black British - Any other Black/African/Caribbean background    815
Black/African/Caribbean/Black British - African                                         633
Black/African/Caribbean/Black British - Caribbean                                       487

Other_Black
Black/African/Caribbean/Black British - African                                         819
Black/African/Caribbean/Black British - Any other Black/African/Caribbean background    750
Black/African/Caribbean/Black British - Caribbean                                       195
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Deciding how to deal with these cases was the trickiest part of the cleaning. First, this is a sensitive issue and it feels wrong for me to decide how people should be labelled. Second, there is clearly no ‘right’ answer here, and I have to use my judgement.  In the end, for most cases, I chose the self_defined_ethnicity.  However, the two big exceptions were when the officer identified the person as Black or White but the person identified themselves as mixed. There were 30000 such cases. If I added them to the ‘Other’ category, this would grossly skew the numbers and misrepresent the situation, so I decided to assign these Black and White (respectively).  Different people will make different judgements on this, and I suppose this is one way our own biases can creep into the data analysis.&lt;/p&gt;

&lt;p&gt;In the end, the distribution of ethnicities in this new column is:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;White    645261
Black    269286
Asian    143468
NaN       40383
Other     34738
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;type&quot;&gt;Type&lt;/h2&gt;
&lt;p&gt;There are 3 types of stop-and-search:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Person search                861870
Person and Vehicle search    246976
Vehicle search                24290
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Theoretically, a vehicle search does not involve any people, and thus should not have any ethnicity attached to it. However, a quick query shows this is not the case:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sas_clean.loc[(sas_clean.type == 'Vehicle search'), 'ethnicity'].value_counts(dropna = False)

NaN      20762
White     1796
Black      843
Asian      619
Other      270
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This gives some indication of how much inherent noise there is in the data.  Given the numbers are relatively small, I did not worry about ignoring these entries and so just removed all Vehicle search entries.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sas_clean = sas_clean[sas_clean.type != 'Vehicle search']
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;outcome&quot;&gt;Outcome&lt;/h2&gt;
&lt;p&gt;Last, I cleaned the outcome column. The distribution of values were:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;A no further action disposal                                    675585
Arrest                                                          126538
False                                                           115566
Community resolution                                             50906
Suspect arrested                                                 30616
Khat or Cannabis warning                                         26428
NaN                                                              22090
Summons / charged by post                                        16366
Penalty Notice for Disorder                                      13617
Offender given drugs possession warning                          12717
Local resolution                                                  4709
Caution (simple or conditional)                                   4520
Suspect summonsed to court                                        2941
Offender given penalty notice                                     2802
Article found - Detailed outcome unavailable                      2651
Offender cautioned                                                 778
Suspected psychoactive substances seized - No further action        16
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I decided to replace these with a numerical value, where 0 represents that the stop-and-search discovered nothing inappropriate, 1 represents a minor infringement with minimal action and 2 represents a major infringement with significant action.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;replacements = {'A no further action disposal': 0,
               'Arrest': 2,
               'False': 0,
               'Community resolution': 1,
               'Suspect arrested': 2,
               'Khat or Cannabis warning': 1,
               'Summons / charged by post': 2,
               'Penalty Notice for Disorder': 2,
               'Offender given drugs possession warning': 1,
               'Local resolution': 1,
               'Caution (simple or conditional)': 1,
                'Suspect summonsed to court': 2,
                'Offender given penalty notice': 2,
                'Article found - Detailed outcome unavailable': 1,
                'Offender cautioned': 1,
                'Suspected psychoactive substances seized - No further action': 1
               }

sas_clean['outcome'] = sas_clean.outcome.replace(to_replace = replacements)
sas_clean.outcome.value_counts(dropna = False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The final distribution of values for outcomes is as follows:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0.0    791151
2.0    192880
1.0    102725
NaN     22090
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is surprising the the majority of stop-and-searches amount to nothing. It makes me wonder what the reasons for this are, and if there is a more efficient means of detecting the actual crimes with fewer false positives.&lt;/p&gt;

&lt;h2 id=&quot;conclusion-and-thoughts&quot;&gt;Conclusion and thoughts&lt;/h2&gt;
&lt;p&gt;That is end of the cleaning and tomorrow I will try to illustrate the patterns in the data with appropriate charts.&lt;/p&gt;

&lt;p&gt;The two main lessons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Data is inherently noisy, and one should not treat data as objective truth. (Though, it is the closest we have got!)&lt;/li&gt;
  &lt;li&gt;A data scientist has significant power to adjust the story, by grouping and cleaning the data differently. It seems that good practice is to be open about how you processed the data and to check how different choices affect the final results.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">In this post, I describe the cleaning I did on the data.</summary></entry><entry><title type="html">Do students do their homework last minute?</title><link href="https://lovkush-a.github.io/blog/2020/06/16/homework.html" rel="alternate" type="text/html" title="Do students do their homework last minute?" /><published>2020-06-16T00:00:00-05:00</published><updated>2020-06-16T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/06/16/homework</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/06/16/homework.html">&lt;p&gt;During my previous job teaching mathematics at the University of Leicester, I did a project to investigate whether students did their homework last minute, which involved some data processing in R.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the STEM Foundation Year at the University of Leicester, we used the e-assessement system Numbas. This system recorded enough information for me to be able to investigate when students did their homework.&lt;/p&gt;

&lt;p&gt;To help understand the charts that will come below, it will help to know how the assessment was structured.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There were two semesters.
    &lt;ul&gt;
      &lt;li&gt;In Semester 1, we taught Physics 1, Physics 2 and Maths 1.&lt;/li&gt;
      &lt;li&gt;In Semester 2, we taught Physics 3, Physics 4 and Maths 2.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The physics modules were structured as follows:
    &lt;ul&gt;
      &lt;li&gt;Each module lasted half a semester.&lt;/li&gt;
      &lt;li&gt;Each module had 4 weekly e-assessments, made available on Monday and had a deadline of 10am on the Monday after.&lt;/li&gt;
      &lt;li&gt;Various other assessments whose details do not matter.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The maths modules were structured as follows:
    &lt;ul&gt;
      &lt;li&gt;Each module lasted a whole semester.&lt;/li&gt;
      &lt;li&gt;Each module had 8 weekly e-assessments, made available each Monday.&lt;/li&gt;
      &lt;li&gt;For Maths 1, there was a single deadline which was the weekend before exams.&lt;/li&gt;
      &lt;li&gt;For Maths 2, there were weekly deadlines like in Physics.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The reason for the inconsistent structure is because we were trying to find out what works.&lt;/p&gt;

&lt;h2 id=&quot;collecting-and-cleaning-the-data&quot;&gt;Collecting and cleaning the data&lt;/h2&gt;
&lt;p&gt;As mentioned above, we used the Numbas e-assessment system which recorded lots of data, including when students attempted their work.  A colleague was in charge of maintaining the system, so I asked them to extract the data for this little project. They did this using a SQL query, and passed the data to me as a csv file.&lt;/p&gt;

&lt;p&gt;I then cleaned the data. This involved:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Removing entries that did not correspond to my students.&lt;/li&gt;
  &lt;li&gt;Determining which assessment each entry corresponded to. This was trickier than expected, because the staff on our team used different naming conventions and because the system produced three different ids for each assessment.&lt;/li&gt;
  &lt;li&gt;Deciding how to deal with fact that we allowed students to attempt the assessment multiple times. In the end, I decided to pick the first attempt out of all these; it had negligible impact on the final charts.&lt;/li&gt;
  &lt;li&gt;Deciding how to deal with fact that a student could start an assessment on one day, but finish it later. I decided to pick the time a student first opened an assessment, which I called ‘Start Time’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;On the technical side, I used R.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I learnt some R by working through &lt;a href=&quot;https://r4ds.had.co.nz/&quot;&gt;R for Data Science&lt;/a&gt;, which is an excellent online book that I highly recommended.&lt;/li&gt;
  &lt;li&gt;For this project, the key tools I used were &lt;a href=&quot;https://r4ds.had.co.nz/tibbles.html&quot;&gt;tibbles&lt;/a&gt;, &lt;a href=&quot;https://r4ds.had.co.nz/pipes.html&quot;&gt;piping&lt;/a&gt; and &lt;a href=&quot;https://ggplot2.tidyverse.org/&quot;&gt;ggplot2&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;The other noteworthy technical aspect of this project was getting the x-axis, representing time, to appear just as I wanted. I remember this took significant effort, banging my head over the table to understand POSIXct and POSIClt.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-charts&quot;&gt;The charts&lt;/h2&gt;
&lt;p&gt;Below are the charts for the Physics modules. The x-axis shows the day a student opened an e-assessment and the y-axis shows the number of student who started on each day. The different colours correspond to the different assessments.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/homework1.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/homework2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Physics 1, 2 and 4 all have the same patterns.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;A good chunk of students open the e-assessment before the weekend.&lt;/li&gt;
  &lt;li&gt;The modal day to open the e-assessment is Sunday, the day before the deadline.&lt;/li&gt;
  &lt;li&gt;Several students open the e-assessment on Monday (so after midnight on Sunday).&lt;/li&gt;
  &lt;li&gt;The bars are shorter in the Physics 2 and Physics 4 charts because fewer students do those modules.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Physics 3 has a different pattern. The first assessment has the same shape as in the other three modules. The other three assessments are flat for a few weeks and then all bunch up in the week beginning Monday 11th Feb. The reason is that at the end of the first week of Physics 3, we extended the deadline for all the assessments to 10am on Monday 18th Feb. (We did this to account for unforeseen circumstances).&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Below are a sample of charts showing the breakdown of timings during Sunday and Monday.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/homework3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I do not think there is anything particularly noteworthy in these charts. The main pattern is that most people who started the work on Sunday did so after 6pm. The thing which struck me was that for each assessment, there were several students who started the work between 3am and 9am.&lt;/p&gt;

&lt;p&gt;As a result of this data, the director of the Foundation Year decided to change the deadlines from 10am on Monday to 10pm on Sunday.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Below are the charts for the two maths modules.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/homework4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Recall that in Maths 1, there was a single deadline for all the assessments, which was the weekend before exam week.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the first half of the semester, there is a decent chunk of students starting the e-assessments.&lt;/li&gt;
  &lt;li&gt;In the second half, engagement drops significantly. My explanation for this is that the e-assessments for Physics 2 were considerably longer/harder than those of Physics 1, but there are likely various factors.&lt;/li&gt;
  &lt;li&gt;A lot of work was done over the Christmas break. To my surprise, a few students left all the work to be done on the final weekend!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Recall that Maths 2 had weekly deadlines. Recall also that Maths 2 runs concurrently with Physics 3 and Physics 4.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;When we extended the deadline in Physics 3, we also had to do it for Maths 2.&lt;/li&gt;
  &lt;li&gt;Like in Physics 4, the deadlines for second half of Maths 2 were weekly.&lt;/li&gt;
  &lt;li&gt;Hence, the first half of Maths 2 resembles Physics 3, and the second half of Maths 2 resembles Physics 4.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Many people who see this will say ‘This is obvious, what is the point?’.  There are two main points.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, it is good to have quantitative data. It provides clearer understanding and also allows us to measure changes from one year to the next.&lt;/li&gt;
  &lt;li&gt;Second, the higher education industry puts too little weight on (appropriate) data and observations. Either a lecturer simply does not care about teaching (in which case they put no weight on anything) or a lecture does care but bases their decisions on an imagined conception of what students are.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What conclusions did I draw from this?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The pattern for weekly deadlines is consistent across the year: there is some activity throughout the week, with a clear peak the day before the deadline.
    &lt;ul&gt;
      &lt;li&gt;One consequence is that we cannot assume comfort with material taught on Monday during a session later in the week, e.g., on Thursday.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Students respond to incentives, just like the rest of us.
    &lt;ul&gt;
      &lt;li&gt;Our choices have a big impact on student habits.&lt;/li&gt;
      &lt;li&gt;Noteworthy to point out that most students do know the deadlines! This means we are communicating our deadlines well.&lt;/li&gt;
      &lt;li&gt;Thinking about incentives is important more generally. E.g. it explains the difference between attendance in lectures and attendance in assessed sessions.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;These findings are particularly important for ‘linear’ subjects, where knowledge/understanding of Week 1 material is required to learn Week 2 material.&lt;/li&gt;
  &lt;li&gt;Shouldn’t judge students or label them as ‘bad students’.
    &lt;ul&gt;
      &lt;li&gt;Better to label the habit, not the individual.&lt;/li&gt;
      &lt;li&gt;This is more to do with human nature, than students in particular.&lt;/li&gt;
      &lt;li&gt;This is mostly about incentives. Designing a course well includes creating incentives which result in good learning behaviours. (Compare with the famous example of opting-in or opting-out of a country’s organ donation registry.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;limitations-of-the-data&quot;&gt;Limitations of the data&lt;/h2&gt;
&lt;p&gt;There are several sources of noise and error in this data. I will say ‘data is positively biased’ to mean that data shows students working earlier than they actually are, and ‘negatively biased’ to say that data shows students are working later than they actually are.&lt;/p&gt;

&lt;p&gt;Sources of positive bias.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Looking at Start Time. Students may open the assessment during the week,
but actually only finish it on the weekend.&lt;/li&gt;
  &lt;li&gt;Students have multiple attempts on the coursework and I only looked at the start time of their earliest attempt.&lt;/li&gt;
  &lt;li&gt;I excluded students who did not attempt the coursework or attempted it late.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sources of negative bias.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There was a ‘Practice Version’ of each e-assessment available. Students were encouraged to use these to practice before attempting the actual assessed version. Some students did this, but a brief look at the data shows that most people did not attempt these.&lt;/li&gt;
  &lt;li&gt;Did not take into account mitigating circumstances, e.g. illness.&lt;/li&gt;
  &lt;li&gt;Does not account for other forms of independent study. E.g. a student might review lectures/workshop questions before attempting the e-assessment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Sources of unknown bias.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Most of our students have done A-Level Maths and/or Physics, so find the year easy. This probably means that students do not need to attempt coursework in a timely manner in order to keep up with the material.&lt;/li&gt;
  &lt;li&gt;This data only relates to specific style of coursework. There is no data on semester long projects, essays, etc. My prediction is that similar patterns will emerge, but spread out according to the size of the task.&lt;/li&gt;
  &lt;li&gt;Several students suspended or withdrew or were terminated during year. Their data will be included in early modules but not in later modules.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">During my previous job teaching mathematics at the University of Leicester, I did a project to investigate whether students did their homework last minute, which involved some data processing in R.</summary></entry><entry><title type="html">Stop and Search, Part I, Data Collection</title><link href="https://lovkush-a.github.io/blog/2020/06/15/sas1.html" rel="alternate" type="text/html" title="Stop and Search, Part I, Data Collection" /><published>2020-06-15T00:00:00-05:00</published><updated>2020-06-15T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/06/15/sas1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/06/15/sas1.html">&lt;p&gt;In light of the current prominence of BlackLivesMatter, I decided to investigate crime in relation to race. Here I describe how I collected the data I will be analysing.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/python/data%20science/2020/06/22/sas3.html&quot;&gt;Stop and Search, Part III, Data Analysis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/06/17/sas2.html&quot;&gt;Stop and Search, Part II, Data Cleaning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In both traditional and social media, the issue of racial discrimination within the police is a hot topic. I decided to investigate this issue and better understand the statistics that go around.&lt;/p&gt;

&lt;p&gt;I googled ‘crime data’ and one of the top results was &lt;a href=&quot;data.police.uk&quot;&gt;data.police.uk&lt;/a&gt;, which seems like a reliable source of data for crime in the UK. With regard to race, the only data available on this website is about ‘stop-and-search’ (as opposed to prison data, for example).&lt;/p&gt;

&lt;h2 id=&quot;stop-and-search&quot;&gt;Stop and search&lt;/h2&gt;
&lt;p&gt;In the UK, a police officer has the legal authority to stop and search you if they have ‘reasonable grounds’ to suspect you’re involved in a crime, e.g. carrying an illegal item.  This &lt;a href=&quot;https://www.gov.uk/police-powers-to-stop-and-search-your-rights&quot;&gt;UK Government website&lt;/a&gt; provides a short and clear summary of the rules, this &lt;a href=&quot;https://www.gov.scot/publications/guide-stop-search-scotland/&quot;&gt;Scottish Government website&lt;/a&gt; also provides clear summary of the rules but with more detail on what counts as reasonable and how a search should be conducted, and finally &lt;a href=&quot;http://www.legislation.gov.uk/ukpga/1984/60/part/I&quot;&gt;here&lt;/a&gt; is the actual legislation, which is predictably written in unclear legalese.&lt;/p&gt;

&lt;h2 id=&quot;downloading-the-data&quot;&gt;Downloading the data&lt;/h2&gt;
&lt;p&gt;I will only describe the final and clean code used to obtain the information I wanted, after several attempts necessary to get everything correct.&lt;/p&gt;

&lt;p&gt;First, I downloaded a JSON file listing the name and ‘id’ of each police force, stored it in a pandas dataframe, and saved it as a csv file. The id is just a shortened version of their name and is used in all the other data sources.  The code to do this is:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;forces_response = requests.get('https://data.police.uk/api/forces')
forces_json = forces_response.json()

force_df = pd.DataFrame({'id':[], 'name': []})

for entry in forces_json:
    force_df.loc[force_df.shape[0]] = [entry['id'], entry['name']]

force_df.to_csv('force.csv')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Next I downloaded a JSON file describing for which months and for which forces stop-and-search data was available:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;availability_response = requests.get('https://data.police.uk/api/crimes-street-dates')
availability_json = availability_response.json()

availability_df = pd.DataFrame({'month':[], 'id': []})

for entry in availability_json:
    date = pd.to_datetime(entry['date'], format='%Y-%m').to_period('M')
    for id in entry['stop-and-search']:
        availability_df.loc[availability_df.shape[0]] = [date, id]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
I then loop through this information and download the stop-and-search data, saving the data onto my laptop.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;for i in range(availability_df.shape[0]):
    force = availability_df.iloc[i].id
    month = availability_df.iloc[i].month.strftime('%Y-%m')
    response = requests.get(f&quot;https://data.police.uk/api/stops-force?force={force}&amp;amp;date={month}&quot;)
    if response.status_code == 200:
        data = response.json()
        with open(f'{month}_{force}.json', 'w') as f:
            json.dump(data, f)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
I add a column to the availability dataframe to track which pieces of data were actually successfully downloaded or not. I do this by trying to open each file, and recording a fail if an error occurs while trying to open it. (While writing this paragraph, I realise I could have done this at the same time as the previous step.)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;availability_df['downloaded'] = True

for i in range(availability_df.shape[0]):
    force = availability_df.iloc[i].id
    month = availability_df.iloc[i].month.strftime('%Y-%m')
    try:
        file = open(f'{month}_{force}.json', 'r')
        file.close()
    except:
        availability_df.iloc[i,2] = False
        print(f'{month}_{force}')

availability_df.to_csv('availability.csv')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;
Lastly, I combine all of the data into one mega pandas dataframe, keeping only those columns that I think will be relevant to my investigations.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;cols = ['age_range', 'outcome', 'self_defined_ethnicity',
       'gender', 'officer_defined_ethnicity', 'type',
       'location.latitude', 'location.longitude', 'force', 'month']

sas_df = pd.DataFrame({col:[] for col in cols})

for i in range(availability_df.shape[0]):
    if availability_df.iloc[i,2]:
        force = availability_df.iloc[i].id
        month = availability_df.iloc[i].month
        month_str = month.strftime('%Y-%m')
        
        file = open(f'{month_str}_{force}.json', 'r')
        data = json.load(file)
        new = pd.json_normalize(data)
        new['force'] = force
        new['month'] = month
        
        sas_df = sas_df.append(new, ignore_index=True)[cols]

sas_df.to_csv('sas.csv')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;a-chart&quot;&gt;A chart&lt;/h2&gt;
&lt;p&gt;It would be sad for this post to have no charts whatsoever, so I quickly created one which just counts the number of stops-and-searches, grouped by ethnicity.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/sas1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;One might say, ‘Look, white people are stopped more than black people, so the police are not racist.’ This is obviously simplistic. The aim of the project is to dig deeper into the data and see what patterns I can find.&lt;/p&gt;</content><author><name></name></author><summary type="html">In light of the current prominence of BlackLivesMatter, I decided to investigate crime in relation to race. Here I describe how I collected the data I will be analysing.</summary></entry><entry><title type="html">AIs for Games, Part III</title><link href="https://lovkush-a.github.io/blog/2020/06/09/games3.html" rel="alternate" type="text/html" title="AIs for Games, Part III" /><published>2020-06-09T00:00:00-05:00</published><updated>2020-06-09T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/06/09/games3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/06/09/games3.html">&lt;p&gt;I describe the various ways I made the algorithm from Part II more efficient. These resulted in big improvements in the efficiency.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/06/04/games2.html&quot;&gt;AIs for Games, Part II&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/20/games1.html&quot;&gt;AIs for Games, Part I&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;performance-before-any-optimisations&quot;&gt;Performance before any optimisations&lt;/h2&gt;
&lt;p&gt;I created a few boards and timed how long it took the algorithm to run a depth-2 search on these boards, starting with both Player 0 and Player 1. An image showing the boards used is at the end of the post.  The results are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 1.59&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 1.58&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.00117&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.00103&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 5.04&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 4.94&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 29.2&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 32.0&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 9.55&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 9.25&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also tried running a depth-3 search on Board 4 for Player 1 (because Player 1 can find a winning move with this depth), but it did not finish running even after a couple of hours of running.&lt;/p&gt;

&lt;h2 id=&quot;the-optimisations&quot;&gt;The optimisations&lt;/h2&gt;
&lt;p&gt;I made several optimisations. I describe them here chronologically, i.e., in the order I tried implementing them.&lt;/p&gt;

&lt;h3 id=&quot;alpha-beta-pruning&quot;&gt;Alpha-beta pruning&lt;/h3&gt;
&lt;p&gt;There are situations where a board position does not need to be analysed, because based on the boards we have already analysed, this board position will definitely not be chosen in optimal player. This resulted in the following times:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 1.34&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 1.39&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.00120&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.00104&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 5.28&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 4.61&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 26.1&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 29.4&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 8.62&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 8.28&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This was not a significant improvement in the times. I was surprised by this. I decided to run cProfile to try to determine why there was not a significant time-save. It seemed to be that the process of creating new boards was taking up a lot of time - and I was only pruning a board after the board was created. I needed to prune before the board was created.&lt;/p&gt;

&lt;p&gt;To achieve this, I had to significantly re-structure the whole program, removing the frontier and instead writing the main function &lt;code class=&quot;highlighter-rouge&quot;&gt;find_move&lt;/code&gt; recursively. The resulting times were a significant improvement:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 1.07&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 1.06&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.00118&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.00102&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 1.90&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 1.86&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 5.33&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 5.41&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 2.98&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 2.90&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Phew! It was satisfying to see the times drop so much, and this motivated me to keep going.&lt;/p&gt;

&lt;h3 id=&quot;stop-if-winning-move-found&quot;&gt;Stop if winning move found&lt;/h3&gt;
&lt;p&gt;If a winning move was found in a current board position, there is no need to continue analysing this position, so can stop this early. Pruning does not detect this (at least, not how I implemented it. This could be a sign I did it wrong…), so I had to manually add this. This resulted in further improvements:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 1.09&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 1.11&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.00104&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.00119&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 0.324&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 0.344&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 0.141&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 0.897&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 3.12&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 2.93&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I also tried running the depth-3 search on board 4, and to my surprise it ended in under a minute!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 4, Depth 3, Time taken: 59.1&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note this would not be representative of a generic depth-3 search, because the winning move is found about 1/6 of the way into the full search.&lt;/p&gt;

&lt;h3 id=&quot;lists-within-lists&quot;&gt;Lists within lists&lt;/h3&gt;
&lt;p&gt;Running cProfile revealed that having nested lists to encode the board slows things down considerably, so I re-wrote the program so that the board was represented by a single list. I was worried this would take a lot of effort, but fortunately it consisted of replacing &lt;code class=&quot;highlighter-rouge&quot;&gt;board[i][j]&lt;/code&gt; with &lt;code class=&quot;highlighter-rouge&quot;&gt;board[6*i + j]&lt;/code&gt;, and other similar simple changes.  This halved the times:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 0.407&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 0.411&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.000520&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.000450&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 0.175&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 0.161&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 0.0645&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 0.424&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 1.41&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 1.41&lt;/li&gt;
  &lt;li&gt;Board: 4, Depth 3, Time taken: 30.7&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;checking-if-the-game-is-over&quot;&gt;Checking if the game is over&lt;/h3&gt;
&lt;p&gt;Running cProfile again showed that the new bottle-neck was checking if the game had ended. This involved looping through the set of all possible winning lines, and checking to see if Player 0 or Player 1 occupied all the positions in each line.&lt;/p&gt;

&lt;p&gt;Originally, I had one for-loop to check if Player 1 won, then another to check if Player 0 won. I changed this to have a single loop, and for each line check if Player 1 or Player 0 won. This resulted in another big chunk of time-saving.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 0.292&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 0.283&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.000499&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.000334&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 0.101&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 0.107&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 0.0359&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 0.261&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 0.943&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 0.915&lt;/li&gt;
  &lt;li&gt;Board: 4, Depth 3, Time taken: 21.6&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Checking cProfile showed that I had halved the time to check if the game had ended, but it was still the biggest bottle neck.&lt;/p&gt;

&lt;p&gt;I then tried to re-design the program to cut down further, but to no avail. For example, I tried to group the set of lines into groups that could be ruled out together, e.g. if I know that position &lt;code class=&quot;highlighter-rouge&quot;&gt;(2,2)&lt;/code&gt; in the board is empty, that rules out 7 of the lines. It will be interesting to know if there is a more efficient way to check if the game has ended!&lt;/p&gt;

&lt;h3 id=&quot;tidying-up-and-fixing-a-bug&quot;&gt;Tidying up and fixing a “bug”&lt;/h3&gt;
&lt;p&gt;My code was becoming untidy (I was not using version control properly, and instead was creating multiple versions of functions in the same file) so I tidied up all the code. While doing this, I discovered that I did not correctly update the prune function during the ‘Lists within lists’ step: I was only pruning boards of at least depth 2, when it could be pruning board of depth 1.  I made the necessary tweak, resulting in the following times:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 0.0896&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 0.0682&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.000272&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.000204&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 0.100&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 0.0539&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 0.313&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 1.13&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 0.204&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 0.310&lt;/li&gt;
  &lt;li&gt;Board: 4, Depth 3, Time taken: 3.18&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Woohoo! What big progress. What used to take hours now only takes 3 seconds.&lt;/p&gt;

&lt;h3 id=&quot;duplicate-board-positions&quot;&gt;Duplicate board positions&lt;/h3&gt;
&lt;p&gt;The last thing I wanted to try was dealing with repeat positions. Previously I only skipped these if the same position occurred and they had same parent. But now I wanted to have a way of skipping board positions regardless of where they were in the game-tree. This took many hours to get correct, because my first attempt caused the algorithm to produce sub-optimal moves, and I had no idea why.&lt;/p&gt;

&lt;p&gt;The error was that when I pruned a board, I would finalise the board’s value, though the board was not fully analysed. Then, when the board occurred somewhere else in the tree, I would use this incomplete value and miss out all the analysis that was pruned the first time around.&lt;/p&gt;

&lt;p&gt;After fixing the bug, the new times are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Board: 0, Player: 0, Time taken: 0.2888009548187256&lt;/li&gt;
  &lt;li&gt;Board: 0, Player: 1, Time taken: 0.15045881271362305&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 0, Time taken: 0.00028705596923828125&lt;/li&gt;
  &lt;li&gt;Board: 1, Player: 1, Time taken: 0.0002300739288330078&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 0, Time taken: 0.0822603702545166&lt;/li&gt;
  &lt;li&gt;Board: 2, Player: 1, Time taken: 0.03888416290283203&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 0, Time taken: 0.30017614364624023&lt;/li&gt;
  &lt;li&gt;Board: 3, Player: 1, Time taken: 0.7593698501586914&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 0, Time taken: 0.3503570556640625&lt;/li&gt;
  &lt;li&gt;Board: 4, Player: 1, Time taken: 0.3380570411682129&lt;/li&gt;
  &lt;li&gt;Board: 4, Depth 3, Time taken: 3.19&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The times are not always better, and some are worse.&lt;/p&gt;

&lt;h2 id=&quot;next-steps-and-final-thoughts&quot;&gt;Next steps and final thoughts&lt;/h2&gt;
&lt;p&gt;The next step is to introduce neural networks. A brief google search reveals that min-max is not appropriate and that I should have been using reinforcement learning. Doh! In the back of my mind, I was curious as to how the neural network could learn the heuristic function; what would the loss/error be that it would minimise?&lt;/p&gt;

&lt;p&gt;Though the optimisation of the min-max algorithm is incomplete (e.g. I do not understand why the latest version is not faster than the previous version), I will end it here. This is because I have already spent a couple of days on this, I have already learnt from this, and it is not necessary for the bigger goal of developing a neural network.&lt;/p&gt;

&lt;p&gt;Some final takeaways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;I should have sketched out a plan of the whole project. Though I had basic knowledge of neural networks, I should have researched a bit more and found out that min-max is not appropriate for neural networks.&lt;/li&gt;
  &lt;li&gt;Be more thorough with testing. It makes spotting bugs easier and quicker.&lt;/li&gt;
  &lt;li&gt;Seed random number generators. I used used random heuristics (to see effects of pruning), but I did not seed them. This means the times above are not fair comparisons, as some random numbers could have lead to more pruning than others.&lt;/li&gt;
  &lt;li&gt;Use proper version control. My code got hideous at one point. At least now I have a better sense of the workflow of git.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code, at this stage of project, can be found on &lt;a href=&quot;https://github.com/Lovkush-A/games/tree/94649838b4e348772a12dbf13c5954b665354371&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;the-boards-used-for-testing&quot;&gt;The boards used for testing&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/games_31.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;</content><author><name></name></author><summary type="html">I describe the various ways I made the algorithm from Part II more efficient. These resulted in big improvements in the efficiency.</summary></entry><entry><title type="html">AIs for Games, Part II</title><link href="https://lovkush-a.github.io/blog/2020/06/04/games2.html" rel="alternate" type="text/html" title="AIs for Games, Part II" /><published>2020-06-04T00:00:00-05:00</published><updated>2020-06-04T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/06/04/games2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/06/04/games2.html">&lt;p&gt;Here I created an algorithm and the code to search through the game-tree to a given maximum depth. This has been written for the game Pentago specifically. The code seems to work, but is highly inefficient.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/06/09/games3.html&quot;&gt;AIs for Games, Part III&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/20/games1.html&quot;&gt;AIs for Games, Part I&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-algorithm&quot;&gt;The algorithm&lt;/h2&gt;
&lt;p&gt;In my last post, I said I wanted to code up an alpha-beta pruning algorithm. (See this &lt;a href=&quot;https://youtu.be/WbzNRTTrX0g&quot;&gt;CS50 Intro to AI&lt;/a&gt; lecture for background on tree-based search and alpha-beta pruning). Over the past couple of weeks, I have been thinking about exactly how the algorithm would work and how I would code it up, and it was surprisingly tricky. I therefore decided to just focus on creating an algorithm that would search through a game-tree up to some maximum depth, but in a way that I could add in the pruning.&lt;/p&gt;

&lt;p&gt;The algorithm should determine the ‘value’ of the current board state and the move that would achieve that value. A value of 1 means that Player 1 will win (with perfect play) and a value of 0 means that Player 0 will win (with perfect play). A value in the middle indicates which player is more likely to win, as judged by the algorithm.&lt;/p&gt;

&lt;p&gt;The general idea of the algorithm is straightforward:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Given a board position, create an initial node and add it to the frontier.&lt;/li&gt;
  &lt;li&gt;While the initial node does not have a value, pick a node from the frontier and do the following:
    &lt;ul&gt;
      &lt;li&gt;Check if the game has ended. If so, determine who won, and set the value of the node appropriately. Then update the value of parent nodes appropriately.&lt;/li&gt;
      &lt;li&gt;Check if the depth of the node is the maximum depth. If so, then estimate the value of the position. For now, I just set this as 0.5, but in future, this will be determined via a neural network. Then update the value of parent nodes appropriately.&lt;/li&gt;
      &lt;li&gt;Create a list of legal moves and possible board positions arising from this node. Create new nodes and add them to the frontier.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Once the initial node has a value, pick a move whose resulting board position has the same value.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The tricky part was the step ‘update the value of the parent nodes appropriately’. It took me some time to flesh out all the details and determine exactly when a parent node should have its value updated. I had to do this in a way so that I could add on the pruning later without having to change the structure of the code. The main ideas were:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Whenever a node has its value determined, the upper or lower bounds of its parent’s node, and only its parent’s node, needs to be updated.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Whenever all of a node’s children’s values are determined, the node’s value can be determined. This will sometimes lead to some recursive updating of node values.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;A big sticking point for me was how to decide when to prune a node: it felt like I needed knowledge of uncle/aunt nodes to do this, but following the ideas above, the grandparent node and parent node should contain enough information to decide if a node can be pruned or not.&lt;/p&gt;

&lt;p&gt;In the end, I managed to get it altogether. The code, at this stage of the project, can be found on &lt;a href=&quot;https://github.com/Lovkush-A/games/tree/e8a8e0afe6d25030d82cf0f0269c5edab625e463&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;an-example&quot;&gt;An example&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/games_21.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows an example winning position for player 1. If it is Player 1’s move, the algorithm finds a winning move using a depth-1 search (play in bottom left, and then rotate bottom left clockwise, giving 5-in-a-row column on left-hand-side).  If it is Player 2’s move, the algorithm returns &lt;code class=&quot;highlighter-rouge&quot;&gt;None&lt;/code&gt; using a depth-2 search, because no matter what 2 does on this turn, 1 will always win.&lt;/p&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code, at this stage of project, can be found on &lt;a href=&quot;https://github.com/Lovkush-A/games/tree/e8a8e0afe6d25030d82cf0f0269c5edab625e463&quot;&gt;github&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;areas-of-improvement&quot;&gt;Areas of improvement&lt;/h2&gt;
&lt;p&gt;The algorithm is highly inefficient. It takes roughly 10-50 seconds to do depth-2 searches, and on the order of hours for depth-3 search. This is way too long! The number of possible positions after 3 moves is roughly a few million, so that shouldn’t take hours to sort through.&lt;/p&gt;

&lt;p&gt;There are many inefficiencies I am aware of and will fix them for my next post. Examples include:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Dealing with repeat positions. Right now, I only avoid positions that repeat if they arise from the same parent.&lt;/li&gt;
  &lt;li&gt;I currently use a list to track which positions have been visited in the search, to check for repeats. This is less efficient than a set, but I can’t use arrays in sets and all my boards are coded as arrays. I will try changing everything to tuples.&lt;/li&gt;
  &lt;li&gt;If a winning/losing sequence is found, it will keep on searching. This is not necessarily a bad thing, because we might want to analyse all the lines, but it definitely slows things down.&lt;/li&gt;
  &lt;li&gt;Not using alpha-beta pruning, yet.&lt;/li&gt;
  &lt;li&gt;Not doing any time analysis. I will run cProfile to systematically find inefficiencies.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Here I created an algorithm and the code to search through the game-tree to a given maximum depth. This has been written for the game Pentago specifically. The code seems to work, but is highly inefficient.</summary></entry><entry><title type="html">Investigating Credit Card Fraud, Part V</title><link href="https://lovkush-a.github.io/blog/2020/05/30/creditcard5.html" rel="alternate" type="text/html" title="Investigating Credit Card Fraud, Part V" /><published>2020-05-30T00:00:00-05:00</published><updated>2020-05-30T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/05/30/creditcard5</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/05/30/creditcard5.html">&lt;p&gt;I complete the hyper-parameter optimisations for the random forest and xgboost models. I then create a final model using these values to produce AUCs of 0.852 and 0.872.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/python/data%20science/2020/06/25/creditcard6.html&quot;&gt;Credit Card Fraud, Part VI, Summary and Lessons from Kaggle&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/29/creditcard4.html&quot;&gt;Investigating Credit Card Fraud, Part IV&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/19/creditcard3.html&quot;&gt;Investigating Credit Card Fraud, Part III&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/16/creditcard2.html&quot;&gt;Investigating Credit Card Fraud, Part II&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/14/creditcard1.html&quot;&gt;Investigating Credit Card Fraud, Part I&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forest-model-hyper-parameter-selection&quot;&gt;Forest model, hyper-parameter selection&lt;/h2&gt;
&lt;p&gt;I tidied up the code from yesterday to allow me to optimise for more than one parameter at once. For each combination of hyper-parameters, I obtained 20 different AUCs (by using five 4-fold cross validations). The results were stored in a pandas dataframe. The code for this is at the bottom of the page.&lt;/p&gt;

&lt;p&gt;I then averaged over all the folds and sorted the results. The code for this and the output is below.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;auc_forest.max_depth.fillna(value = 0, inplace = True)
auc_forest_mean = auc_forest.groupby(['n_estimators', 'max_depth', 'max_features']).auc.mean()
auc_forest_mean.sort_values(ascending = False).head(20)

n_estimators  max_depth  max_features    auc
50.0          0.0        10.0            0.774015
              50.0       10.0            0.774015
60.0          0.0        10.0            0.772589
              50.0       10.0            0.772589
              10.0       10.0            0.772573
50.0          10.0       10.0            0.772328
40.0          10.0       10.0            0.771290
80.0          0.0        10.0            0.771108
              50.0       10.0            0.771108
40.0          0.0        10.0            0.770744
              50.0       10.0            0.770744
50.0          0.0        7.0             0.770522
              50.0       7.0             0.770522
80.0          10.0       10.0            0.770487
50.0          10.0       7.0             0.770472
60.0          50.0       7.0             0.770472
              0.0        7.0             0.770472
              10.0       7.0             0.770025
40.0          50.0       5.0             0.769278
                         auto            0.769278
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;A few things were found by doing this:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The best options for the hyper-parameters are n_estimators = 50, max_depth = None and max_features = 10.&lt;/li&gt;
  &lt;li&gt;max_depth = None and max_depth = 50 produced the same models. This means that maximum depth achieved without any limits is less than 50.&lt;/li&gt;
  &lt;li&gt;max_features = auto and max_features = 5 produced the same models. This is obvious in retrospect: auto means taking the square root of the number of features, and we had about 30 features.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;forest-model-final-model&quot;&gt;Forest model, final model&lt;/h2&gt;
&lt;p&gt;Using these hyper-parameters, I created a the final Random Forest model. The precision-recall curve is below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_5_forest.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_forest.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;For comparison, the very first random forest model is also included. As can be seen, there is an improvement but a seemingly minimal one. Based on examples I have seen elsewhere, these minor improvements are what can be expected from hyper-parameter optimisations.&lt;/p&gt;

&lt;h2 id=&quot;xgboost-model&quot;&gt;XGBoost model&lt;/h2&gt;
&lt;p&gt;I repeated the process above for XGBoost models.  The best parameter settings were as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_estimators  max_depth  learning_rate    auc
50.0          5.0        0.05             0.761125
100.0         5.0        0.02             0.760002
50.0          10.0       0.05             0.759094
              15.0       0.05             0.758146
100.0         10.0       0.02             0.757185
              15.0       0.02             0.756748
200.0         10.0       0.02             0.747032
              15.0       0.02             0.743830
50.0          15.0       0.10             0.742954
              10.0       0.10             0.739922
100.0         10.0       0.05             0.737840
              15.0       0.05             0.737013
50.0          10.0       0.02             0.729299
              15.0       0.02             0.729239
              5.0        0.02             0.729049
200.0         5.0        0.02             0.727433
50.0          15.0       0.30             0.726696
              5.0        0.20             0.726479
100.0         5.0        0.20             0.724851
              15.0       0.30             0.722728
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Using the settings from the top row, I created my final model, whose precision-recall curve is below.  I have included the original curve, too.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_5_xgb.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_xgb.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;!! After doing the optimisations, the model became worse! The AUC decreased by 0.002. The  explanation for this must be that removing 99% of the data actually changes the behaviour of the model.&lt;/p&gt;

&lt;p&gt;I re-did the process but only removing 90% of the data (recall from Part II that in XGBoost, removing 90% of the data did not decrease performance that much). This time, the optimal settings were as follows:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_estimators  max_depth  learning_rate    auc
200.0         10.0       0.10             0.816130
              5.0        0.10             0.815648
100.0         5.0        0.10             0.807745
              10.0       0.10             0.806940
200.0         10.0       0.05             0.805212
              5.0        0.05             0.801478
50.0          10.0       0.10             0.797015
              5.0        0.10             0.794567
100.0         5.0        0.05             0.793189
              10.0       0.05             0.792732
200.0         5.0        0.02             0.785652
              10.0       0.02             0.783957
50.0          5.0        0.05             0.779087
              10.0       0.05             0.778968
100.0         5.0        0.02             0.776565
              10.0       0.02             0.775092
50.0          5.0        0.02             0.761190
              10.0       0.02             0.760388
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;The optimal parameters changed (thankfully!).  I then re-created the final model and this time there was an improvement:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_5_xgb2.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_1_xgb.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;next-time&quot;&gt;Next time&lt;/h2&gt;
&lt;p&gt;My next blog post will be the final one in this series. I will summarise what I have done and what I have learnt. I will also have a look at what others did and see what I can learn from them.&lt;/p&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code is provided for the Random Forest optimisation. The code for XGBoost is similar.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;modules&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;model_selection&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision_recall_curve&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metrics&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;matplotlib&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pyplot&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;plt&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;seaborn&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sns&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sklearn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ensemble&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;xgboost&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;XGBClassifier&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itertools&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;creditcard.csv&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Class&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;drop&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'Class'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Time'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;versus&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;which&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;takes&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;returns&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
   
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision_recall_curve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;options&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hyperparameter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;40&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;60&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'auto'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;7&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;frame&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;auc_forest&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'n_estimators'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
                           &lt;span class=&quot;s1&quot;&gt;'max_depth'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
                           &lt;span class=&quot;s1&quot;&gt;'max_features'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
                           &lt;span class=&quot;s1&quot;&gt;'fold'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[],&lt;/span&gt;
                           &lt;span class=&quot;s1&quot;&gt;'auc'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
                          &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;


&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;through&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hyper&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;parameter&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;space&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rs&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;itertools&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;product&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;max_depth&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;max_features&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fraudulent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;claims&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fitting&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;auc_forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auc_forest&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;md&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;*&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">I complete the hyper-parameter optimisations for the random forest and xgboost models. I then create a final model using these values to produce AUCs of 0.852 and 0.872.</summary></entry><entry><title type="html">Investigating Credit Card Fraud, Part IV</title><link href="https://lovkush-a.github.io/blog/2020/05/29/creditcard4.html" rel="alternate" type="text/html" title="Investigating Credit Card Fraud, Part IV" /><published>2020-05-29T00:00:00-05:00</published><updated>2020-05-29T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/05/29/creditcard4</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/05/29/creditcard4.html">&lt;p&gt;I begin the hyper-parameter optimisations. I started by looking at &lt;code class=&quot;highlighter-rouge&quot;&gt;n_estimators&lt;/code&gt; in the Random Forest model. The first attempt to determine which value was the best produced a surprising range of performances, so I delved further to better understand how the performance can depend on the folds.&lt;/p&gt;

&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/python/data%20science/2020/06/25/creditcard6.html&quot;&gt;Credit Card Fraud, Part VI, Summary and Lessons from Kaggle&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/30/creditcard5.html&quot;&gt;Investigating Credit Card Fraud, Part V&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/19/creditcard3.html&quot;&gt;Investigating Credit Card Fraud, Part III&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/16/creditcard2.html&quot;&gt;Investigating Credit Card Fraud, Part II&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/2020/05/14/creditcard1.html&quot;&gt;Investigating Credit Card Fraud, Part I&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;first-attempt&quot;&gt;First attempt&lt;/h2&gt;
&lt;p&gt;I used a k-fold cross validation with 4 folds to determine what is a good number of estimators for the Random Forest model. The code to do this is at the bottom. The table below shows the AUC metrics obtained.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Fold&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_estimators = 10&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_estimators = 50&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_estimators = 100&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_estimators = 200&lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;n_estimators = 500&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.765&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.793&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.775&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.770&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.756&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.683&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.690&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.691&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.680&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.664&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.766&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.783&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.781&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.784&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.774&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;3&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.815&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.841&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.838&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.833&lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;0.826&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;From this table, we can see that the AUC depends a lot more on the fold rather than the hyper-parameter. I was surprised at how much the AUC could vary, depending on how the data was chopped up. Nevertheless, it is still clear that the optimal choice for the number of estimators is either 50 or 100. However, it is hard to judge if 50 is definitely better than the default of 100; it is better in 3 out of the 4 folds but maybe this was just a fluke.&lt;/p&gt;

&lt;p&gt;I wanted to better understand how the AUC depends on the folds, and make a better decision about which hyper-parameter is better, so I decided to repeat this process many times and see the resulting patterns.&lt;/p&gt;

&lt;h2 id=&quot;second-attempt&quot;&gt;Second attempt&lt;/h2&gt;
&lt;p&gt;I repeated the first attempt 20 times and stored the results in a pandas dataframe. I then plotted scatterplots and histograms to visualise the patterns.  In each of them, I compared the performance against the default of 100 estimators. As always, the code for this is at the bottom.&lt;/p&gt;

&lt;h3 id=&quot;n_estimators10&quot;&gt;n_estimators=10&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est10_hist.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est10_scatter.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram shows that the distribution of AUC values when the number of estimators is 10 is worse than the default values.  The scatterplot shows the default setting has a better AUC on the majority of folds - but not every time!&lt;/p&gt;

&lt;h3 id=&quot;n_estimators50&quot;&gt;n_estimators=50&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est50_hist.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est50_scatter.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histograms almost perfectly overlap! But we do see a little extra blue on the right and extra orange on the left which means n=50 is better.  The scatterplot makes this clearer, showing that having 50 estimators produces larger AUC in most of the folds.&lt;/p&gt;

&lt;h3 id=&quot;n_estimators200-and-n_estimators500&quot;&gt;n_estimators=200 and n_estimators=500&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est200_hist.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est200_scatter.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est500_hist.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/creditcard_4_forest_n_est500_scatter.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;From these charts, we see that as we increase the number of estimators beyond 100, the model performs worse. Though we can see this in the table in the first attempt, these charts make it much clearer.&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;
&lt;p&gt;Visualisations are nice! Though the first k-fold validation gave the same conclusions as twenty k-fold validations, the latter is far more convincing and enlightening. In addition to being more certain that n=50 is a superior choice, I have gained knowledge about how much the AUC can vary as the data varies.&lt;/p&gt;

&lt;p&gt;Furthermore, the idea of removing data to speed up the fitting (from Part II of the series) really paid off. Generating these charts required 320 fittings altogether. Without removing the data, this would have taken multiple days, so I would never have done it.&lt;/p&gt;

&lt;p&gt;Next time, I will complete the hyper-parameter optimisations and present my final models.&lt;/p&gt;

&lt;h2 id=&quot;the-code-for-first-attempt&quot;&gt;The code for first attempt&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;versus&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;X_test&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;y_test&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_test_split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;X&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;test_size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0.2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;object&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
           &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;determine&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;given&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict_proba&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision_recall_curve&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;recall&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;precision&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForest&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variable&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;loop&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;through&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hyper&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;parameter&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;values&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;and&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;folds&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimator&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                   &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fraudulent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;claims&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fitting&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;code-for-second-attempt&quot;&gt;Code for second attempt&lt;/h2&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;list&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForest&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;50&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;200&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;500&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;variables&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;store&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;5&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;auc_df&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'n_estimators_'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]):&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;
                       &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))})&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;create&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;20&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;different&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFolds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;so&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;we&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;models&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;each&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;value&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;hyperparameter&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;KFold&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_splits&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;shuffle&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
               &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimator&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;RandomForestClassifier&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;n_estimators&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;n_estimator&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                                       &lt;span class=&quot;n&quot;&gt;random_state&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

        &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;kf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xtv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ytv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;iloc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;valid&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

            &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;remove&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;99&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;the&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;non&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fraudulent&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;claims&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;training&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;to&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;speed&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;up&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fitting&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;%&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;100&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;|&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;selection&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                
            &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Xv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yt_reduced&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;yv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_current&lt;/span&gt;
            &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;+=&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;

    &lt;span class=&quot;p&quot;&gt;#&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;update&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dataframe&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;auc_df&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;latest&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;batch&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;of&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt;    
    &lt;span class=&quot;n&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;j&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;auc_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;loc&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;auc_df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]]&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;aucs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[:,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;j&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">I begin the hyper-parameter optimisations. I started by looking at n_estimators in the Random Forest model. The first attempt to determine which value was the best produced a surprising range of performances, so I delved further to better understand how the performance can depend on the folds.</summary></entry><entry><title type="html">Bacon numbers via Recursive SQL</title><link href="https://lovkush-a.github.io/blog/2020/05/24/recursion_sql.html" rel="alternate" type="text/html" title="Bacon numbers via Recursive SQL" /><published>2020-05-24T00:00:00-05:00</published><updated>2020-05-24T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/2020/05/24/recursion_sql</id><content type="html" xml:base="https://lovkush-a.github.io/blog/2020/05/24/recursion_sql.html">&lt;p&gt;I create a table of actors and their &lt;a href=&quot;https://en.wikipedia.org/wiki/Six_Degrees_of_Kevin_Bacon#Bacon_numbers&quot;&gt;bacon numbers&lt;/a&gt; using recursion in SQL. The largest finite bacon number is 13.&lt;/p&gt;

&lt;h2 id=&quot;sql-in-python&quot;&gt;SQL in Python&lt;/h2&gt;
&lt;p&gt;I ran the SQL commands in Python using the module sqlite3. I used &lt;a href=&quot;https://swcarpentry.github.io/sql-novice-survey/10-prog/index.html&quot;&gt;this article&lt;/a&gt; to help me set it all up. Below is the code to run queries on the database &lt;em&gt;movies.db&lt;/em&gt; in python.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    import sqlite3

    def run_sql(query):
        connection = sqlite3.connect('movies.db')
        cursor = connection.cursor()
        cursor.execute(query)
        results = cursor.fetchall()
        cursor.close()
        connection.close()
    
    return results
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;understanding-the-database&quot;&gt;Understanding the database&lt;/h2&gt;
&lt;p&gt;The data is in the form of a database, &lt;em&gt;movies.db&lt;/em&gt;, and was obtained from the online course &lt;a href=&quot;https://cs50.harvard.edu/x/2020/psets/7/movies/&quot;&gt;CS50&lt;/a&gt;, which in turn was obtained from imdb. To find out the structure of the tables in the database, I ran the following code (obtained from &lt;a href=&quot;https://stackoverflow.com/questions/305378/list-of-tables-db-schema-dump-etc-using-the-python-sqlite3-api&quot;&gt;stackoverflow&lt;/a&gt;).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    def produce_schema():
        sql = &quot;SELECT name FROM sqlite_master WHERE type = 'table';&quot;
        tables = run_sql(sql)
    
        schema = {}
        for table in tables:
            sql = f&quot;SELECT sql FROM sqlite_master WHERE type = 'table' and name = '{table[0]}'&quot;
            results = run_sql(sql)
            print(results[0][0])
            schema[table[0]] = results[0][0]
    
        return schema

    produce_schema()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;For the task of determining bacon numbers, the relevant tables and columns are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;people&lt;/code&gt;, with columns &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;name&lt;/code&gt;. Each person has a unique id. There are 1044499 people in the table.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stars&lt;/code&gt;, with columns &lt;code class=&quot;highlighter-rouge&quot;&gt;movie_id&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;person_id&lt;/code&gt;. Each row tells you that the actor with id &lt;code class=&quot;highlighter-rouge&quot;&gt;person_id&lt;/code&gt; starred in the movie with id &lt;code class=&quot;highlighter-rouge&quot;&gt;movie_id&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kevin Bacon has an id of 102, found by running the query &lt;code class=&quot;highlighter-rouge&quot;&gt;SELECT id FROM people WHERE name = 'Kevin Bacon' &lt;/code&gt;&lt;/p&gt;

&lt;h2 id=&quot;the-recursive-query&quot;&gt;The recursive query&lt;/h2&gt;
&lt;p&gt;To understand recursion in SQL, I recommend &lt;a href=&quot;https://www.essentialsql.com/recursive-ctes-explained/&quot;&gt;this guide&lt;/a&gt; which explains recursion and how to use recursion in SQL, and then the &lt;a href=&quot;https://www.sqlite.org/lang_with.html&quot;&gt;SQLite documentation&lt;/a&gt; to better understand some implementation details and what options are available to you.&lt;/p&gt;

&lt;p&gt;The query I created to produce the table of bacon numbers is:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    WITH RECURSIVE
     costars(id1, id2) AS
     ( 
       SELECT stars1.person_id, stars2.person_id
       FROM stars AS stars1
       JOIN stars AS stars2
       ON stars1.movie_id = stars2.movie_id
     ),
     bacon(id, num) AS
     (
       VALUES(102, 0)
       UNION
       SELECT id2, num+1
       FROM bacon JOIN costars
       ON bacon.id = costars.id1
       WHERE num &amp;lt; 13
     )
     SELECT bacon.id, name, MIN(num)
     FROM bacon JOIN people ON bacon.id = people.id
     GROUP BY bacon.id
     ORDER BY num
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;The table &lt;code class=&quot;highlighter-rouge&quot;&gt;costars&lt;/code&gt; lists all pairs of actors that co-starred in a movie. This is obtained by doing a self-join of the stars table.&lt;/li&gt;
  &lt;li&gt;The table &lt;code class=&quot;highlighter-rouge&quot;&gt;bacon&lt;/code&gt; is the main table. &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt; is the id of the actor and &lt;code class=&quot;highlighter-rouge&quot;&gt;num&lt;/code&gt; is the bacon number.
    &lt;ul&gt;
      &lt;li&gt;It starts off with the entry &lt;code class=&quot;highlighter-rouge&quot;&gt;(102,0)&lt;/code&gt;.  102 is Kevin Bacon’s id and 0 is Kevin Bacon’s Bacon number.&lt;/li&gt;
      &lt;li&gt;Then for any entry &lt;code class=&quot;highlighter-rouge&quot;&gt;(id, num)&lt;/code&gt; already in the table, we add a new row &lt;code class=&quot;highlighter-rouge&quot;&gt;(id2, num+1)&lt;/code&gt;, whenever &lt;code class=&quot;highlighter-rouge&quot;&gt;id2&lt;/code&gt; co-starred with &lt;code class=&quot;highlighter-rouge&quot;&gt;id&lt;/code&gt;.&lt;/li&gt;
      &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;num &amp;lt; 13&lt;/code&gt; indicates we will only have a maximum bacon number of 13. Without this manual limit, the recursive query would never end. This is because the underlying data is not acyclic: e.g. if a is a co-star with b, then b is a co-star of a. The number 13 was chosen via trial-and-error. The resulting table does not change if I increase the limit further, which implies that the maximum bacon number is 13.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;In the end, I select the relevant data from this recursive construction. Because each actor can appear many times in this construction, I use &lt;code class=&quot;highlighter-rouge&quot;&gt;GROUP BY&lt;/code&gt; to ensure each actor appears only once. I use &lt;code class=&quot;highlighter-rouge&quot;&gt;MIN(num)&lt;/code&gt; to select each actor’s earliest appearance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The two main problems with this query are that:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is inefficient. There is huge redundancy as actors appear many times in the recursive construction. I do not think there is a way of avoiding this within SQL.&lt;/li&gt;
  &lt;li&gt;I have to know what the maximum Bacon number is for the query to produce a complete list. I found this using trial-and-error.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bacon-number-of-13&quot;&gt;Bacon number of 13&lt;/h2&gt;
&lt;p&gt;By running a simple query, I find there are two people with the maximum bacon number of 13, Javier Ordonez and Kimberly Peters.  Using the program created for &lt;a href=&quot;https://cs50.harvard.edu/ai/2020/projects/0/degrees/&quot;&gt;this CS50 AI&lt;/a&gt; project, I could find the path from these actors to Kevin Bacon. As expected, it takes 13 steps (always satisfying to see two different programs being consistent!) and they are below. Note they have the same path.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Javier Ordonez/Kimberly Peters and Amanda Brass starred in PRND&lt;/li&gt;
  &lt;li&gt;Amanda Brass and Michael Bayouth starred in Park Reverse Neutral Drive, PRND (Director’s cut)&lt;/li&gt;
  &lt;li&gt;Michael Bayouth and Brandy Bourdeaux starred in Grease Trek&lt;/li&gt;
  &lt;li&gt;Brandy Bourdeaux and Kim Beuché starred in Murder Inside of Me&lt;/li&gt;
  &lt;li&gt;Kim Beuché and Ed Baccari starred in Island, Alicia&lt;/li&gt;
  &lt;li&gt;Ed Baccari and Aida Angotti starred in Late Watch&lt;/li&gt;
  &lt;li&gt;Aida Angotti and Lamont Copeland starred in Bottom Out&lt;/li&gt;
  &lt;li&gt;Lamont Copeland and Ashley Marie Arnold starred in Eye Was Blind&lt;/li&gt;
  &lt;li&gt;Ashley Marie Arnold and Sid Bernstein starred in The Rodnees: We Mod Like Dat!&lt;/li&gt;
  &lt;li&gt;Sid Bernstein and Chuck Berry starred in The Beatles: The Lost Concert&lt;/li&gt;
  &lt;li&gt;Chuck Berry and Eric Clapton starred in Chuck Berry Hail! Hail! Rock ‘n’ Roll&lt;/li&gt;
  &lt;li&gt;Eric Clapton and Tom Cruise starred in Close Up&lt;/li&gt;
  &lt;li&gt;Tom Cruise and Kevin Bacon starred in A Few Good Men&lt;/li&gt;
&lt;/ol&gt;</content><author><name></name></author><summary type="html">I create a table of actors and their bacon numbers using recursion in SQL. The largest finite bacon number is 13.</summary></entry></feed>