<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.1">Jekyll</generator><link href="https://lovkush-a.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lovkush-a.github.io/blog/" rel="alternate" type="text/html" /><updated>2022-10-29T02:33:40-05:00</updated><id>https://lovkush-a.github.io/blog/feed.xml</id><title type="html">Lovkush Agarwal</title><subtitle>A blog for my data science learning and projects</subtitle><entry><title type="html">Using data to improve professional squash rankings</title><link href="https://lovkush-a.github.io/blog/data%20science/2021/10/12/squash_elo.html" rel="alternate" type="text/html" title="Using data to improve professional squash rankings" /><published>2021-10-12T00:00:00-05:00</published><updated>2021-10-12T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/2021/10/12/squash_elo</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/2021/10/12/squash_elo.html"><![CDATA[<h2 id="project-overview">Project Overview</h2>
<p>Using the ELO rating system and past few decades of squash match data, I created ratings that are superior to the current simplistic ratings used by the Professional Squash Association (PSA). Code for this project is available in the <a href="https://github.com/Lovkush-A/squash_elo">GitHub repository for this project</a>.</p>

<h2 id="problem-statement">Problem Statement</h2>
<p>There are two big problems in the current way squash ratings are determined by PSA.</p>

<ol>
  <li>
    <p>A player gets rating points depending on which tournment they play in and which round they reach in the tournament. There is zero weight put on who you actually play against. E.g. it ought to be more impressive to beat the world’s best player in Round 1, then to reach a quarter-final by beating low ranked players (because you had a lucky draw).</p>
  </li>
  <li>
    <p>The current rating points are not meaningful. It is revealing that ratings point are never mentioned in squash broadcasts.</p>
  </li>
</ol>

<p>The proposed strategy is to use the ELO rating system.
The underlying ideas behind ELO are straightforward:</p>
<ul>
  <li>Use the difference between ratings of players to compute an expected score for an individual match.</li>
  <li>Changes in ratings after an individual match are proportional to the difference between the true score and the expected score.</li>
</ul>

<p>The full details are as follows:</p>

<ul>
  <li>If two players have ratings <code class="highlighter-rouge">r1</code> and <code class="highlighter-rouge">r2</code> respectively, then the predicted odds of player 1 beating player 2 is <code class="highlighter-rouge">p(r1, r2) = 1 / (1 + 10**((r2 - r1) / 400))</code>
    <ul>
      <li>Note there is nothing fundamentally important about the numbers 10 or 400 here. These are simply the numbers used in chess, the most famous use case of the ELO rating system.</li>
      <li>And yes, this is just (a scaled version) of the sigmoid function!</li>
    </ul>
  </li>
  <li>If Player 1 with rating <code class="highlighter-rouge">r1</code> beats Player 2 with rating <code class="highlighter-rouge">r2</code>, then the ratings are updated as follows:
    <ul>
      <li>The change in rating is <code class="highlighter-rouge">delta = K*(1 - p(r1, r2))</code>.
        <ul>
          <li>The variable <code class="highlighter-rouge">K</code> is a hyper-parameter. The larger <code class="highlighter-rouge">K</code> is, the bigger the changes in ratings will be.</li>
          <li>The quantity <code class="highlighter-rouge">1-p(r1, r2)</code> is the difference between the true score of 1 (representing victory) and predicted score of <code class="highlighter-rouge">p(r1, r2)</code>.</li>
        </ul>
      </li>
      <li>Player 1’s new rating is <code class="highlighter-rouge">r1+delta</code></li>
      <li>Player 2’s new rating is <code class="highlighter-rouge">r2-delta</code></li>
    </ul>
  </li>
  <li>If a player has not yet played any matches, we assign them a default starting rating of 1500.
    <ul>
      <li>This is arbitrary, since ELO ratings only depend on differences in ratings.</li>
    </ul>
  </li>
</ul>

<p>And that’s it!</p>

<p>By construction, ELO gaurantees to fix problem 1 because the ratings are completely based on who you beat, rather than which round of a tournament you reach.</p>

<p>However, the second problem is not guaranteed to be fixed by ELO, and will require empirical verification. This leads on to the metrics used to evaluate ELO.</p>

<h2 id="metrics">Metrics</h2>
<p>We evaluate ELO by computing the following ‘calibration metric’ for various values of <code class="highlighter-rouge">p</code>:</p>

<ul>
  <li>In all the matches in which the ELO rating system predicts that the higher-rated player has probability <code class="highlighter-rouge">p</code> of winning, what is the true rate at which the higher-rated player actually wins.</li>
</ul>

<p>For example, if the ELO rating system believes that the higher rated player has 70% odds of winning, then we should observe the higher rated player winning approximately 70% of the time.</p>

<p>These calibration metrics provide a measure of how well (or not) a rating system can fix problem 2.</p>

<h2 id="data-exploration">Data Exploration</h2>
<p>The raw data I used contains the male match history from the past few decades. It includes players’ names, their seed for the tournament, who won the match, the scores of the games in the match (usually) and some other minor details.</p>

<p>Here are a few entries from the dataframe to illustrate:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">tournament_index</th>
      <th style="text-align: right">round</th>
      <th style="text-align: right">players</th>
      <th style="text-align: right">result</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">Quarter-finals</td>
      <td style="text-align: right">[1] Tayyab Aslam (PAK) bt Farhan Hashmi (PAK)</td>
      <td style="text-align: right">7-11, 11-9, 11-5, 11-5 (32m)</td>
    </tr>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">Quarter-finals</td>
      <td style="text-align: right">[7] Israr Ahmed (PAK) bt [9/16] Waqas Mehboob …</td>
      <td style="text-align: right">11-3, 11-3, 11-8 (23m)</td>
    </tr>
    <tr>
      <td style="text-align: right">0</td>
      <td style="text-align: right">Quarter-finals</td>
      <td style="text-align: right">[4] Amaad Fareed (PAK) bt [5] Farhan Zaman (PAK)</td>
      <td style="text-align: right">11-8, 11-7, 12-10 (25m)</td>
    </tr>
  </tbody>
</table>

<p>Not all of the data in this dataframe was needed for the project and there was a little bit of dirty data. Here are the cleaning and feature extraction steps taken:</p>
<ul>
  <li>Dropping columns that were not useful for the analysis (<code class="highlighter-rouge">round</code> and <code class="highlighter-rouge">tournament_index</code>).</li>
  <li>Extracting the score in games from the score in points provided in the <code class="highlighter-rouge">result</code> column. In retrospect, this was not needed for the project, but it is still useful if I want to extend the project to make use of games scores. See possible improvements below.</li>
  <li>Keeping only those rows in which one player beat another player and dropping all other rows.
    <ul>
      <li>The vast majority of rows dropped were because a player automatically gets through in a particular round. E.g. if there are 48 players in a tournament, then in Round 1, players seeded 1 to 16 will automatically get through to Round 1 without having to beat anybody.</li>
      <li>There were exactly two other rows that got dropped. One is recorded as “No shows” and the other is recorded as “Final not played due to unsafe court conditions.”</li>
    </ul>
  </li>
  <li>Parsing the <code class="highlighter-rouge">players</code> column of the raw data.
    <ul>
      <li>Extract the name (and seed and country) of the winner and loser of the match.</li>
      <li>This was done using regex. If you are interested, the regex patterns used are in the <code class="highlighter-rouge">parse_player_entry</code> function in the <a href="https://github.com/Lovkush-A/squash_elo/blob/main/notebooks/01-la-processing.ipynb">processing notebook</a>.</li>
    </ul>
  </li>
</ul>

<p>At the end of this exploration, cleaning and feature extraction, the key information we end up with is a dataframe with two columns (name of winner and name of loser) where each row is a single match, and it is ordered chronologically.</p>
<h2 id="data-visualisation">Data Visualisation</h2>
<p>As far as I know, there are no data visualisations that help with the next task of calculating ELO ratings. If you run the <a href="https://github.com/Lovkush-A/squash_elo/blob/main/notebooks/01-la-processing.ipynb">processing notebook</a>, you will see some visuals (e.g. showing distribution of players’ win percentages) but they have no influence on the next steps.</p>

<h2 id="data-preprocessing">Data Preprocessing</h2>
<p>This has already been discussed in the data exploration section above.</p>

<h2 id="implementation">Implementation</h2>
<p>The code to calculcate the ELO rating systems is in the <a href="https://github.com/Lovkush-A/squash_elo/blob/main/notebooks/02-la-analysis.ipynb">analysis notebook</a>.</p>

<p>Surprisingly, there were minimal complications in the implementation of the ELO rating system. I just had to write functions that calculate how to update ELO ratings based on a single match, then loop through all matches and update ELO ratings one-by-one.</p>

<p>One implementation detail worth noting is how I chose an initial value for the hyper-parameter <code class="highlighter-rouge">K</code> (see problem statement section). The choice was based on the values used in chess (the most famous place ELO ratings are used) and they are given in the <a href="https://en.wikipedia.org/wiki/Elo_rating_system#Mathematical_details">wikipedia article on ELO rating system</a>.</p>

<h2 id="refinement">Refinement</h2>
<p>The refinement process was straightforward: I simply tried various values for the hyper-parameter <code class="highlighter-rouge">K</code> and looked at how their values in the ‘calibration metrics’ compared. The first solution tried used <code class="highlighter-rouge">K=32</code> and the final solution uses <code class="highlighter-rouge">K=100</code>.</p>

<h2 id="results">Results</h2>

<p>Here is table of ‘calibration metrics’ for <code class="highlighter-rouge">K=32</code>:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Predicted probability of higher &lt;/br&gt;rated player winning.&lt;/br&gt; Rounded to nearest 0.05</th>
      <th style="text-align: right">Number of predictions</th>
      <th style="text-align: right">Observed fraction of matches that &lt;/br&gt;higher rated player won</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0.50</td>
      <td style="text-align: right">7386</td>
      <td style="text-align: right">0.466152</td>
    </tr>
    <tr>
      <td style="text-align: right">0.55</td>
      <td style="text-align: right">11801</td>
      <td style="text-align: right">0.566647</td>
    </tr>
    <tr>
      <td style="text-align: right">0.60</td>
      <td style="text-align: right">9542</td>
      <td style="text-align: right">0.657514</td>
    </tr>
    <tr>
      <td style="text-align: right">0.65</td>
      <td style="text-align: right">8010</td>
      <td style="text-align: right">0.746067</td>
    </tr>
    <tr>
      <td style="text-align: right">0.70</td>
      <td style="text-align: right">6759</td>
      <td style="text-align: right">0.803817</td>
    </tr>
    <tr>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">5772</td>
      <td style="text-align: right">0.867983</td>
    </tr>
    <tr>
      <td style="text-align: right">0.80</td>
      <td style="text-align: right">4967</td>
      <td style="text-align: right">0.900946</td>
    </tr>
    <tr>
      <td style="text-align: right">0.85</td>
      <td style="text-align: right">4062</td>
      <td style="text-align: right">0.932546</td>
    </tr>
    <tr>
      <td style="text-align: right">0.90</td>
      <td style="text-align: right">3128</td>
      <td style="text-align: right">0.953005</td>
    </tr>
    <tr>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">2112</td>
      <td style="text-align: right">0.974905</td>
    </tr>
    <tr>
      <td style="text-align: right">1.00</td>
      <td style="text-align: right">369</td>
      <td style="text-align: right">0.981030</td>
    </tr>
  </tbody>
</table>

<p>The pattern here is that the predicted odds of winning are underconfident: if the ELO rating thinks that the higher-rated player has 80% odds of winning, they actually win 90% of the time.
My instinct was that the ELO rating was not updating quick enough based on the data, i.e. that <code class="highlighter-rouge">K</code> is too small.
But I was not 100% sure of this, so I did some (basic and manual) hyperparameter tuning, creating a function that loops through several values of <code class="highlighter-rouge">K</code> (namely 10, 50, 100, 200 and 500).</p>

<p>Manually looking at the tables shows that <code class="highlighter-rouge">K</code> = 100 is the best value out of these. Here are the values for the calibration metrics we get:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: right">Predicted probability of higher rated player winning. Rounded to nearest 0.05</th>
      <th style="text-align: right">Number of predictions</th>
      <th style="text-align: right">Observed fraction of matches that higher rated player won</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: right">0.50</td>
      <td style="text-align: right">2566</td>
      <td style="text-align: right">0.518316</td>
    </tr>
    <tr>
      <td style="text-align: right">0.55</td>
      <td style="text-align: right">5315</td>
      <td style="text-align: right">0.559548</td>
    </tr>
    <tr>
      <td style="text-align: right">0.60</td>
      <td style="text-align: right">5292</td>
      <td style="text-align: right">0.611678</td>
    </tr>
    <tr>
      <td style="text-align: right">0.65</td>
      <td style="text-align: right">5238</td>
      <td style="text-align: right">0.668385</td>
    </tr>
    <tr>
      <td style="text-align: right">0.70</td>
      <td style="text-align: right">5401</td>
      <td style="text-align: right">0.700796</td>
    </tr>
    <tr>
      <td style="text-align: right">0.75</td>
      <td style="text-align: right">5642</td>
      <td style="text-align: right">0.755583</td>
    </tr>
    <tr>
      <td style="text-align: right">0.80</td>
      <td style="text-align: right">6024</td>
      <td style="text-align: right">0.813081</td>
    </tr>
    <tr>
      <td style="text-align: right">0.85</td>
      <td style="text-align: right">6507</td>
      <td style="text-align: right">0.849393</td>
    </tr>
    <tr>
      <td style="text-align: right">0.90</td>
      <td style="text-align: right">7258</td>
      <td style="text-align: right">0.900248</td>
    </tr>
    <tr>
      <td style="text-align: right">0.95</td>
      <td style="text-align: right">8834</td>
      <td style="text-align: right">0.937967</td>
    </tr>
    <tr>
      <td style="text-align: right">1.00</td>
      <td style="text-align: right">5831</td>
      <td style="text-align: right">0.967930</td>
    </tr>
  </tbody>
</table>

<p>You can see that the predictions are well callibrated!</p>

<h2 id="reflection">Reflection</h2>
<p>I personally find these results astonishing. By using a straightforward rule to update players’ ratings based on their old ratings, we are able to get a meaningful rating system that provides callibrated odds on who will win a match.</p>

<p>I was not expecting this at all! Before doing the project, I was anticipating having to tweak the algorithm (e.g. by including some ‘domain knowledge’ somehow), but the ELO rating system just worked out-of-the-box. It is always nice when things simply work out!</p>

<h2 id="improvement">Improvement</h2>
<p>There are numerous ways this project could be improved or extended. Here are just a few possibilities:</p>

<ul>
  <li>Automatically update ratings as new results come in, by scraping live match data.</li>
  <li>Create a web app with these squash ratings. Allow users to see predictions for upcoming matches and tournaments, and explore past data.</li>
  <li>Currently we only use the final result of the match. However, we should be able to use the score in games to improve the ratings: winning 3-0 is more impressive than winning 3-2 and that should be reflected in the update rule. The way to do this is to have more refined definition of ‘true score’. Right now true score is 1 for victory, but can imagine having true score of 0.9 for winning 3-0 and true score of 0.7 for winning 3-2. This would require some tuning.</li>
  <li>Creating a general purpose ELO-rating package that people can use to create ELO ratings and predictions for any sports or competitions they are interested in.</li>
</ul>]]></content><author><name></name></author><category term="data science" /><summary type="html"><![CDATA[Improving the ratings of professional squash players using data and the ELO rating system]]></summary></entry><entry><title type="html">Similarity trees and NaN trees</title><link href="https://lovkush-a.github.io/blog/data%20science/2021/04/17/trees_variants.html" rel="alternate" type="text/html" title="Similarity trees and NaN trees" /><published>2021-04-17T00:00:00-05:00</published><updated>2021-04-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/2021/04/17/trees_variants</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/2021/04/17/trees_variants.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>I summarise the two main ideas of the paper <a href="http://saketsathe.net/downloads/simforest.pdf">Similarity Forests</a> by Sathe and Aggarwal: trees via similarities and trees with <code class="highlighter-rouge">NaN</code> values.</p>

<h2 id="notation">Notation</h2>
<p>This is a supervised learning setting so the dataset consists of a set of tuples <code class="highlighter-rouge">(X_i, y_i)</code> where:</p>
<ul>
  <li><code class="highlighter-rouge">i</code> ranges from <code class="highlighter-rouge">1</code> to <code class="highlighter-rouge">N</code>, so <code class="highlighter-rouge">N</code> is the number of datapoints</li>
  <li>Each <code class="highlighter-rouge">X_i</code> is a <code class="highlighter-rouge">D</code>-dimensional vector, so <code class="highlighter-rouge">D</code> is the number of features. The <code class="highlighter-rouge">d</code>th entry of <code class="highlighter-rouge">X_i</code> will be denoted <code class="highlighter-rouge">X_id</code></li>
  <li>Each <code class="highlighter-rouge">y_i</code> is a scalar and is the target or prediction variable</li>
</ul>

<h2 id="trees-via-similarities">Trees via similarities</h2>
<p>In a standard decision tree, the decision or rule at each node takes the following form:</p>
<ul>
  <li>Let <code class="highlighter-rouge">d</code> be one of the possible feature columns</li>
  <li>Let <code class="highlighter-rouge">a</code> be a constant</li>
  <li>Then <code class="highlighter-rouge">X_i</code> is in the left or right child node depending on whether <code class="highlighter-rouge">X_id</code> is smaller than or bigger than <code class="highlighter-rouge">a</code></li>
</ul>

<p>At each step, the <code class="highlighter-rouge">d</code> and <code class="highlighter-rouge">a</code> are chosen to (greedily) minimise some loss function, e.g. Gini impurity.</p>

<p>For this process to be possible, we need to have access to the individual entries <code class="highlighter-rouge">X_id</code>. What Sathe and Aggarwal show is that you can construct a decision tree without access to these individual entries <code class="highlighter-rouge">X_id</code> as long as you have access to some form of similarity between your datapoints. This is analagous to the kernel trick for SVMs: you only need to know how to calculate the dot products between data points; you do not need to calculate the vector representation of each of the data points.</p>

<p>So suppose we have access to the similarities between all data points <code class="highlighter-rouge">S</code>, where <code class="highlighter-rouge">S_ij</code> is equal to the similarity between <code class="highlighter-rouge">i</code>th and <code class="highlighter-rouge">j</code>th datapoints. Then the decision rule at each node takes the following form:</p>
<ul>
  <li>Let <code class="highlighter-rouge">j</code> and <code class="highlighter-rouge">k</code> be distinct values from <code class="highlighter-rouge">1</code> to <code class="highlighter-rouge">N</code>, corresponding to two of the datapoints</li>
  <li>Let <code class="highlighter-rouge">a</code> be a constant</li>
  <li>Then <code class="highlighter-rouge">X_i</code> is in the left or right child node depending on whether <code class="highlighter-rouge">S_ij - S_ik</code> is smaller than or bigger than <code class="highlighter-rouge">a</code></li>
</ul>

<p>The intuition for this is that you are projecting all your datapoints onto the line joining <code class="highlighter-rouge">X_j</code> and <code class="highlighter-rouge">X_k</code>, and then picking a point on the line to split the datapoints in two. For a derivation of why <code class="highlighter-rouge">S_ij - S_ik</code> corresponds to this projection, and a diagram that helps visualise this projection, you can read the original paper.</p>

<p>And that’s it! There are various details I have skipped over (e.g. how to choose <code class="highlighter-rouge">j</code> and <code class="highlighter-rouge">k</code> at each node), but using <code class="highlighter-rouge">S_ij - S_ik</code> instead of <code class="highlighter-rouge">X_id</code> is the central idea.</p>

<h2 id="trees-with-nan-values">Trees with <code class="highlighter-rouge">NaN</code> values</h2>
<p>What happens if <code class="highlighter-rouge">X_id</code> is a NaN value? The standard algorithm will not work. In the paper, they describe a possible way of dealing with this. I do not know if this paper is the first to come up with this idea, but it is where I found out about it so I am giving them the credit.</p>

<p>Before describing their solution, spend a couple of minutes thinking about how you might adjust the decision tree algorithm to deal with NaN values (without imputation).</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>No really, I recommend you give it some thought. You will learn more by actively engaging and thinking for yourself, rather than passively reading.</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>The idea in their paper is quite neat. In the standard decision tree, each node has two children nodes. To deal with with NaN values, just introduce a third child containing the NaN entries! From here, there are various ways of dealing with these NaN-nodes. In the paper, Sathe and Aggarwal make all these nodes be leaf nodes and the prediction at these NaN-nodes is equal to the modal class of its parent. Another option is to just treat these NaN-nodes like any other node, and continue to split on them.  I imagine the strategy one chooses will depend on the particular situation one is in, and whether there are any patterns in the NaN values.</p>

<h2 id="conclusion">Conclusion</h2>
<p>To summarise, the paper contains two main new ideas:</p>
<ul>
  <li>To create decision trees from similarities, use <code class="highlighter-rouge">S_ij - S_ik</code> instead of <code class="highlighter-rouge">X_id</code> to decide how to split at each node.</li>
  <li>To deal with NaN values, create a third NaN-node.</li>
</ul>]]></content><author><name></name></author><category term="data science" /><summary type="html"><![CDATA[I summarise the two main ideas in Sathe and Aggarwal's paper Similarity Forests.]]></summary></entry><entry><title type="html">Examples of collider bias</title><link href="https://lovkush-a.github.io/blog/data%20science/causality/tutorial/2021/02/21/collider.html" rel="alternate" type="text/html" title="Examples of collider bias" /><published>2021-02-21T00:00:00-06:00</published><updated>2021-02-21T00:00:00-06:00</updated><id>https://lovkush-a.github.io/blog/data%20science/causality/tutorial/2021/02/21/collider</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/causality/tutorial/2021/02/21/collider.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In this post, I will describe four different examples of collider bias. In the past couple of months, it is probably the single biggest idea I have learnt and opened my eyes to new possibilities and subtleties in data. In particular, it reveals and explains why conditioning on some variable Z when you are trying to understand the effect of X on Y can make things worse; I used to think one ought to condition on as many variables as possible!</p>

<p>I will describe a few examples that illustrate what collider bias is, rather than giving a formal definition, because I admittedly do not yet know the formal definition of a collider!</p>

<p>The examples I describe are from <a href="https://mixtape.scunning.com">Causal Reasoning: The Mixtape</a> and from <a href="https://www.stitcher.com/show/the-turing-podcast/episode/being-an-epidemiologist-in-2020-76960769">this interview of Peter Tenant</a> on the Turing Podcast.</p>

<h2 id="main-intuition-of-collider-bias">Main intuition of collider bias</h2>
<p>The main intuition I have is that a variable Z is a collider if by conditioning on Z you create a spurious correlation between two other variables A and B (which in turn muddles up our analysis of the causal effect of X on Y).</p>

<h2 id="example-1-movie-stars">Example 1. Movie stars.</h2>
<p>Let us assume that to become a highly successful movie star, you must be either particularly talented or particularly good-looking (yes yes, looks are subjective, but you know what I mean). Let us also reasonably assume that in the whole population, there is no correlation between talent and looks.</p>

<p>With that set up, we can now say that if you condition on whether someone is a movie star, you create a negative correlation between talent and looks.</p>

<p>For a fuller explanation and some diagrams to visualise what is happening, see Section 3.1.6 from <a href="https://mixtape.scunning.com/dag.html">Causal Inference: The Mixtape</a>.</p>

<h2 id="example-2-elite-students">Example 2. Elite students.</h2>
<p>This is similar to the movie stars example, except this time we have the effect that by conditioning on whether somebody is at an elite institution or not will create a spurious negative correlation between talent and hardwork. A genius does not need to work hard to succeed whereas merely excellent individuals also need to work hard to get in.</p>

<h2 id="example-3-low-birth-weight-paradox">Example 3. Low birth weight paradox</h2>
<p>This is a real example and it stumped the medical community for many years. The effect of smoking on infant mortality was being investigated. It was already known that smoking caused a higher risk of low-birth weight and that low birth weight increases the risk of infant mortality. We would like to know whether smoking has any other effects on infant mortality apart from causing low birth weight. So, you do what is natural and you condition on birthweight and now look at the effect of smoking on mortality. This is where the paradox comes in: it was found that smoking had a protective effect!</p>

<p>One’s first instinct is there is a mistake in the data, but that is not the case. The explanation, as you might guess, is that it is a result of collider bias! Here are the relevant facts:</p>
<ul>
  <li>There are various causes of birthweight. Smoking is one of them, but there are others.</li>
  <li>The other causes of low birthweight have a greater negative risk on infant mortality, separate to their effect on low birthweight.</li>
</ul>

<p>As a challenge, combine all these facts to work out how conditioning on birthweight can create the finding that smoking has a protective effect.</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>Really, have a go. You learn more by actively engaging with an idea, than just reading somebody else’s thoughts.</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>The explanation is as follows.</p>
<ul>
  <li>When you condition on birthweight, you create a correlation between smoking and the other causes of birthweight. Specifically, a negative correlation.</li>
  <li>Hence, more smoking ‘causes’ less of the other causes of low birthweight, which causes a reduction in mortality.</li>
</ul>

<p>If you did not follow that, there is a good chance my explanation is not ideal. I recommend you read through <a href="https://mixtape.scunning.com/dag.html">Chapter 3 on Causal Diagrams</a>, and then look at <a href="https://twitter.com/pwgtennant/status/1030896340891586560?lang=en">Peter Tennant’s tweet</a> containing the causal diagram for this situation.</p>

<h2 id="example-4-gender-pay-gap">Example 4. Gender pay gap</h2>
<p>When understanding the gender pay gap, it is obvious that one should condition on the job. There is a big difference between:</p>
<ul>
  <li>Pay gap being a result of women being paid less than men for the same job</li>
  <li>Pay gap being a result of women tending to do lower paid jobs than men do</li>
</ul>

<p>However, the thing that is not obvious is that conditioning on the job can create a spurious correlation. Let us make the following two assumptions:</p>
<ul>
  <li>Women face discrimination in the job-hunting process</li>
  <li>There is a positive correlation between ability/talent and pay</li>
</ul>

<p>Now, conditioning on jobs will create a spurious correlation between gender and ability - because women are discriminated against in the job-hunting process, women will tend to have a higher ability then men in the same job. Because of the second assumption, we would then observe that women are paid more than men, after conditioning on jobs!</p>

<p>To summarise, in this simplistic model women are facing discrimination but the standard approach of conditioning on jobs will make it look like it is in fact men who are being discriminated against.</p>

<p>For a fuller discussion and explanation (along with causal diagrams), see <a href="https://mixtape.scunning.com/dag.html">Section 3.1.5 in Causal Inference The Mixtape</a>.</p>

<h2 id="conclusion">Conclusion</h2>
<p>I hope these examples of collider bias were insightful. Like I said at the start, the main insight for me was that conditioning on a variable can sometimes create noise, and the reason is that if two variables A and B both cause Z, then conditioning on Z will create a spurious correlation between A and B.</p>

<p>A worrying thing about this is it reveals just how difficult answering causal questions is. As seen in the birthweight and gender examples above, you can observe the totally opposite effect as a result of collider bias. As far as I can tell, the only way to avoid collider bias is to have considered all the possible causes and effects, and to appropriately condition on the necessary variables. I guess I will just have to add the clause ‘assuming all relevant causes have been taken into account’ at the start of any claim I now read!</p>]]></content><author><name></name></author><category term="data science" /><category term="causality" /><category term="tutorial" /><summary type="html"><![CDATA[Collider bias can completely skew research findings. I describe some examples that highlight how this non-obvious bias arises.]]></summary></entry><entry><title type="html">Using Data Science to Create Art</title><link href="https://lovkush-a.github.io/blog/data%20science/data%20viz/2021/01/01/art.html" rel="alternate" type="text/html" title="Using Data Science to Create Art" /><published>2021-01-01T00:00:00-06:00</published><updated>2021-01-01T00:00:00-06:00</updated><id>https://lovkush-a.github.io/blog/data%20science/data%20viz/2021/01/01/art</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/data%20viz/2021/01/01/art.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>While working with Lexitas to process deposition transcripts, I was trying to find the best tools and settings to visual clusters of questions. Several of the choices lead to visually striking plots. I was pleasantly surprised by this. My mentors enjoyed looking at these, so I figured other people mght like them as well.</p>

<h2 id="how-the-art-was-created">How the art was created</h2>
<p>The dataset was a list of questions taken from a single deposition transcript. Importantly, and surprisingly, all the images to follow are all representations of this <em>same dataset</em>.</p>

<p>I then vectorised the sentences using various tools (e.g. tfidf and glove vectors) and did dimension reduction using either tSNE or UMAP. I then plotted the resulting data using Bokeh.</p>

<h2 id="the-art">The art</h2>
<h3 id="the-firework">The Firework</h3>
<p><img src="/blog/images/art_firework.png" alt="image" /></p>

<h3 id="the-cell">The Cell</h3>
<p><img src="/blog/images/art_cell.png" alt="image" /></p>

<h3 id="the-dipole">The Dipole</h3>
<p><img src="/blog/images/art_dipole.png" alt="image" /></p>

<h3 id="the-meteor">The Meteor</h3>
<p><img src="/blog/images/art_meteor.png" alt="image" /></p>

<h3 id="the-rabbit">The Rabbit</h3>
<p><img src="/blog/images/art_rabbit.png" alt="image" /></p>

<h2 id="final-thoughts">Final Thoughts</h2>
<p>What do you think? The main thing that surprised me about this is that these are all representations of the same underlying dataset! When I showed these to my mentors, they said that there is a little joke that if you spend long enough, you can get whatever you want in unsupervised learning.</p>

<p>Lastly, I think it is nice that while I am grinding away trying to determine what is the most valuable for my clients, I can get these little pleasant surprises on the side.</p>]]></content><author><name></name></author><category term="data science" /><category term="data viz" /><summary type="html"><![CDATA[While trying to find the best clustering of some text data, I unintentionally stumbled upon some visually striking plots, which I think are highly aesthetic and artistic.]]></summary></entry><entry><title type="html">Presentations. Turning good slides into great slides</title><link href="https://lovkush-a.github.io/blog/data%20science/communication/2020/12/23/demoday.html" rel="alternate" type="text/html" title="Presentations. Turning good slides into great slides" /><published>2020-12-23T00:00:00-06:00</published><updated>2020-12-23T00:00:00-06:00</updated><id>https://lovkush-a.github.io/blog/data%20science/communication/2020/12/23/demoday</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/communication/2020/12/23/demoday.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In this post, I will describe how I turned a decent presentation into a great one. But first some context.</p>

<p>At the end of the <a href="https://faculty.ai/fellowship/">Faculty Data Science Fellowship</a>, Faculty hosts an event called ‘Demo Day’, in which the fellows each give a 5-minute presentatation to showcase what they achieved in the project. Over 300 people from the data science industry attend this event, so these presentations are a big deal.  You can see many of the demo day presentations on <a href="https://www.youtube.com/channel/UCaiUkj_9GAncPqJxvbBUkWA/videos">Faculty’s YouTube channel</a>.</p>

<p>Because Demo Day is important, we literally spend a whole week preparing the presentation, giving our talk to a dozen or so different people in Faculty, and iteratively improve based on their feedback.</p>

<p>I found this to be an insightful experience. My first draft of the presentation was decent, but it was incredible to get all the feedback on how I can improve. I find this to be a rare experience, because there is a societal norm to always be positive and avoid (constructive) criticism. (Or I just don’t know how to ask for feedback!)  Furthermore, if you search online or attend a workshop on presentations, you will be shown examples of terrible presentations and told about basic things (don’t have paragraphs of text, use consistent colours and fonts, etc.).  Sometimes you do get some real insights, e.g. <a href="https://waitbutwhy.com/2016/03/doing-a-ted-talk-the-full-story.html">Tim Urban’s</a> post about how they prepared for their TED talk.</p>

<p>So, I hope this blogpost will be useful, and will help you improve your presentations in the future as much as it will help me improve mine.</p>

<h2 id="structure-of-blogpost">Structure of blogpost</h2>
<p>I will focus on the visual aspect of the presentation, i.e. the slides. For each section of slides I will:</p>
<ul>
  <li>Show the earliest version of the section</li>
  <li>Provide context for the slides, by quoting roughly what I would say</li>
  <li>Ask you to think how you might improve the slides</li>
  <li>Show you different versions of the slides</li>
  <li>Describe what changes were made and why they were made</li>
</ul>

<p>I strongly encourage you to really think about how you might improve the slides. You will learn more if you do this, than if you just passively read.</p>

<h2 id="slides-1">Slides 1</h2>
<p>Version 1:
<img src="/blog/images/demoday1_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>Suppose Mo is a surgeon at his local private hospital. He is unhappy because he feels like he is experiencing discrimination from them. So he does what any good American would do, and he sues them! Lawyers are hired to resolve the disagreement. In order for the lawyers to do their job, they need to know what the facts of the matter are, and one way to do this is to interview potential witnesses.</p>
</blockquote>

<p>How would you improve the slides? This might the hardest example to think of improvements, because the slides are already pretty good, unlike many of the other slides to come.</p>

<p>Version 2:
<img src="/blog/images/demoday1_2.gif" alt="image" /></p>

<p>Changes:
There are three changes here, and they were all suggested by a Graphic Designer <a href="https://www.linkedin.com/in/nadinedaouk/">Nadine Daouk</a>.</p>
<ul>
  <li>Have the icons of the lawyers match the style of the other icons</li>
  <li>Have thick-ish margins / white space on the outside of the slides</li>
  <li>Use different sizes of icons, to create ‘hierarchy’ in the slide. For me, this is the single big new idea I learnt from the whole experience. The idea is that on every slide, it should be clear what is the most important piece of information on it. In this case, Mo and the hospital are the two major people in the situation, followed by the lawyers, followed by the witnesses. That is not clear in the final slide of Version 1, but it is clear in Version 2.</li>
</ul>

<p>Additional thoughts:</p>

<p>First, it was such a pleasure to watch an expert at work. I am still taken aback by how Nadine provided feedback on my presentation: for every single slide, she instantly noticed if something was off and knew precisely what to do to improve it. It really was remarkable.</p>

<p>Second, this idea of hierarchy is an ‘advanced’ idea. It only becomes relevant once you have removed all the basic errors (e.g. having too much information on the slide) <em>and</em> if there is a difference in importance in the information in the slide. Another example of hierarchy is changing the font size or colour to indicate which words on a slide are vital and which are secondary.</p>

<h2 id="slides-2">Slides 2</h2>
<p>Version 1:
<img src="/blog/images/demoday2_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>Each witness is interviewed one at a time. There is somebody in the room furiously typing away, and at the end of the interview a written record of everything that was said is created.</p>
</blockquote>

<p>How would you improve the slides? Big hint: I will be repeating myself quite a lot!</p>

<p>Version 2:
<img src="/blog/images/demoday2_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Have the icons of the lawyers, typewriter and table match the style of the other icons</li>
  <li>Have margins / white space on the outside of the slides</li>
</ul>

<p>Additional thoughts:
There is going to be significant repetition in the slides, but that is the nature of these things. Lastly, in the final version of the presentation, these slides were actually cut because I had to shorten this part of the talk.</p>

<h2 id="slides-3">Slides 3</h2>
<p>Version 1:
<img src="/blog/images/demoday3_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>To give you a sense of what questions are asked, here are some examples. […] And they go on, and on, and on.</p>
</blockquote>

<p>How would you improve the slides?</p>

<p>Version 2:
<img src="/blog/images/demoday3_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Another fellow <a href="https://www.linkedin.com/in/zoltan-laczko-b8b488190/">Zoltan</a> said that in the first version, the slides looked a bit too tidy and suggested making it look like more of a mess.</li>
</ul>

<h2 id="slides-4">Slides 4</h2>
<p>Version 1:
<img src="/blog/images/demoday4_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>At the end of the day, the lawyers need to go through this mess of information, find out the important themes and details, and determine how to best represent their clients.</p>
</blockquote>

<p>How would you improve the slides?</p>

<p>Version 2:
<img src="/blog/images/demoday4_2.gif" alt="image" /></p>

<p>Version 3:
<img src="/blog/images/demoday4_3.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Change the icon of the lawyer</li>
  <li>Make the lawyer clearer to see. I made several attempts at this, and each time it was a consistent piece of feedback that it was hard to see. Looking at the final version now, I could have had a few words above the lawyers, to maintain the effect of the lawyers being swamped by all the information.</li>
</ul>

<h2 id="slides-5">Slides 5</h2>
<p>Version 1:
<img src="/blog/images/demoday5_1.png" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>Who are Lexitas Legal. Simply put, they provide services to make lawyers’ lives easier. E.g. they provide the tools and expertise to create the written records, or the tools to do online depositions.</p>
</blockquote>

<p>How would you improve the slides? Based on previous slides, you should be able to give some improvements!</p>

<p>Version 2:
<img src="/blog/images/demoday5_2.png" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Change the icons to match the style of other icons in the slides</li>
  <li>Change the phrasing of the title and the slide. These slides came after the explanation about depositions. <a href="https://www.linkedin.com/in/mariadiaz2010/">Maria Diaz</a> noticed that this felt a bit jarring - and I agreed. However, to understand what Lexitas does, you first have to understand what a deposition is. The compromise I came up with is that instead of saying ‘Who are Lexitas Legal’, I would say ‘So how does Lexitas fit into all this?’. This created a smoother transition.</li>
</ul>

<h2 id="slides-6">Slides 6</h2>
<p>Version 1:
<img src="/blog/images/demoday6_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>Lexitas is sitting on mountains of transcripts. They wanted to know if there were any nuggets of gold hidden within and if data science was the right tool to find it. The short answer is ‘Yes!’</p>
</blockquote>

<p>How would you improve the slides? I suspect many of you will be able to point out some ways to make it better.</p>

<p>Version 2:
<img src="/blog/images/demoday6_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li><a href="https://www.linkedin.com/in/elliotchristou/">Elliot Christou</a> pointed out how the transparency of all the icons made the mountain of transcripts look very untidy. I made them non-transparent, and also tidied up the mountain so the spacing was uniform.</li>
  <li><a href="https://www.linkedin.com/in/nadinedaouk/">Nadine Daouk</a> said the nugget of gold was small and should be made more prominent.</li>
  <li>And as for many of the previous slides, I adjusted the spacing to include a margin / white space around the outside of the slide.</li>
</ul>

<p>However, this was not the final version. Here is the final version:
<img src="/blog/images/demoday6_3.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Following the incredibly insightful advice of expert story-teller <a href="https://www.linkedin.com/in/jonny-howell-70250855">Jonny Howell</a>, I removed the forced metaphor of the California Gold Rush and references to nuggets of gold, and used the phrase ‘hidden value’ instead.  However, as a result of removing the explicit verbal references to gold, my technical mentor <a href="https://www.linkedin.com/in/saniajevtic/">Sania Jevtic</a> thought that gold nugget was actually a boxing glove punching its way through the mountain of transcripts! This is definitely not what I wanted, so following their suggestion, I replaced it with a pot of gold.</li>
  <li><a href="https://www.linkedin.com/in/caroline-ames-6a709765/">Caroline Ames</a> made the suggestion of adding some text to these slides. I think it is nice to fill in the white space, and also emphasise the question.</li>
</ul>

<h2 id="slides-7">Slides 7</h2>
<p>Version 1:
<img src="/blog/images/demoday7_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>This app I created visualises all the questions in a deposition. You select a case, hit the button, and out pops this visual. Each dot represents a question and similar questions are grouped together. This is an interactive visual so as you move your mouse over the dots, the question is revealed. If you hover your mouse over this clump, you’ll see they are all about complaints on working conditions. Another example is that this cluster is all about emails.</p>
</blockquote>

<p>How would you improve the slides? As a hint, how would you make it clearer that these screenshots are from a web-app?</p>

<p>Version 2:
<img src="/blog/images/demoday7_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>The biggest change was to have the first screenshot include the web browser, to make clear that this was an app. This was suggested to me by <a href="https://www.linkedin.com/in/nadinedaouk/">Nadine Daouk</a> and <a href="https://www.linkedin.com/in/josh-muncke/">Josh Muncke</a>. (Actually, Josh suggested I get an image of a laptop with a blank screen and put the app within the laptop’s screen. This was too fiddly for me, and I think what I did by screenshotting the browser works nicely.)</li>
  <li>A few other cosmetic changes, all due to Nadine, were to make the arrows horizontal, make the arrows black, and to make sure the plots aligned from slide to slide. (In Version 1, there is a small change between slides 3 and 4).</li>
</ul>

<p>Once again, this was not the final version:</p>

<p>Version 3:
<img src="/blog/images/demoday7_3.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>I cut out some slides. This is because I had to streamline the slides as they were too long.</li>
  <li><a href="https://www.linkedin.com/in/savina-van-der-straten-waillet-29a79758/">Savina van der Straten Waillet</a> said that I rushed through the app. My solution to this was to remove all the words from the slide with the app, as I assumed that they (and presumably other people) would try to read what was there.</li>
  <li>Tidied up the redaction boxes. This is something I noticed myself. After paying attention to details for so long, you start to notice things by yourself!</li>
</ul>

<h2 id="slides-8">Slides 8</h2>
<p>Version 1:
<img src="/blog/images/demoday8_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>A second app I created is a Similar Question Finder. A lawyer enters any question they are interested in, hits the button, and the app will output all the questions, along with their answers, that are most similar. […]</p>
</blockquote>

<p>How would you improve the slides?</p>

<p>Version 2:
<img src="/blog/images/demoday8_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Like for the first app, I removed the text from the screenshot of the app.</li>
  <li>I resized the window of the app to make it fit nicer into the page</li>
  <li>I trimmed out a lot of content, due to time restrictions</li>
  <li>I tidied up the red boxes</li>
</ul>

<h2 id="slides-9">Slides 9</h2>
<p>Version 1:
<img src="/blog/images/demoday9_1.gif" alt="image" /></p>

<p>Context:</p>
<blockquote>
  <p>The next slide is for the more technical members of the audience, and gives list of main tools used to create the app […]</p>
</blockquote>

<p>How would you improve the slides?</p>

<p>Version 2:
<img src="/blog/images/demoday9_2.gif" alt="image" /></p>

<p>Changes:</p>
<ul>
  <li>Most people did not comment on these slides, but <a href="https://www.linkedin.com/in/caroline-ames-6a709765/">Caroline Ames</a> suggested creating a flowchart so that even people without any technical knowledge can get some vague sense of the process.</li>
</ul>

<p>Additional thoughts:
Though the new version is aesthetically nicer than the old version, in retrospect, I actually think the old version was more appropriate given my script and my time limitation. This is an interesting situation where the flowchart has the potential to create a better presentation, but it is harder to make it work.</p>

<h2 id="final-remarks-and-lessons-learnt">Final remarks and lessons learnt</h2>
<p>I hope these examples were insightful for you, that it will help you improve the quality of your slides. I should say that this is only a sample of the changes that were made. E.g. I experimented with adding background colour, but a white background felt the best in the end.</p>

<p>The main lessons I learnt from this experience:</p>
<ul>
  <li>Think about ‘hierarchy’. Make clear which information is the most important in a slide.</li>
  <li>Have margins / white space around the outside of slides. More importantly, <em>use the gridlines functionality</em> to do this.</li>
  <li>Finally, and most important of all, get feedback from a wide range of people. Different people notice different things, and all their little contributions add up to considerable improvement.</li>
</ul>

<p>Comments, suggestions, or thoughts are highly welcome! In particular, I would be interested to know if you have any different ideas on how to improve the slides.</p>]]></content><author><name></name></author><category term="data science" /><category term="communication" /><summary type="html"><![CDATA[I spent a solid week improving my Demo Day talk with the help of several people at Faculty. In this post, I detail the changes that were made and big lessons I learnt.]]></summary></entry><entry><title type="html">A surprising bug caused by regex</title><link href="https://lovkush-a.github.io/blog/nlp/regex/2020/12/20/regex.html" rel="alternate" type="text/html" title="A surprising bug caused by regex" /><published>2020-12-20T00:00:00-06:00</published><updated>2020-12-20T00:00:00-06:00</updated><id>https://lovkush-a.github.io/blog/nlp/regex/2020/12/20/regex</id><content type="html" xml:base="https://lovkush-a.github.io/blog/nlp/regex/2020/12/20/regex.html"><![CDATA[<h2 id="introduction">Introduction</h2>
<p>In my <a href="https://faculty.ai/fellowship/">Faculty Fellowship</a>, my project was to analyse legal deposition transcripts using NLP. As always, the first step was to clean the data and extract the useful information from the raw text from the transcripts.</p>

<p>I created a bunch of functions to do this processing and tested it on a few documents, and it all seemed fine.  I then ran the program on the complete set of documents, but it would mysteriously get stuck on a particular transcript.  After spending a couple of hours combing through my code, I managed to isolate the problem to this innocent looking regex search:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pat</span> <span class="o">=</span> <span class="s">r'BY (M[RS]\. ([A-Z]+ ?)+|([A-Z]+ ?)+, ESQ)'</span>
<span class="n">match</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">pat</span><span class="p">,</span> <span class="s">'SAME BY ANY MEANS UNLESS UNDER THE DIRECT CONTROL'</span><span class="p">)</span>
</code></pre></div></div>

<p>The pattern is looking for something like ‘BY MR. JOHN SMITH’ or ‘BY JOHN SMITH, ESQ’.</p>

<p>I was extremely confused. Why should this regex search break anything? It obviously should return false: there is so ‘MR’ or ‘MS’ or ‘ESQ’ in the string, so why is it breaking?</p>

<h2 id="catastrophic-backtracking">Catastrophic backtracking</h2>
<p>I asked about this in the Faculty Slack channel, and I was helpfully directed towards the fantastic resource <a href="regex101.com">regex101</a>. If you enter the regex pattern and string from above into the website, it reveals the problem is ‘catastrophic backtracking’. This basically means that the search tree that is used behind the scenes is humungous - leading to a regex search that would take forever to complete.</p>

<p>More explicitly, instead of searching for ‘ESQ’ up front, the regex search function creates all possible candidate strings that match the rest of the regex pattern, and then checks if each of those have ‘ESQ’ at the end. Based on how I wrote the pattern, the total number of possible candidate strings is enormous, because every substring of characters corresponds to a branch point in the tree.</p>

<h2 id="a-solution">A solution</h2>
<p>A simple solution was to add a word boundary:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">BY</span> <span class="p">(</span><span class="n">M</span><span class="p">[</span><span class="n">RS</span><span class="p">]</span>\<span class="o">.</span> <span class="p">([</span><span class="n">A</span><span class="o">-</span><span class="n">Z</span><span class="p">]</span><span class="o">+</span>\<span class="n">b</span> <span class="err">?</span><span class="p">)</span><span class="o">+|</span><span class="p">([</span><span class="n">A</span><span class="o">-</span><span class="n">Z</span><span class="p">]</span><span class="o">+</span>\<span class="n">b</span> <span class="err">?</span><span class="p">)</span><span class="o">+</span><span class="p">,</span> <span class="n">ESQ</span><span class="p">)</span>
</code></pre></div></div>
<p>This prevents branching occuring at every single character, and instead branching only occurs at every single word boundary.</p>

<h2 id="conclusions">Conclusions</h2>
<ul>
  <li><a href="regex101.com">regex101</a> is a fantastic tool! I will be using this any time I need to use regex in the future.</li>
  <li>Debugging is harder than I already thought it was. Even things which I think are completely safe can be the cause of bugs.</li>
</ul>]]></content><author><name></name></author><category term="nlp" /><category term="regex" /><summary type="html"><![CDATA[My program to process hundreds of documents would mysteriously stop on a particular document. After a couple of hours of checking all my code, I managed to isolate the problem to a particular regex search. I describe what I learnt in this blog post.]]></summary></entry><entry><title type="html">Squash rankings, Part III, All hail Bokeh!</title><link href="https://lovkush-a.github.io/blog/python/data%20science/data%20viz/2020/11/01/squash3.html" rel="alternate" type="text/html" title="Squash rankings, Part III, All hail Bokeh!" /><published>2020-11-01T00:00:00-05:00</published><updated>2020-11-01T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/data%20viz/2020/11/01/squash3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/data%20viz/2020/11/01/squash3.html"><![CDATA[<h2 id="other-posts-in-series">Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/python/data%20science/2020/09/28/squash2.html">Squash rankings, Part II, dimension reduction and clustering</a></p>
  </li>
  <li>
    <p><a href="/blog/python/scraping/2020/09/17/squash1.html">Squash rankings, Part I, Scraping wikipedia and data analysis</a></p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>One of my mentors at the Faculty Fellowship, Sania Jevtic, recommended I try using Bokeh to plot various charts I was making. I tried it and without too much effort I managed to get it to work. It is AMAZING! The interactivity you get from it is fantastic and allows for much easier understanding of your data and richer presentation of your data. I will try to illustrate by redrawing some of the charts from Part II of this series.</p>

<p>I will be re-making the three charts below:</p>

<p><img src="/blog/images/squash2_malecluster.png" alt="image" /></p>

<p>Recall that the colours are clusters made by doing k-mean clustering on the original dataset, not doing clustering on each of the dimensionally-reduced datasets.</p>

<h2 id="bokeh-charts">Bokeh charts</h2>

<html lang="en">
  
  <head>
    
      <meta charset="utf-8" />
      <title>Bokeh Plot</title>
      
      
        
          
        
        
          
        <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js" integrity="sha384-T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM" crossorigin="anonymous"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="a1121850-5f93-4e82-a97f-6e47190ce637" data-root-id="3676"></div>
            
          
        
      
      
        <script type="application/json" id="3968">
          {"c67160e1-6f6b-4b1d-b68f-5b2a1204fa65":{"roots":{"references":[{"attributes":{"background_fill_color":"white","below":[{"id":"3687"}],"center":[{"id":"3690"},{"id":"3694"}],"left":[{"id":"3691"}],"renderers":[{"id":"3706"}],"title":{"id":"3677"},"toolbar":{"id":"3699"},"x_range":{"id":"3679"},"x_scale":{"id":"3683"},"y_range":{"id":"3681"},"y_scale":{"id":"3685"}},"id":"3676","subtype":"Figure","type":"Plot"},{"attributes":{"bottom_units":"screen","fill_alpha":0.5,"fill_color":"lightgrey","left_units":"screen","level":"overlay","line_alpha":1.0,"line_color":"black","line_dash":[4,4],"line_width":2,"right_units":"screen","top_units":"screen"},"id":"3698","type":"BoxAnnotation"},{"attributes":{},"id":"3712","type":"BasicTickFormatter"},{"attributes":{"overlay":{"id":"3698"}},"id":"3695","type":"BoxZoomTool"},{"attributes":{"text":"PCA of Male Squash Player Rankings"},"id":"3677","type":"Title"},{"attributes":{},"id":"3714","type":"Selection"},{"attributes":{},"id":"3713","type":"UnionRenderers"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"field":"clusters","transform":{"id":"3675"}},"line_alpha":{"value":0.1},"line_color":{"field":"clusters","transform":{"id":"3675"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3705","type":"Scatter"},{"attributes":{"fill_color":{"field":"clusters","transform":{"id":"3675"}},"line_color":{"field":"clusters","transform":{"id":"3675"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3704","type":"Scatter"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"3695"},{"id":"3696"},{"id":"3697"}]},"id":"3699","type":"Toolbar"},{"attributes":{},"id":"3710","type":"BasicTickFormatter"},{"attributes":{},"id":"3683","type":"LinearScale"},{"attributes":{"source":{"id":"3674"}},"id":"3707","type":"CDSView"},{"attributes":{"axis":{"id":"3687"},"grid_line_color":null,"ticker":null},"id":"3690","type":"Grid"},{"attributes":{},"id":"3681","type":"DataRange1d"},{"attributes":{"formatter":{"id":"3710"},"ticker":{"id":"3692"}},"id":"3691","type":"LinearAxis"},{"attributes":{"callback":null,"tooltips":[["Player","@players"],["Best rank","@best_rank"],["Years in Top 10","@years_in_top10"],["Earliest year","@earliest_year"]]},"id":"3696","type":"HoverTool"},{"attributes":{},"id":"3688","type":"BasicTicker"},{"attributes":{},"id":"3685","type":"LinearScale"},{"attributes":{},"id":"3692","type":"BasicTicker"},{"attributes":{"data":{"best_rank":[1,1,1,1,1,1,1,2,2,1,1,1,4,4,1,1,3,1,2,5,6,6,5,2,5,4,4,1,5,7,7,4,3,7,4,5,7,5,5,3,7,8,9,9,8,9,9,10,10,10,10],"clusters":["0","2","0","0","1","1","1","0","0","1","1","1","0","0","1","1","0","1","2","0","0","0","2","0","2","2","2","0","2","2","0","0","0","0","0","0","2","1","0","2","0","0","2","0","2","2","0","0","2","2","2"],"earliest_year":["1996","2016","1996","1997","2010","2006","2003","1997","1996","2004","2000","2004","1998","2001","2001","2005","2002","2003","2015","1998","1996","1998","2016","1999","2017","2012","2014","2002","2015","2019","1999","2000","1996","1996","1996","1996","2013","2008","1996","2014","2001","1996","2010","1999","2007","2015","2000","1998","2011","2018","2012"],"players":[" Peter Nicol"," Ali Farag"," Jansher Khan"," Jonathon Power"," Mohamed El Shorbagy"," Ramy Ashour"," Gr\u00e9gory Gaultier"," Ahmed Barada"," Rodney Eyles"," Nick Matthew"," David Palmer"," Amr Shabana"," Paul Johnson"," Stewart Boswell"," Thierry Lincou"," James Willstrop"," Anthony Ricketts"," Karim Darwish"," Karim Abdel Gawad"," Martin Heath"," Del Harris"," Dan Jenson"," Marwan El Shorbagy"," John White"," Paul Coll"," Omar Mosaad"," Tarek Momen"," Lee Beachill"," Miguel \u00c1ngel Rodr\u00edguez"," Diego Elias"," Stefan Casteleyn"," David Evans"," Simon Parke"," Craig Rowland"," Chris Walker"," Brett Martin"," Borja Gol\u00e1n"," Peter Barker"," Anthony Hill"," Simon R\u00f6sner"," Ong Beng Hee"," Mark Chaloner"," Laurens Jan Anjema"," Alex Gough"," Wael El Hindi"," Mathieu Castagnet"," Paul Price"," Derek Ryan"," Mohd Azlan Iskandar"," Mohamed Abouelghar"," Daryl Selby"],"x":{"__ndarray__":"6UhB5GHoHcAojcaOrzkvQAhxhMmXlSrAe75Q4b9tG8AOASrFpo4oQBvkZLzOjx1ALxPLk31nHUAO6vfU9CwmwA8dG7gapirANUJq09W8HEDL7xCNDB/Mv9kilmgKnxJAZ7dMcoAMJcBxKm3pQrIcwK+F3k9H1ti/bVEc7PReHkAFSjlwJ3wKwIkKaXqWYglAwSV0hpFqLkDVFAB8vh4lwNv1fy+daizAekavFo91KMCPNeur0lkvQC7pIBNVpxDAg0B0LyI4MEBDvhjjXWMlQDyycOWA+yxA80dosim/CMB9dG6Vt/EtQMqhZFhrUTFAVMkVDBGUJcAXKWWiX9kgwOOp5jH5FCfAEdjQzrMCLsAQkl77jRcswFtSSztCLSzANiv/PppvI0BdnBIyaMAbQASICBpRAynAyL3SGnkvLUCfgB2hbRgZwPQpCtAPfiTAT7qcoqb1EkBvfrMPI8YjwMBjYNfb1wFAfiUyVJYyKUCWMlEfxOIgwOW6QkXFLSjAlrTPt6K4GEABKc47kQQwQKY50gXm1SBA","dtype":"float64","order":"little","shape":[51]},"y":{"__ndarray__":"ofkUQYeBIUBnMV8nQcTZP9//q5jorf8/AeCpTCB1HUCrWn9nxbkaQL0y7FHO6B9A7P3MIRjMKEDdyNBCLez/P+WHSiogrPI/u0qRU0DFJUCKi1+Y3qUhQG5vrl/kFyBApE74wNj/0b/jcelCScv5v+evDqxlbx1AbDmye2ewIEDifEks4yrpP6rt5HqkFR1Ap1Mg3rsh7z9O88gFDv7vv3BcyoUXOQLA8cI233iqC8CTCwiwp1ADwEcLHJHHBBJAa+Ccz+LmDMBR2fUuU9/+v74Cx6Zmr+K/YizlZZJAAUBUDXYkYdQGwIamBxPlZxvA+sqaOz9iEcBo6ULruDz/v9xojz1/e/0/r+2UtszCD8Bb0+s+NDj3v2EVHNrVsf+/w607OxOuFMAkxvCIYf3xPw418P9/0eu/17JwpFwO2D+axfpGHzQHwBeZ7CHfgP6/ZJqKdM/+HMC+auMmlMMTwMOUF/9N7xHAx4A60xjIG8Dxz99+BbsUwPxBg3QmCBrA8wFFIOlFIMDFDvgl5gYiwMckJHT6Ux3A","dtype":"float64","order":"little","shape":[51]},"years_in_top10":[10,4,3,9,10,11,15,4,3,14,11,11,3,2,10,11,4,10,6,3,2,1,4,7,3,3,5,5,3,1,1,2,5,1,2,2,2,7,3,6,3,3,1,2,3,2,2,1,1,1,2]},"selected":{"id":"3714"},"selection_policy":{"id":"3713"}},"id":"3674","type":"ColumnDataSource"},{"attributes":{},"id":"3679","type":"DataRange1d"},{"attributes":{"formatter":{"id":"3712"},"ticker":{"id":"3688"}},"id":"3687","type":"LinearAxis"},{"attributes":{},"id":"3697","type":"ResetTool"},{"attributes":{"data_source":{"id":"3674"},"glyph":{"id":"3704"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"3705"},"selection_glyph":null,"view":{"id":"3707"}},"id":"3706","type":"GlyphRenderer"},{"attributes":{"factors":["0","1","2"],"palette":["#1f77b4","#ff7f0e","#2ca02c"]},"id":"3675","type":"CategoricalColorMapper"},{"attributes":{"axis":{"id":"3691"},"dimension":1,"grid_line_color":null,"ticker":null},"id":"3694","type":"Grid"}],"root_ids":["3676"]},"title":"Bokeh Application","version":"2.2.3"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('3968').textContent;
                  var render_items = [{"docid":"c67160e1-6f6b-4b1d-b68f-5b2a1204fa65","root_ids":["3676"],"roots":{"3676":"a1121850-5f93-4e82-a97f-6e47190ce637"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>

<html lang="en">
  
  <head>
    
      <meta charset="utf-8" />
      <title>Bokeh Plot</title>
      
      
        
          
        
        
          
        <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js" integrity="sha384-T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM" crossorigin="anonymous"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="bcf7546e-220d-461c-b866-c8bf1b1deab6" data-root-id="3381"></div>
            
          
        
      
      
        <script type="application/json" id="3665">
          {"a027ea3a-ca4e-4258-936c-ac9559330638":{"roots":{"references":[{"attributes":{"background_fill_color":"white","below":[{"id":"3392"}],"center":[{"id":"3395"},{"id":"3399"}],"left":[{"id":"3396"}],"renderers":[{"id":"3411"}],"title":{"id":"3382"},"toolbar":{"id":"3404"},"x_range":{"id":"3384"},"x_scale":{"id":"3388"},"y_range":{"id":"3386"},"y_scale":{"id":"3390"}},"id":"3381","subtype":"Figure","type":"Plot"},{"attributes":{"data_source":{"id":"3379"},"glyph":{"id":"3409"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"3410"},"selection_glyph":null,"view":{"id":"3412"}},"id":"3411","type":"GlyphRenderer"},{"attributes":{},"id":"3402","type":"ResetTool"},{"attributes":{},"id":"3388","type":"LinearScale"},{"attributes":{},"id":"3390","type":"LinearScale"},{"attributes":{"fill_color":{"field":"clusters","transform":{"id":"3380"}},"line_color":{"field":"clusters","transform":{"id":"3380"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3409","type":"Scatter"},{"attributes":{"callback":null,"tooltips":[["Player","@players"],["Best rank","@best_rank"],["Years in Top 10","@years_in_top10"],["Earliest year","@earliest_year"]]},"id":"3401","type":"HoverTool"},{"attributes":{},"id":"3397","type":"BasicTicker"},{"attributes":{},"id":"3393","type":"BasicTicker"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"field":"clusters","transform":{"id":"3380"}},"line_alpha":{"value":0.1},"line_color":{"field":"clusters","transform":{"id":"3380"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3410","type":"Scatter"},{"attributes":{"formatter":{"id":"3415"},"ticker":{"id":"3397"}},"id":"3396","type":"LinearAxis"},{"attributes":{"formatter":{"id":"3417"},"ticker":{"id":"3393"}},"id":"3392","type":"LinearAxis"},{"attributes":{},"id":"3418","type":"UnionRenderers"},{"attributes":{"text":"tSNE of Male Squash Player Rankings"},"id":"3382","type":"Title"},{"attributes":{},"id":"3419","type":"Selection"},{"attributes":{"overlay":{"id":"3403"}},"id":"3400","type":"BoxZoomTool"},{"attributes":{"axis":{"id":"3396"},"dimension":1,"grid_line_color":null,"ticker":null},"id":"3399","type":"Grid"},{"attributes":{"axis":{"id":"3392"},"grid_line_color":null,"ticker":null},"id":"3395","type":"Grid"},{"attributes":{"data":{"best_rank":[1,1,1,1,1,1,1,2,2,1,1,1,4,4,1,1,3,1,2,5,6,6,5,2,5,4,4,1,5,7,7,4,3,7,4,5,7,5,5,3,7,8,9,9,8,9,9,10,10,10,10],"clusters":["0","2","0","0","1","1","1","0","0","1","1","1","0","0","1","1","0","1","2","0","0","0","2","0","2","2","2","0","2","2","0","0","0","0","0","0","2","1","0","2","0","0","2","0","2","2","0","0","2","2","2"],"earliest_year":["1996","2016","1996","1997","2010","2006","2003","1997","1996","2004","2000","2004","1998","2001","2001","2005","2002","2003","2015","1998","1996","1998","2016","1999","2017","2012","2014","2002","2015","2019","1999","2000","1996","1996","1996","1996","2013","2008","1996","2014","2001","1996","2010","1999","2007","2015","2000","1998","2011","2018","2012"],"players":[" Peter Nicol"," Ali Farag"," Jansher Khan"," Jonathon Power"," Mohamed El Shorbagy"," Ramy Ashour"," Gr\u00e9gory Gaultier"," Ahmed Barada"," Rodney Eyles"," Nick Matthew"," David Palmer"," Amr Shabana"," Paul Johnson"," Stewart Boswell"," Thierry Lincou"," James Willstrop"," Anthony Ricketts"," Karim Darwish"," Karim Abdel Gawad"," Martin Heath"," Del Harris"," Dan Jenson"," Marwan El Shorbagy"," John White"," Paul Coll"," Omar Mosaad"," Tarek Momen"," Lee Beachill"," Miguel \u00c1ngel Rodr\u00edguez"," Diego Elias"," Stefan Casteleyn"," David Evans"," Simon Parke"," Craig Rowland"," Chris Walker"," Brett Martin"," Borja Gol\u00e1n"," Peter Barker"," Anthony Hill"," Simon R\u00f6sner"," Ong Beng Hee"," Mark Chaloner"," Laurens Jan Anjema"," Alex Gough"," Wael El Hindi"," Mathieu Castagnet"," Paul Price"," Derek Ryan"," Mohd Azlan Iskandar"," Mohamed Abouelghar"," Daryl Selby"],"x":{"__ndarray__":"A0Agw7mNwEIkWZbBINwWwwpQ2MLE1OfC8YKmwiN27MHR+IDBYyS6wtgOBMOai+LCdYctwm+7UcKSivnCG4zNwiW598I1bu3CvDvGQs3aV8J7XKrCcRujwtVc9EKegQbDlMkFQ9Q+GUNf2eJCjzAAw1cRAENEeBRDhsGjwtozK8JiQirCT66vwvNrccLCBYrCImYkQzPfFEMod2HCvBHMQtonVcIOiXrC+Y9PQ9oSkMKh1VRDhuUrQ1uqgcK0gKbC6MtEQ2rZJkM5Sj9D","dtype":"float32","order":"little","shape":[51]},"y":{"__ndarray__":"ARc5wtM3bsJgsRZDH/oqwkn6BsMVnevCSRHjwl8jCkOeFQxD0tzWwhqNoMIenczC+ob8QrEz3UKbIJDCAObqwvGuT8GW27bCB3qTwhc8AENlRhhD/2oKQw2kh8ICvArCDwl+wjKj2MIF9qXCLfLAwSaEnsIm5HPCO+X9QqHLu0IsUTlD1oMjQ9sCK0OtnDFDW53LwtDi9sIl+zdDVweuwi56o0ILV0hCt6zPwhZpdkIcE+fC6Myown3vikItEYdCIUPbwj9skcKE2sTC","dtype":"float32","order":"little","shape":[51]},"years_in_top10":[10,4,3,9,10,11,15,4,3,14,11,11,3,2,10,11,4,10,6,3,2,1,4,7,3,3,5,5,3,1,1,2,5,1,2,2,2,7,3,6,3,3,1,2,3,2,2,1,1,1,2]},"selected":{"id":"3419"},"selection_policy":{"id":"3418"}},"id":"3379","type":"ColumnDataSource"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"3400"},{"id":"3401"},{"id":"3402"}]},"id":"3404","type":"Toolbar"},{"attributes":{"source":{"id":"3379"}},"id":"3412","type":"CDSView"},{"attributes":{},"id":"3415","type":"BasicTickFormatter"},{"attributes":{},"id":"3384","type":"DataRange1d"},{"attributes":{},"id":"3417","type":"BasicTickFormatter"},{"attributes":{"bottom_units":"screen","fill_alpha":0.5,"fill_color":"lightgrey","left_units":"screen","level":"overlay","line_alpha":1.0,"line_color":"black","line_dash":[4,4],"line_width":2,"right_units":"screen","top_units":"screen"},"id":"3403","type":"BoxAnnotation"},{"attributes":{"factors":["0","1","2"],"palette":["#1f77b4","#ff7f0e","#2ca02c"]},"id":"3380","type":"CategoricalColorMapper"},{"attributes":{},"id":"3386","type":"DataRange1d"}],"root_ids":["3381"]},"title":"Bokeh Application","version":"2.2.3"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('3665').textContent;
                  var render_items = [{"docid":"a027ea3a-ca4e-4258-936c-ac9559330638","root_ids":["3381"],"roots":{"3381":"bcf7546e-220d-461c-b866-c8bf1b1deab6"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>

<html lang="en">
  
  <head>
    
      <meta charset="utf-8" />
      <title>Bokeh Plot</title>
      
      
        
          
        
        
          
        <script type="text/javascript" src="https://cdn.bokeh.org/bokeh/release/bokeh-2.2.3.min.js" integrity="sha384-T2yuo9Oe71Cz/I4X9Ac5+gpEa5a8PpJCDlqKYO0CfAuEszu1JrXLl8YugMqYe3sM" crossorigin="anonymous"></script>
        <script type="text/javascript">
            Bokeh.set_log_level("info");
        </script>
        
      
      
    
  </head>
  
  
  <body>
    
      
        
          
          
            
              <div class="bk-root" id="77dbe8d8-934a-4568-938e-5fd1ff971c0c" data-root-id="3094"></div>
            
          
        
      
      
        <script type="application/json" id="3370">
          {"6ecdfff2-0dec-4bda-b5d9-aa2a2108e600":{"roots":{"references":[{"attributes":{"background_fill_color":"white","below":[{"id":"3105"}],"center":[{"id":"3108"},{"id":"3112"}],"left":[{"id":"3109"}],"renderers":[{"id":"3124"}],"title":{"id":"3095"},"toolbar":{"id":"3117"},"x_range":{"id":"3097"},"x_scale":{"id":"3101"},"y_range":{"id":"3099"},"y_scale":{"id":"3103"}},"id":"3094","subtype":"Figure","type":"Plot"},{"attributes":{"data":{"best_rank":[1,1,1,1,1,1,1,2,2,1,1,1,4,4,1,1,3,1,2,5,6,6,5,2,5,4,4,1,5,7,7,4,3,7,4,5,7,5,5,3,7,8,9,9,8,9,9,10,10,10,10],"clusters":["0","2","0","0","1","1","1","0","0","1","1","1","0","0","1","1","0","1","2","0","0","0","2","0","2","2","2","0","2","2","0","0","0","0","0","0","2","1","0","2","0","0","2","0","2","2","0","0","2","2","2"],"earliest_year":["1996","2016","1996","1997","2010","2006","2003","1997","1996","2004","2000","2004","1998","2001","2001","2005","2002","2003","2015","1998","1996","1998","2016","1999","2017","2012","2014","2002","2015","2019","1999","2000","1996","1996","1996","1996","2013","2008","1996","2014","2001","1996","2010","1999","2007","2015","2000","1998","2011","2018","2012"],"players":[" Peter Nicol"," Ali Farag"," Jansher Khan"," Jonathon Power"," Mohamed El Shorbagy"," Ramy Ashour"," Gr\u00e9gory Gaultier"," Ahmed Barada"," Rodney Eyles"," Nick Matthew"," David Palmer"," Amr Shabana"," Paul Johnson"," Stewart Boswell"," Thierry Lincou"," James Willstrop"," Anthony Ricketts"," Karim Darwish"," Karim Abdel Gawad"," Martin Heath"," Del Harris"," Dan Jenson"," Marwan El Shorbagy"," John White"," Paul Coll"," Omar Mosaad"," Tarek Momen"," Lee Beachill"," Miguel \u00c1ngel Rodr\u00edguez"," Diego Elias"," Stefan Casteleyn"," David Evans"," Simon Parke"," Craig Rowland"," Chris Walker"," Brett Martin"," Borja Gol\u00e1n"," Peter Barker"," Anthony Hill"," Simon R\u00f6sner"," Ong Beng Hee"," Mark Chaloner"," Laurens Jan Anjema"," Alex Gough"," Wael El Hindi"," Mathieu Castagnet"," Paul Price"," Derek Ryan"," Mohd Azlan Iskandar"," Mohamed Abouelghar"," Daryl Selby"],"x":{"__ndarray__":"GLQIQcz3QkCSxxtB6B8GQccG4EBGyeNAxJ/9QOyNF0HgGhpB00P4QG0w9kAOKuxAFyUQQUSECkG12/9ACLXtQDC6AEF59/BAyAU/QKp2DUENexFBuV8LQRiLUkDFNv9AoG9CQDgXhUAoaGFAVCEBQRw2WUBqW1tAC64HQQoHB0F3KRVBTRcNQS83GEHvrhNBbaKLQOOzq0CUZQlB5hdjQIhJ/EAjpv5AkcKeQKmQ+UAqc6pAY/6HQKQnAEG28QNBhqqTQIsEd0Aa+5JA","dtype":"float32","order":"little","shape":[51]},"y":{"__ndarray__":"kzI6PqVWgEBAXaO/q1MVP9pCJkDfQA1AnwfxP5QVYr/DpLC/oMoBQN61iT9XYPQ/NMCPvz2TLr9LLJI/fKYWQKlzJL5WHMs/p/6FQHqyrr8gnQfAURrbv8lne0AYIAc/i4xkQMjJaUDGV4xAuf0wPtOae0A6VltAvF3Tv91whL9x7M6/gdQRwPwgCcAPwe+/BmCJQIxSX0BNcA/AXwZlQGJDh79nA/u//UCJQOzC0b/58IJAmqd9QE8qwb+aUvW/XamNQP/VgUC0hYBA","dtype":"float32","order":"little","shape":[51]},"years_in_top10":[10,4,3,9,10,11,15,4,3,14,11,11,3,2,10,11,4,10,6,3,2,1,4,7,3,3,5,5,3,1,1,2,5,1,2,2,2,7,3,6,3,3,1,2,3,2,2,1,1,1,2]},"selected":{"id":"3132"},"selection_policy":{"id":"3131"}},"id":"3092","type":"ColumnDataSource"},{"attributes":{"fill_alpha":{"value":0.1},"fill_color":{"field":"clusters","transform":{"id":"3093"}},"line_alpha":{"value":0.1},"line_color":{"field":"clusters","transform":{"id":"3093"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3123","type":"Scatter"},{"attributes":{"data_source":{"id":"3092"},"glyph":{"id":"3122"},"hover_glyph":null,"muted_glyph":null,"nonselection_glyph":{"id":"3123"},"selection_glyph":null,"view":{"id":"3125"}},"id":"3124","type":"GlyphRenderer"},{"attributes":{},"id":"3110","type":"BasicTicker"},{"attributes":{},"id":"3099","type":"DataRange1d"},{"attributes":{},"id":"3131","type":"UnionRenderers"},{"attributes":{},"id":"3115","type":"ResetTool"},{"attributes":{},"id":"3128","type":"BasicTickFormatter"},{"attributes":{},"id":"3130","type":"BasicTickFormatter"},{"attributes":{"text":"UMAP of Male Squash Player Rankings"},"id":"3095","type":"Title"},{"attributes":{},"id":"3103","type":"LinearScale"},{"attributes":{"formatter":{"id":"3128"},"ticker":{"id":"3110"}},"id":"3109","type":"LinearAxis"},{"attributes":{},"id":"3106","type":"BasicTicker"},{"attributes":{"active_drag":"auto","active_inspect":"auto","active_multi":null,"active_scroll":"auto","active_tap":"auto","tools":[{"id":"3113"},{"id":"3114"},{"id":"3115"}]},"id":"3117","type":"Toolbar"},{"attributes":{},"id":"3097","type":"DataRange1d"},{"attributes":{},"id":"3101","type":"LinearScale"},{"attributes":{"axis":{"id":"3105"},"grid_line_color":null,"ticker":null},"id":"3108","type":"Grid"},{"attributes":{"overlay":{"id":"3116"}},"id":"3113","type":"BoxZoomTool"},{"attributes":{"formatter":{"id":"3130"},"ticker":{"id":"3106"}},"id":"3105","type":"LinearAxis"},{"attributes":{"bottom_units":"screen","fill_alpha":0.5,"fill_color":"lightgrey","left_units":"screen","level":"overlay","line_alpha":1.0,"line_color":"black","line_dash":[4,4],"line_width":2,"right_units":"screen","top_units":"screen"},"id":"3116","type":"BoxAnnotation"},{"attributes":{"source":{"id":"3092"}},"id":"3125","type":"CDSView"},{"attributes":{},"id":"3132","type":"Selection"},{"attributes":{"axis":{"id":"3109"},"dimension":1,"grid_line_color":null,"ticker":null},"id":"3112","type":"Grid"},{"attributes":{"factors":["0","1","2"],"palette":["#1f77b4","#ff7f0e","#2ca02c"]},"id":"3093","type":"CategoricalColorMapper"},{"attributes":{"callback":null,"tooltips":[["Player","@players"],["Best rank","@best_rank"],["Years in Top 10","@years_in_top10"],["Earliest year","@earliest_year"]]},"id":"3114","type":"HoverTool"},{"attributes":{"fill_color":{"field":"clusters","transform":{"id":"3093"}},"line_color":{"field":"clusters","transform":{"id":"3093"}},"x":{"field":"x"},"y":{"field":"y"}},"id":"3122","type":"Scatter"}],"root_ids":["3094"]},"title":"Bokeh Application","version":"2.2.3"}}
        </script>
        <script type="text/javascript">
          (function() {
            var fn = function() {
              Bokeh.safely(function() {
                (function(root) {
                  function embed_document(root) {
                    
                  var docs_json = document.getElementById('3370').textContent;
                  var render_items = [{"docid":"6ecdfff2-0dec-4bda-b5d9-aa2a2108e600","root_ids":["3094"],"roots":{"3094":"77dbe8d8-934a-4568-938e-5fd1ff971c0c"}}];
                  root.Bokeh.embed.embed_items(docs_json, render_items);
                
                  }
                  if (root.Bokeh !== undefined) {
                    embed_document(root);
                  } else {
                    var attempts = 0;
                    var timer = setInterval(function(root) {
                      if (root.Bokeh !== undefined) {
                        clearInterval(timer);
                        embed_document(root);
                      } else {
                        attempts++;
                        if (attempts > 100) {
                          clearInterval(timer);
                          console.log("Bokeh: ERROR: Unable to run BokehJS code because BokehJS library is missing");
                        }
                      }
                    }, 10, root)
                  }
                })(window);
              });
            };
            if (document.readyState != "loading") fn();
            else document.addEventListener("DOMContentLoaded", fn);
          })();
        </script>
    
  </body>
  
</html>

<p>So what’s so amazing about these? They look basically the same. The big advantage is in their interactivity! Try hovering your mouse over the points. What happens?</p>

<p>Cool, right! I can’t stop being amazed at this. It makes the chart so much more informative and greatly aids in data exploration. For example, I can now just hover over the anomalous orange dot to find out what is going on: it is Peter Barker who has had a long successful career like the other orange dots, but has never actually reached the top few spots in the world rankings like the green dots. Another example is that I can understand some of the nuanced structure in the charts and find sub-clusters. For example, in the PCA diagram, the four points that are the highest in the green cluster form a sub-cluster of the players Ali Farag, Karim Abdel Gawad, Tarek Momen and Simon Rosner, who are excellent players who are consistently in the Top 5, but not legendary. (Ali Farag is an exception to this - he will undoubtedly join the orange cluster but his career is too short at the moment to join the cluster of elite players).</p>

<h2 id="conclusion">Conclusion</h2>
<p>Bokeh is amazing! This is what I managed to do after less than an hour of Googling around and adapting examples. If you look at the documentation, there is a whole load of other things that are possible, e.g. different kinds of interactivity.</p>

<p>I highly recommend you try using it.</p>

<h2 id="code">Code</h2>
<p>Below is a sample of the code to illustrate how to create the Bokeh plots above.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># import packages
</span><span class="kn">from</span> <span class="nn">sklearn.manifold</span> <span class="kn">import</span> <span class="n">TSNE</span>
<span class="kn">from</span> <span class="nn">sklearn.cluster</span> <span class="kn">import</span> <span class="n">KMeans</span>

<span class="kn">from</span> <span class="nn">bokeh.plotting</span> <span class="kn">import</span> <span class="n">figure</span><span class="p">,</span> <span class="n">output_notebook</span><span class="p">,</span> <span class="n">show</span><span class="p">,</span> <span class="n">save</span><span class="p">,</span> <span class="n">ColumnDataSource</span>
<span class="kn">from</span> <span class="nn">bokeh.models</span> <span class="kn">import</span> <span class="n">HoverTool</span><span class="p">,</span> <span class="n">CategoricalColorMapper</span>
<span class="kn">from</span> <span class="nn">bokeh.palettes</span> <span class="kn">import</span> <span class="n">d3</span>
<span class="kn">from</span> <span class="nn">bokeh.transform</span> <span class="kn">import</span> <span class="n">factor_cmap</span>
<span class="n">output_notebook</span><span class="p">()</span>

<span class="c1"># use tsne to do dimension reduction
</span><span class="n">tsne_model</span> <span class="o">=</span> <span class="n">TSNE</span><span class="p">(</span><span class="n">n_components</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">perplexity</span> <span class="o">=</span> <span class="mi">5</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">male_tsne</span> <span class="o">=</span> <span class="n">tsne_model</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">players_m</span><span class="p">)</span>

<span class="c1"># use kmeans on original dataset to determine clusters
</span><span class="n">kmeans</span> <span class="o">=</span> <span class="n">KMeans</span><span class="p">(</span><span class="n">n_clusters</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">players_m</span><span class="p">)</span>

<span class="c1"># create the bokeh plot
</span><span class="n">source</span> <span class="o">=</span> <span class="n">ColumnDataSource</span><span class="p">(</span>
    <span class="n">data</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">male_tsne</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span>
        <span class="n">y</span><span class="o">=</span><span class="n">male_tsne</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">],</span>
        <span class="n">clusters</span> <span class="o">=</span> <span class="p">[</span><span class="n">f</span><span class="s">'{i}'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">kmeans</span><span class="o">.</span><span class="n">labels_</span><span class="p">],</span>
        <span class="n">players</span><span class="o">=</span><span class="n">players_m</span><span class="o">.</span><span class="n">index</span><span class="p">,</span>
        <span class="n">best_rank</span><span class="o">=</span><span class="n">players_m</span><span class="o">.</span><span class="n">best_rank</span><span class="p">,</span>
        <span class="n">years_in_top10</span><span class="o">=</span><span class="n">players_m</span><span class="o">.</span><span class="n">years_in_top10</span><span class="p">,</span>
        <span class="n">earliest_year</span><span class="o">=</span><span class="n">players_m</span><span class="o">.</span><span class="n">earliest_year</span>
    <span class="p">)</span>
<span class="p">)</span>

<span class="n">palette</span> <span class="o">=</span> <span class="n">d3</span><span class="p">[</span><span class="s">'Category10'</span><span class="p">][</span><span class="mi">3</span><span class="p">]</span>
<span class="n">color_map</span> <span class="o">=</span> <span class="n">CategoricalColorMapper</span><span class="p">(</span><span class="n">factors</span><span class="o">=</span><span class="p">[</span><span class="n">f</span><span class="s">'{i}'</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)],</span>
                                   <span class="n">palette</span><span class="o">=</span><span class="n">palette</span><span class="p">)</span>

<span class="n">TOOLS</span><span class="o">=</span><span class="s">"box_zoom,hover,reset"</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">figure</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s">"PCA of Male Squash Player Rankings"</span><span class="p">,</span> <span class="n">tools</span><span class="o">=</span><span class="n">TOOLS</span><span class="p">)</span>
<span class="n">p</span><span class="o">.</span><span class="n">background_fill_color</span> <span class="o">=</span> <span class="s">"white"</span>
<span class="n">p</span><span class="o">.</span><span class="n">xgrid</span><span class="o">.</span><span class="n">grid_line_color</span> <span class="o">=</span> <span class="bp">None</span>
<span class="n">p</span><span class="o">.</span><span class="n">ygrid</span><span class="o">.</span><span class="n">grid_line_color</span> <span class="o">=</span> <span class="bp">None</span>

<span class="n">p</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">x</span><span class="o">=</span><span class="s">'x'</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s">'y'</span><span class="p">,</span>
          <span class="n">color</span><span class="o">=</span><span class="p">{</span><span class="s">'field'</span><span class="p">:</span> <span class="s">'clusters'</span><span class="p">,</span> <span class="s">'transform'</span><span class="p">:</span> <span class="n">color_map</span><span class="p">},</span>
          <span class="n">source</span><span class="o">=</span><span class="n">source</span><span class="p">)</span>

<span class="n">hover</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="nb">dict</span><span class="p">(</span><span class="nb">type</span><span class="o">=</span><span class="n">HoverTool</span><span class="p">))</span>
<span class="n">hover</span><span class="o">.</span><span class="n">tooltips</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s">"Player"</span><span class="p">,</span> <span class="s">"@players"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Best rank"</span><span class="p">,</span> <span class="s">"@best_rank"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Years in Top 10"</span><span class="p">,</span> <span class="s">"@years_in_top10"</span><span class="p">),</span>
    <span class="p">(</span><span class="s">"Earliest year"</span><span class="p">,</span> <span class="s">"@earliest_year"</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">show</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>
<span class="n">save</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">filename</span> <span class="o">=</span> <span class="s">'squash3_malepca.html'</span><span class="p">)</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="python" /><category term="data science" /><category term="data viz" /><summary type="html"><![CDATA[Bokeh is amazing! I learnt about it earlier this week and I want to illustrate its prowess by remaking plots from Part II series using Bokeh.]]></summary></entry><entry><title type="html">Visualising L1 and L2 regularisation, Part II, Lessons learnt from an experienced programmer</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/10/18/l1l2reg2.html" rel="alternate" type="text/html" title="Visualising L1 and L2 regularisation, Part II, Lessons learnt from an experienced programmer" /><published>2020-10-18T00:00:00-05:00</published><updated>2020-10-18T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/10/18/l1l2reg2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/10/18/l1l2reg2.html"><![CDATA[<h2 id="other-posts-in-series">Other posts in series</h2>

<ul>
  <li><a href="/blog/data%20science/python/2020/10/11/l1l2reg.html">Visualising L1 and L2 regularisation</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In the first post in this series, I produced various animations to help visualise L1 vs L2 regularisation. However, the way I produced those animations was not 100% programmatic. This is what I did:</p>
<ul>
  <li>Have a for loop which produces each chart then saves it as a png file.</li>
  <li>Manually use an online gif tool to combine the png files.</li>
</ul>

<p>However, I knew it was possible to create a video programmatically in matplotlib, as I had done it before to <a href="/blog/data%20science/python/2020/09/10/sgd1.html">visualise gradient descent</a>. But I could not work out how to adapt the functions to this case.  Therefore, I decided to ask for help in the Faculty Slack channel. I got two helpful responses.</p>

<ul>
  <li>One by Will Fawcett which told me about the command line tool <code class="highlighter-rouge">convert</code> that can create the gifs</li>
  <li>Second by Tom Begley who recommended using <code class="highlighter-rouge">FFMpegWriter</code>. Furthermore, he actually created a pull request in which he adapted my code to illustrate how to use it!</li>
</ul>

<p>This was the first time I had asked for help in the general Slack channel, so I was taken aback by the help that was provided.</p>

<p>Anyway, in the pull request, in addition to illustrating how to use <code class="highlighter-rouge">FFMpegWriter</code>, there were various other little things that were changed and things I could learn from. Therefore, I am writing this blogpost to maximise how much I learn from the experience.</p>

<h2 id="lessons-learnt">Lessons learnt</h2>
<h3 id="automatic-code-formatting-for-notebooks">Automatic code formatting for notebooks</h3>
<p>A noticeable change in the pull request was that many of the changes concerned code formatting. Below is an example.</p>

<p>Before:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_cost_fn</span><span class="p">(</span><span class="n">centre_x</span><span class="p">,</span> <span class="n">centre_y</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>  <span class="n">reg_const</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">"""
    returns a convex cost function
    """</span>
    <span class="k">if</span> <span class="n">reg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">'l1'</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">reg_const</span><span class="o">*</span><span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">'l2'</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">reg_const</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">'max'</span><span class="p">:</span>
        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span> <span class="o">+</span> <span class="n">reg_const</span><span class="o">*</span><span class="p">(</span><span class="n">x</span><span class="o">**</span><span class="mi">10</span> <span class="o">+</span> <span class="n">y</span><span class="o">**</span><span class="mi">10</span><span class="p">)</span><span class="o">**</span><span class="mf">0.1</span>

    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div>

<p>After:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_cost_fn</span><span class="p">(</span><span class="n">centre_x</span><span class="p">,</span> <span class="n">centre_y</span><span class="p">,</span> <span class="n">reg</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">reg_const</span><span class="o">=</span><span class="mi">0</span><span class="p">):</span>
    <span class="s">"""
    returns a convex cost function
    """</span>
    <span class="k">if</span> <span class="n">reg</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">"l1"</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="n">reg_const</span> <span class="o">*</span> <span class="p">(</span><span class="nb">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="nb">abs</span><span class="p">(</span><span class="n">y</span><span class="p">))</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">"l2"</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="n">reg_const</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>
            <span class="p">)</span>

    <span class="k">elif</span> <span class="n">reg</span> <span class="o">==</span> <span class="s">"max"</span><span class="p">:</span>

        <span class="k">def</span> <span class="nf">cost</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
            <span class="k">return</span> <span class="p">(</span>
                <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">centre_x</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">centre_y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
                <span class="o">+</span> <span class="n">reg_const</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">**</span> <span class="mi">10</span> <span class="o">+</span> <span class="n">y</span> <span class="o">**</span> <span class="mi">10</span><span class="p">)</span> <span class="o">**</span> <span class="mf">0.1</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">cost</span>
</code></pre></div></div>

<p>I knew there was no way Tom did this manually so surmised he used an automatic code formatter. A quick Google search revealed various options and I will be sure to make use of these in the future.</p>

<h3 id="decorators">Decorators</h3>
<p>I already knew about decorators, but somehow never thought of making use of them.</p>

<p>Before:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_y_l1_coordinate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
<span class="o">...</span>
<span class="n">create_y_l1_coordinates</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vectorize</span><span class="p">(</span><span class="n">create_y_l1_coordinate</span><span class="p">)</span>
</code></pre></div></div>

<p>After:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">np</span><span class="o">.</span><span class="n">vectorize</span>
<span class="k">def</span> <span class="nf">create_y_l1_coordinate</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">r</span><span class="p">):</span>
<span class="o">...</span>
</code></pre></div></div>

<h3 id="garbage-collection">Garbage collection</h3>
<p>Garbage collection is something I have briefly read about, but have not fully understood. However, a certain aspect of Tom’s example gave me some insight. It is best explained by showing the example. Tom created the <code class="highlighter-rouge">update</code> function within an <code class="highlighter-rouge">initialise</code> function:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">initialise</span><span class="p">(</span><span class="n">angles</span><span class="p">,</span> <span class="n">ball_radius</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">norm</span><span class="o">=</span><span class="s">"l1"</span><span class="p">):</span>
    <span class="n">centres_x</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
    <span class="n">centres_y</span> <span class="o">=</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>

    <span class="n">f</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>

    <span class="n">x_ball</span><span class="p">,</span> <span class="n">y_ball</span> <span class="o">=</span> <span class="n">create_ball_boundary</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="s">"l1"</span><span class="p">)</span>
    <span class="c1"># comma is needed to pull poly out of list length 1
</span>    <span class="n">ax</span><span class="o">.</span><span class="n">fill</span><span class="p">(</span><span class="n">x_ball</span><span class="p">,</span> <span class="n">y_ball</span><span class="p">,</span> <span class="s">"b"</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>

    <span class="p">(</span><span class="n">marker</span><span class="p">,)</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="mi">0</span><span class="p">,</span>
        <span class="n">marker</span><span class="o">=</span><span class="s">"o"</span><span class="p">,</span>
        <span class="n">color</span><span class="o">=</span><span class="s">"red"</span><span class="p">,</span>
        <span class="n">label</span><span class="o">=</span><span class="s">"Minimum value of cost within shaded region"</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
        <span class="n">cost</span> <span class="o">=</span> <span class="n">create_cost_fn</span><span class="p">(</span><span class="n">centres_x</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">centres_y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
        <span class="p">(</span>
            <span class="n">cost_inputs_x</span><span class="p">,</span>
            <span class="n">cost_inputs_y</span><span class="p">,</span>
            <span class="n">cost_outputs</span><span class="p">,</span>
        <span class="p">)</span> <span class="o">=</span> <span class="n">create_cost_inputs_outputs</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
        <span class="n">cont</span> <span class="o">=</span> <span class="n">ax</span><span class="o">.</span><span class="n">contour</span><span class="p">(</span>
            <span class="n">cost_inputs_x</span><span class="p">,</span>
            <span class="n">cost_inputs_y</span><span class="p">,</span>
            <span class="n">cost_outputs</span><span class="p">,</span>
            <span class="n">levels</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">51</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="n">x_min</span><span class="p">,</span> <span class="n">y_min</span> <span class="o">=</span> <span class="n">minimise_cost</span><span class="p">(</span><span class="n">cost</span><span class="p">,</span> <span class="n">x_ball</span><span class="p">,</span> <span class="n">y_ball</span><span class="p">)</span>
        <span class="n">marker</span><span class="o">.</span><span class="n">set_xdata</span><span class="p">(</span><span class="n">x_min</span><span class="p">)</span>
        <span class="n">marker</span><span class="o">.</span><span class="n">set_ydata</span><span class="p">(</span><span class="n">y_min</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">cont</span>

    <span class="k">return</span> <span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">update</span>
</code></pre></div></div>

<p>Before seeing this example, I would have said that <code class="highlighter-rouge">centres_x</code>, <code class="highlighter-rouge">centres_y</code> and <code class="highlighter-rouge">marker</code> would be ‘garbage collected’ and the variables would not be preserved. However, the fact the program works means that those variables must be preserved. Looking at the code, the only way this is possible is because they are used in the <code class="highlighter-rouge">update</code> function (and the fact that <code class="highlighter-rouge">update</code> is returned by <code class="highlighter-rouge">initialise</code>).</p>

<p>Though I cannot say I fully understand what is going on, I do have a better appreciation for garbage collection in python, and how there is a counter that tracks the number of things that are somehow using/referring to a python object.</p>

<p>Lastly, this example highlights some things I have read about bad programming practice. If something went wrong, the update function would be hard to debug, as it is making to changes to variables that are not parameters to the function.  I do not mind it in this example, as Tom was trying to illustrate how to use FFMpegWriter and it is not worth his time to use optimal programming practice in some little learning project. Furthermore, it has indirectly increased the amount I have learnt from the experience!</p>

<h3 id="how-to-use-ffmpegwriter">How to use <code class="highlighter-rouge">FFMpegWriter</code></h3>
<p>Of course, the main thing I learnt from Tom’s help was how to use <code class="highlighter-rouge">FFMpegWriter</code> and adjust for the different nature of these plots compared to other animation examples I had seen/created previously.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">f</span><span class="p">,</span> <span class="n">ax</span><span class="p">,</span> <span class="n">update</span> <span class="o">=</span> <span class="n">initialise</span><span class="p">(</span><span class="n">angles</span><span class="p">)</span>
<span class="n">writer</span> <span class="o">=</span> <span class="n">FFMpegWriter</span><span class="p">(</span><span class="n">fps</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>

<span class="k">with</span> <span class="n">writer</span><span class="o">.</span><span class="n">saving</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="s">"l1-cost.mp4"</span><span class="p">,</span> <span class="mi">100</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">angles</span><span class="p">)):</span>
        <span class="n">cont</span> <span class="o">=</span> <span class="n">update</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
        <span class="n">writer</span><span class="o">.</span><span class="n">grab_frame</span><span class="p">()</span>
        <span class="n">remove_contours</span><span class="p">(</span><span class="n">cont</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="how-how-i-might-have-found-the-solution-myself">How how I might have found the solution myself</h3>
<p>I asked Tom: ‘How would I have found out about the syntax to remove the contours, given that standard animation examples use ‘updates’ (e.g. you use updates for the red marker).  What would I have googled for, or what documentation would I have read?’</p>

<p>Their response: ‘Yeah that was the nastiest bit here. Normally I look at the return value of whatever plotting function I used and try to tab-complete <code class="highlighter-rouge">.set_</code> to see what’s available. In the case of the contour there was nothing obvious so I googled something like “update matplotlib contour animation” and got <a href="https://stackoverflow.com/questions/23250004/updating-contours-for-matplotlib-animation">this stackoverflow answer</a> which told me I can’t update contours, I have to remove them instead.’</p>

<p>Nothing mind-blowing, but I always find it useful to find out how other people solve their problems. I cannot remember what I searched for when I was trying to solve the problem for myself, but I guess I must not have focussed my search on the key aspect of my plots that were causing the problems, namely, the contours.</p>

<h3 id="goal-for-the-future">Goal for the future</h3>
<p>To guage where I am at, I asked Tom how long it took them to create the pull request. The answer was roughly 30 minutes! For me, that is incredibly fast. It is encouraging to see what I can strive for and what I will be able to achieve with continued practice and experience.</p>

<h2 id="conclusion">Conclusion</h2>
<p>As you can see, I learnt a great deal from this experience. The main lesson of course is to learn from others. With enough effort, I probably could have arrived at a solution myself, however, it is much faster this way <em>and</em> I learnt a whole bunch of other little things on the side.</p>

<h2 id="github-repository">Github Repository</h2>
<p>Here is the link to the <a href="https://github.com/Lovkush-A/l1l2_regularisation">Github repository</a> in case you’re interested in looking at the full code and comparing things for yourself.</p>]]></content><author><name></name></author><category term="data science" /><category term="python" /><summary type="html"><![CDATA[The process I used to make the animations was inefficient and not programmatic. I could not work out how to adapt the matplotlib animation tools to my situation so I asked for help. Here I describe what I learnt from the help that I received.]]></summary></entry><entry><title type="html">Visualising L1 and L2 regularisation</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg.html" rel="alternate" type="text/html" title="Visualising L1 and L2 regularisation" /><published>2020-10-11T00:00:00-05:00</published><updated>2020-10-11T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg.html"><![CDATA[<h2 id="other-posts-in-series">Other posts in series</h2>

<ul>
  <li><a href="/blog/data%20science/python/2020/10/18/l1l2reg2.html">Visualising L1 and L2 regularisation, Part II, Lessons learnt from an experienced programmer</a></li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>In this <a href="https://medium.com/@davidsotunbo/ridge-and-lasso-regression-an-illustration-and-explanation-using-sklearn-in-python-4853cd543898">medium post comparing L1 and L2 regularisation</a> there is an image showing how L1 regularisation is more likely to make one of the parameters equal to zero than L2 regularisation.</p>

<p>One of my <a href="https://faculty.ai/fellowship/">co-fellows at Faculty</a> pointed out that this image is not convincing, because it could just be a case of a cherry-picked cost function. As I had never made any effort to properly understand L1 versus L2 regularisation previously, this was good motivation for me to better to understand.</p>

<p>The results are bunch of visuals that are below.</p>

<h2 id="varying-cost-function-with-parameters-restricted-to-l1-or-l2-balls">Varying cost function with parameters restricted to L1 or L2 balls</h2>

<h3 id="parameters-restricted-to-l1-ball">Parameters restricted to L1 ball</h3>

<p><img src="/blog/images/l1l2reg_l11.gif" alt="image" /></p>

<h3 id="parameters-restricted-to-l2-ball">Parameters restricted to L2 ball</h3>

<p><img src="/blog/images/l1l2reg_l21.gif" alt="image" /></p>

<p>In the plots above:</p>
<ul>
  <li>The circular things that are moving around are contour plots of cost functions. They are all convex.</li>
  <li>The shaded regions are L1 and L2 balls, i.e. all points where the L1 or L2 norm of the parameters are less than some fixed radius r.</li>
  <li>The red dot is the parameter which minimizes the cost function, given the restriction of being within the ballh.</li>
</ul>

<p>What can be seen is that restricting the parameters to an L1-ball results in one of the two paramaters being zero, most of the time.  The L2-ball has no preference for values.</p>

<p>This matches the general descriptions I have seen of L1 regularisation in various blog posts and articles.</p>

<h2 id="varying-cost-functions-with-l1-or-l2-regularisations">Varying cost functions with L1 or L2 regularisations</h2>
<p>An issue with the above plots is that I have forced my parameters to be within an L1 or L2 ball. In regularisation, the parameters can have any value, but there is a regularisation term added to incentivise the model to reduce the L1 or L2 norm.</p>

<p>(Having this forced restriction corresponds to having a regularisation term that is zero if the parameters are inside the ball and infinity if the parameters are outside the ball. So normal regularisation can be thought of as being a smoothed out version of this forced restriction.)</p>

<p>To check that the insights gained in the above plots do work when we have regularisation I created a couple more plots.</p>

<ul>
  <li>I created a cost function <code class="highlighter-rouge">(x-x0)^2 + (y-y0))^2 + r*norm((x,y))</code></li>
  <li><code class="highlighter-rouge">r</code> is a regularisation constant.</li>
  <li><code class="highlighter-rouge">norm</code> is the L1 norm in the first plot and the L2 norm in the second plot.</li>
  <li>I determined the coordinates <code class="highlighter-rouge">(x', y')</code> that minimised the cost function above.</li>
  <li>I added those coordinates to a scatterplot.</li>
  <li>I then varied <code class="highlighter-rouge">x0</code> and <code class="highlighter-rouge">y0</code>, producing many cost functions, and plotting the resulting <code class="highlighter-rouge">(x', y')</code> coordinates in the scatterplot.</li>
</ul>

<h3 id="optimal-parameters-with-l1-regularisation">Optimal parameters with L1 regularisation</h3>

<p><img src="/blog/images/l1l2reg_l12.png" alt="image" /></p>

<h3 id="optimal-parameters-with-l2-regularisation">Optimal parameters with L2 regularisation</h3>

<p><img src="/blog/images/l1l2reg_l22.png" alt="image" /></p>

<p>We can see in the plots above that the pattern continues. L1 regularisation will force parameters to zero, and L2 regularisation does not have any preferred direction - L2 just wants the length of the vector to be smaller.</p>

<h2 id="max-norm">Max norm</h2>
<p>To check your understanding, imagine how the plots above would look if we replaced the L1 and L2 norms with the max norm, <code class="highlighter-rouge">max_norm((x,y)) = max(|x|, |y|)</code>.</p>

<p>No really, take a couple of minutes to think this through. You learn the most by actively engaging with the ideas rather than passively reading somebody else’s thoughts.</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>Well, here are the two plots. Minor note, the plots are for the L10 norm, but it is a good enough approximation to the max-norm.</p>

<p><img src="/blog/images/l1l2reg_linf1.gif" alt="image" /></p>

<p><img src="/blog/images/l1l2reg_linf2.png" alt="image" /></p>

<h2 id="conclusions">Conclusions</h2>
<p>I am glad I produced these plots. I have read before that L1-regularisation ought to have parameters go to zero, but I never really understood it, but now I have some feel for it.  Also, it was good python practice.</p>]]></content><author><name></name></author><category term="data science" /><category term="python" /><summary type="html"><![CDATA[I create various charts to help visualise the difference between L1 and L2 regularisation. The pattern is clear and L1 regularisation does tend to force parameters to zero.]]></summary></entry><entry><title type="html">Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case" /><published>2020-10-01T00:00:00-05:00</published><updated>2020-10-01T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4.html"><![CDATA[<h2 id="other-posts-in-series">Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and <strong>S</strong>GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/python/2020/09/10/sgd1.html">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</a></p>
  </li>
</ul>

<h2 id="introduction">Introduction</h2>
<p>I will recap my investigations into fitting sinusoidal data using sinusoidal models with SGD.</p>
<ul>
  <li>In Part I, I described my attempt at using GD and my belief that the failure to fit was due to the model getting stuck in a local minimum where the amplitude is small. I hoped that SGD would fix this problem.</li>
  <li>I tried using SGD but it did not help (described in Part III).</li>
  <li>I then tried using regularisation to solve the amplitude issue (described below).</li>
  <li>I had the idea of investigating the loss function in detail. However, I decided it would be better to separate off this detailed investigation into sinusoidal models into a separate post (this one), and have a post discussing only the stochasticity (Part III).</li>
  <li>While writing up Part III, for the sake of completeness, I investigated the learning rate. This solved the issue!</li>
  <li>However, even though I solved the issues, I thought it would still be worthwhile to write-up my experiments with regularisation and also to carry out the investigation into the loss function. So here we are!</li>
</ul>

<h2 id="regularisation">Regularisation</h2>
<p>Based on the examples I had tried, the amplitude would always tend to zero. Hence, I thought it would be worth adding a regularisation term that punishes having small amplitudes.</p>

<p>The loss function was <code class="highlighter-rouge">loss = mse(y_est, y)</code>. After the regularisation, it became <code class="highlighter-rouge">loss = mse(y_est, y) - parameters_est[0]</code>.  Why did I choose this regularisation?</p>

<ul>
  <li>I believed that the amplitude would naturally tend to small values. Thus, I want to punish small values and encourage large values.</li>
  <li>Smaller losses should correspond to better models. Therefore, the larger the amplitude, the smaller the loss should be.</li>
  <li>Subtracting the amplitude from the loss achieves this. (Note that the first element of <code class="highlighter-rouge">parameters_est</code> is the amplitude).</li>
  <li>By differentiating, this regularisation causes the amplitude to increase by a constant amount each step, so there is a constant upward pressure on the amplitude.</li>
</ul>

<p>Below is the first result of introducing this regularisation.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin1.mp4" type="video/mp4" />
  </video>
</figure>

<p>As you can see, there are moments where the model gets close to the data. This got my hopes up, and made me feel like I was onto something.</p>

<p>I tried various other things to see if I could make it better. I tried changing the weight of the regularisation term. I tried adding other regularisation terms (because in the experiments, it looked like there was now a tendency for the frequency to keep increasing). I can’t remember if I tried other things or not. Suffice it to say, I made no progress.</p>

<p>Below is an animation of an experiment which involved changing the weight of the regularisation term. I include it only because I thought it was particularly funky and visually interesting.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin2.mp4" type="video/mp4" />
  </video>
</figure>

<h2 id="visualising-the-loss-function">Visualising the loss function</h2>
<p>After failing to get regularisation to work, I decided I should try to visualise the loss function, and find out exactly where the local minima were, and hopefully better understand why things were not working.</p>

<p>The process I followed was:</p>
<ul>
  <li>Create data to be fit. That was just <code class="highlighter-rouge">y = sin(x)</code></li>
  <li>Create generic sinusoidal models <code class="highlighter-rouge">y_est = a*sin(b*x + c) + d</code></li>
  <li>Vary the parameters <code class="highlighter-rouge">a,b,c,d</code> and calculate the loss, <code class="highlighter-rouge">mse(y, y_est)</code></li>
  <li>Plot graphs to visualise the loss function</li>
</ul>

<p>To begin, I set <code class="highlighter-rouge">c</code> and <code class="highlighter-rouge">d</code> to <code class="highlighter-rouge">0</code> and varied <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. <code class="highlighter-rouge">a</code> is the amplitude and <code class="highlighter-rouge">b</code> is the frequency (multiplied by <code class="highlighter-rouge">2*pi</code>) or the coefficient of <code class="highlighter-rouge">x</code>. The reason for fixing <code class="highlighter-rouge">c</code> and <code class="highlighter-rouge">d</code> is that it was the amplitude and the frequency which were giving the most trouble.</p>

<p>The first animation below shows a sequence of charts. Each individual chart shows how the loss varies with frequency, and from chart to chart the amplitude is changing.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_loss_vs_freq.mp4" type="video/mp4" />
  </video>
</figure>

<p>As can be seen from this, there are many local minima, so the model might get stuck in the wrong one. Eye-balling the chart, if the initial frequency is below 0.5 or above 1.7, then gradient descent will push the frequency away from the optimal value of 1. It is now clear why there should be a tendency for the frequency to increase, as we saw in the SGD examples in Part III.</p>

<p>The next animation is the opposite. For each individual chart, we see how the loss varies with amplitude, and from chart to chart we are modifying the frequency.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_loss_vs_amp.mp4" type="video/mp4" />
  </video>
</figure>

<p>Fantastic! This I feel like I can understand. For the majority of frequencies, the optimal value for amplitude is zero and the amplitude will just slide its way to that value. Only for a narrow range of frequencies is the optimal value of the amplitude non-zero.</p>

<p>To summarise, based on these two animations, here is what I would predict:</p>
<ul>
  <li>Regardless of amplitude, there is a narrow band of frequencies which would result in SGD finding the global minimum. Otherwise, you will get stuck in some other local minimum.</li>
  <li>For ‘small’ and ‘large’ frequencies, the amplitude will want to decay to zero. For a certain range of frequncies, the amplitude will tend towards a sensible value.</li>
</ul>

<p>As I am writing this up and thinking things through, I am starting to wonder about my conclusion in Part III about the sinusoidal model. In Part III, I concluded that the issue all along was having an inappropriate learning rate, but the two animations above suggest there is more to it. Did I just get lucky and stumble upon starting parameters which fit the criteria I described above, and hence that is why I got the sinusoidal model to fit?  There’s only one way to find out, which is to do more experimentation!</p>

<h2 id="investigating-parameter-initialisation">Investigating parameter initialisation</h2>
<p>The steps for the investigation are as follows:</p>
<ul>
  <li>Create data to be fit and generic model, as above.</li>
  <li>Initialise the estimated parameters: <code class="highlighter-rouge">a=1, b=?, c=0, d=0</code>. We will be varying the value of initial value of <code class="highlighter-rouge">b</code></li>
  <li>Do SGD and visualise the learning</li>
</ul>

<p>I start by trying a large value for the frequency, 5.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f5.mp4" type="video/mp4" />
  </video>
</figure>

<p>So, as predicted, the frequency gets stuck in some sub-optimal value and the amplitude tends to zero. It looks like I did just get lucky in Part III.</p>

<p>Frequency of 2 and 1.5 is similar:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f2.mp4" type="video/mp4" />
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f15.mp4" type="video/mp4" />
  </video>
</figure>

<p>Frequency of 1.2:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f12.mp4" type="video/mp4" />
  </video>
</figure>

<p>We get the model converging to the data! Though this is to be expected, it is still satisfying to see it actually work. With a bit of manual experimentation, the cut-off between these two behaviours is roughly 1.46.</p>

<p>How about lower frequencies? A frequency of 0.6 converges to the correct model:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f06.mp4" type="video/mp4" />
  </video>
</figure>

<p>And a frequency of 0.5 converges to a different minima:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f05.mp4" type="video/mp4" />
  </video>
</figure>

<p>Again, this is consistent with the frequncy vs loss charts above, where you can see there are local minima to the left of the global minimum.</p>

<h2 id="conclusion">Conclusion</h2>
<p>This has been a bit of a topsy-turvy learning experience. I am still surprised at how much I learnt from this basic example. And having struggled with this simple example, I better appreciate how impressive it is to get complicated neural networks to learn.</p>]]></content><author><name></name></author><category term="data science" /><category term="python" /><summary type="html"><![CDATA[I end this series by describing some experiments I did with the sinusoidal case, before I realised that the learning rate was too big. Spoiler alert: turns out my first instincts from Part I were correct all along...]]></summary></entry></feed>