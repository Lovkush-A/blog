<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://lovkush-a.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lovkush-a.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-09-17T15:22:44-05:00</updated><id>https://lovkush-a.github.io/blog/feed.xml</id><title type="html">Lovkush Agarwal</title><subtitle>A blog for my data science learning and projects</subtitle><entry><title type="html">Squash rankings, Part I, Scraping wikipedia and data analysis</title><link href="https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1.html" rel="alternate" type="text/html" title="Squash rankings, Part I, Scraping wikipedia and data analysis" /><published>2020-09-17T00:00:00-05:00</published><updated>2020-09-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I am going through the CS109 Harvard lectures on data science. I just watched a couple of lectures on web-scraping with BeautifulSoup, so I wanted to practice. I decided to scrape squash ranking data from wikipedia, as I am a avid fan of the sport. On wikipedia, the best information I could find was the top 10 players at the end of each year for the past 25 years or so.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The results of the scraping process are in the following tables, summarising some key stats for the players. The tables are ordered by players’ average rank in the dataset. It is worth emphasising that the data only contains information on Top 10 rankings and only at the end of each year; this will skew the data in various ways.&lt;/p&gt;

&lt;h3 id=&quot;summary-for-female-players&quot;&gt;Summary for female players&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|                  player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year |
|------------------------:|-------------:|---------------:|----------:|-----------:|--------------:|------------:|
|         Michelle Martin |     1.400000 |              5 |         1 |          2 |          1994 |        1998 |
|       Sarah Fitz-Gerald |     2.333333 |              9 |         1 |          5 |          1994 |        2002 |
|             Nicol David |     2.357143 |             14 |         1 |          6 |          2004 |        2017 |
|       Raneem El Weleily |     2.444444 |              9 |         1 |          7 |          2011 |        2019 |
|          Leilani Rorani |     2.750000 |              4 |         1 |          7 |          1998 |        2001 |
|        Nour El Sherbini |     3.142857 |              7 |         1 |          6 |          2012 |        2019 |
|         Rachael Grinham |     3.909091 |             11 |         1 |          8 |          2001 |        2011 |
|             Carol Owens |     4.100000 |             10 |         1 |          8 |          1994 |        2003 |
|           Laura Massaro |     4.111111 |              9 |         2 |          9 |          2010 |        2018 |
|          Cassie Jackman |     4.545455 |             11 |         2 |          8 |          1994 |        2004 |
|         Natalie Grinham |     4.666667 |              9 |         2 |          9 |          2003 |        2013 |
|        Natalie Grainger |     5.100000 |             10 |         3 |          7 |          1999 |        2009 |
|              Sue Wright |     5.250000 |              4 |         4 |          8 |          1994 |        1998 |
|           Jenny Duncalf |     5.375000 |              8 |         2 |          9 |          2005 |        2013 |
|            Nouran Gohar |     5.400000 |              5 |         3 |          9 |          2015 |        2019 |
|           Linda Elriani |     5.555556 |              9 |         3 |          9 |          1997 |        2005 |
|          Suzanne Horner |     5.625000 |              8 |         2 |          9 |          1994 |        2001 |
|            Tania Bailey |     5.666667 |              6 |         5 |          9 |          1999 |        2007 |
|        Vanessa Atkinson |     5.875000 |              8 |         1 |         10 |          2002 |        2010 |
|           Camille Serme |     5.888889 |              9 |         3 |         10 |          2010 |        2019 |
|           Nour El Tayeb |     6.000000 |              5 |         3 |          8 |          2014 |        2019 |
|             Kasey Brown |     6.000000 |              2 |         5 |          7 |          2010 |        2011 |
|              Liz Irving |     6.200000 |              5 |         3 |         10 |          1994 |        1998 |
|           Alison Waters |     6.400000 |             10 |         3 |         10 |          2008 |        2018 |
|             Joelle King |     6.666667 |              6 |         4 |         10 |          2012 |        2019 |
|         Vicky Botwright |     7.000000 |              4 |         5 |          9 |          2003 |        2007 |
|            Amanda Sobhy |     7.000000 |              2 |         7 |          7 |          2016 |        2019 |
|        Sarah-Jane Perry |     7.000000 |              3 |         6 |          8 |          2017 |        2019 |
|            Low Wee Wern |     7.333333 |              3 |         7 |          8 |          2012 |        2014 |
|          Madeline Perry |     7.428571 |              7 |         4 |         10 |          2006 |        2013 |
|          Sabine Schoene |     7.500000 |              4 |         6 |          9 |          1995 |        1998 |
|       Omneya Abdel Kawy |     7.900000 |             10 |         4 |         10 |          2004 |        2016 |
|            Fiona Geaves |     8.444444 |              9 |         5 |         10 |          1994 |        2004 |
| Laura Lengthorn-Massaro |     8.500000 |              2 |         8 |          9 |          2008 |        2009 |
|                Annie Au |     8.750000 |              4 |         8 |         10 |          2011 |        2015 |
|             Tesni Evans |     9.000000 |              2 |         9 |          9 |          2018 |        2019 |
|          Rebecca Macree |     9.250000 |              4 |         8 |         10 |          2001 |        2004 |
|         Stephanie Brind |     9.250000 |              4 |         7 |         10 |          1999 |        2003 |
|            Claire Nitch |     9.333333 |              3 |         9 |         10 |          1994 |        1996 |
|             Jane Martin |     9.500000 |              2 |         9 |         10 |          1994 |        1995 |
|        Hania El Hammamy |    10.000000 |              1 |        10 |         10 |          2019 |        2019 |
|         Shelley Kitchen |    10.000000 |              2 |        10 |         10 |          2007 |        2008 |
|         Dipika Pallikal |    10.000000 |              1 |        10 |         10 |          2012 |        2012 |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary-for-male-players&quot;&gt;Summary for male players&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|                 player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year |
|-----------------------:|-------------:|---------------:|----------:|-----------:|--------------:|------------:|
|            Peter Nicol |     2.400000 |             10 |         1 |          8 |          1996 |        2005 |
|              Ali Farag |     3.250000 |              4 |         1 |          7 |          2016 |        2019 |
|           Jansher Khan |     3.333333 |              3 |         1 |          8 |          1996 |        1998 |
|         Jonathon Power |     3.666667 |              9 |         1 |          9 |          1997 |        2005 |
|    Mohamed El Shorbagy |     3.700000 |             10 |         1 |         10 |          2010 |        2019 |
|            Ramy Ashour |     3.909091 |             11 |         1 |          7 |          2006 |        2016 |
|       Grégory Gaultier |     4.000000 |             15 |         1 |         10 |          2003 |        2018 |
|           Ahmed Barada |     4.250000 |              4 |         2 |          7 |          1997 |        2000 |
|           Rodney Eyles |     4.333333 |              3 |         2 |          7 |          1996 |        1998 |
|           Nick Matthew |     4.642857 |             14 |         1 |         10 |          2004 |        2017 |
|           David Palmer |     4.727273 |             11 |         1 |          9 |          2000 |        2011 |
|            Amr Shabana |     4.727273 |             11 |         1 |          9 |          2004 |        2014 |
|           Paul Johnson |     5.000000 |              3 |         4 |          7 |          1998 |        2000 |
|        Stewart Boswell |     5.000000 |              2 |         4 |          6 |          2001 |        2002 |
|         Thierry Lincou |     5.200000 |             10 |         1 |          9 |          2001 |        2010 |
|        James Willstrop |     5.272727 |             11 |         1 |         10 |          2005 |        2017 |
|       Anthony Ricketts |     5.500000 |              4 |         3 |          7 |          2002 |        2006 |
|          Karim Darwish |     5.600000 |             10 |         1 |          9 |          2003 |        2013 |
|      Karim Abdel Gawad |     5.666667 |              6 |         2 |          9 |          2015 |        2019 |
|           Martin Heath |     5.666667 |              3 |         5 |          6 |          1998 |        2000 |
|             Del Harris |     6.000000 |              2 |         6 |          6 |          1996 |        1997 |
|             Dan Jenson |     6.000000 |              1 |         6 |          6 |          1998 |        1998 |
|     Marwan El Shorbagy |     6.250000 |              4 |         5 |          9 |          2016 |        2019 |
|             John White |     6.571429 |              7 |         2 |         10 |          1999 |        2007 |
|              Paul Coll |     6.666667 |              3 |         5 |          8 |          2017 |        2019 |
|            Omar Mosaad |     6.666667 |              3 |         4 |          8 |          2012 |        2016 |
|            Tarek Momen |     6.800000 |              5 |         4 |         10 |          2014 |        2019 |
|           Lee Beachill |     6.800000 |              5 |         1 |         10 |          2002 |        2006 |
| Miguel Ángel Rodríguez |     7.000000 |              3 |         5 |         10 |          2015 |        2019 |
|            Diego Elias |     7.000000 |              1 |         7 |          7 |          2019 |        2019 |
|       Stefan Casteleyn |     7.000000 |              1 |         7 |          7 |          1999 |        1999 |
|            David Evans |     7.000000 |              2 |         4 |         10 |          2000 |        2001 |
|            Simon Parke |     7.000000 |              5 |         3 |         10 |          1996 |        2000 |
|          Craig Rowland |     7.000000 |              1 |         7 |          7 |          1996 |        1996 |
|           Chris Walker |     7.000000 |              2 |         4 |         10 |          1996 |        1997 |
|           Brett Martin |     7.000000 |              2 |         5 |          9 |          1996 |        1997 |
|            Borja Golán |     7.000000 |              2 |         7 |          7 |          2013 |        2014 |
|           Peter Barker |     7.142857 |              7 |         5 |          9 |          2008 |        2014 |
|           Anthony Hill |     7.333333 |              3 |         5 |          9 |          1996 |        1999 |
|           Simon Rösner |     7.500000 |              6 |         3 |         10 |          2014 |        2019 |
|           Ong Beng Hee |     7.666667 |              3 |         7 |          8 |          2001 |        2003 |
|          Mark Chaloner |     8.666667 |              3 |         8 |         10 |          1996 |        2002 |
|     Laurens Jan Anjema |     9.000000 |              1 |         9 |          9 |          2010 |        2010 |
|             Alex Gough |     9.000000 |              2 |         9 |          9 |          1999 |        2000 |
|          Wael El Hindi |     9.000000 |              3 |         8 |         10 |          2007 |        2009 |
|      Mathieu Castagnet |     9.000000 |              2 |         9 |          9 |          2015 |        2016 |
|             Paul Price |     9.500000 |              2 |         9 |         10 |          2000 |        2001 |
|             Derek Ryan |    10.000000 |              1 |        10 |         10 |          1998 |        1998 |
|    Mohd Azlan Iskandar |    10.000000 |              1 |        10 |         10 |          2011 |        2011 |
|     Mohamed Abouelghar |    10.000000 |              1 |        10 |         10 |          2018 |        2018 |
|            Daryl Selby |    10.000000 |              2 |        10 |         10 |          2012 |        2013 |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;patterns-and-observations&quot;&gt;Patterns and observations&lt;/h2&gt;
&lt;p&gt;It is satisfying to be able to compare the various big names in squash.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From this, the most outstanding player is Nicol David. They have the largest number of years in the top 10 of any female player by a big margin, and she has the 3rd best average rating. Only two players have a higher average rating: Sarah Fitz-Gerald’s average is for practical purposes the same (2.33 vs 2.35) and Michelle Martin’s is probably skewed by the fact the data only goes as far back as 1994.&lt;/li&gt;
  &lt;li&gt;A stand-out statistic is that Gaultier has been in the Top 10 for 15 years! Though it is mentioned by commentators frequently, it is only when I compare it to other players’ durations do I fully appreciate how incredible the achievement is. Nick Matthew is not far behind with 14 years.&lt;/li&gt;
  &lt;li&gt;Another surprise for me is how high Ramy Ashour is, compared to other players. It is well-recognised that he is the best player of his generation by a large margin, but he has also been plagued by injury for most of it, too. I would have predicted that it would have had a noticable dent on his stats. It is scary to think how much better his stats would have been if he did not have injuries!&lt;/li&gt;
  &lt;li&gt;Ali Farag’s average is very high. Though I do not want to diminish this achievement, I think this is a reflection of how the modern game has fewer elite players, whereas ten years ago, 7 or 8 of the top 10 players had all achieved a World Ranking of 1.&lt;/li&gt;
  &lt;li&gt;It is interesting to see general patterns. E.g. the players who have spent the most years in the top 10 are also players with higher averages and higher maximum ranks. For the males, this pattern is stark: if they have spent at least 9 years in the Top-10 rankings then they have reached the No. 1 spot.  For the females, the pattern is not as clear. This might suggeset that female squash has had a few players that have dominated the top spot, with the remaining players competiting for the other spots in the top 10.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Like I said earlier, the data is skewed in various ways.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The data only includes rankings at the end of each year. This will hide variations throughout the year. If you use monthly data instead, I expect the patterns will be clearer, and the best players will stand out even more from the good players.&lt;/li&gt;
  &lt;li&gt;The data only includes rankings that are in the top 10. For the absolute best players, this is not a loss of much data, but for the players in the 5-10 range, significant data is missing about their ranking history.&lt;/li&gt;
  &lt;li&gt;The data only goes back to the early 90s. This skews data by missing out the achievements of previous great players. The main one is Jahangir Khan, who had a 500+ match winning streak in the 80s!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;I will see if I can find more detailed ranking information, so I can get a more fine-grained analysis of the players. A project I have in the back of my mind is to create my own ranking system based on match history, and see if I can create a system which is more predictive than the current system. I think this should be possible, but again, I will need to see if I can obtain the relevant data.&lt;/p&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.core.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://en.wikipedia.org/wiki/Official_Women&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%27&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s_Squash_World_Ranking'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'https://en.wikipedia.org/wiki/Official_Men&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%27&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s_Squash_World_Ranking'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_not_numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;table_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'th'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;n_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'td'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;    
    
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'td'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_not_numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
           &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;url_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    headers contains id needed to cut html into two pieces
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id=&quot;Year_end_world_top_10_players'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id=&quot;Year-end_number_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tables&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'html.parser'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'player'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#     years_in_top10 = df.player.value_counts()
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'player'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'average_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'years_in_top10'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'worst_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;s&quot;&gt;'earliest_year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'latest_year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'average_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;players_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;players_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Other posts in series</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD" /><published>2020-09-17T00:00:00-05:00</published><updated>2020-09-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/data%20science/python/2020/09/10/sgd1.html&quot;&gt;Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot;&gt;Michael Nielsen’s online book on neural networks&lt;/a&gt;.) Therefore, it should be possible for a neural network to model the datasets I created in the first post, and it should be interesting to see the visualisations of the learning taking place.&lt;/p&gt;

&lt;h2 id=&quot;linear-data&quot;&gt;Linear data&lt;/h2&gt;
&lt;p&gt;I created some linear data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x + b + noise&lt;/code&gt;, and then tried to fit a neural network to it.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This is enchanting. I never thought I’d see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again.&lt;/p&gt;

&lt;p&gt;You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-5&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-4&lt;/code&gt;) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well.&lt;/p&gt;

&lt;p&gt;I next tried increasing the learning rate. This next video is an example when the learning rate was &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; throughout.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the first one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate.&lt;/p&gt;

&lt;p&gt;I next tried introducing a cutoff point where the learning rate increases from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;3e-3&lt;/code&gt;. I have two examples of this.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;In both of these videos, there is a moment in the middle where things go a bit wacky for a few iterations and then quickly settle back down. This wacky moment occurs precisely when the increase in the learning rate kicks in. My explanation for this is that we are seeing the algorithm jump away from one local minimum and moving towards another.&lt;/p&gt;

&lt;p&gt;One thing I should have said is that in each of these videos, the training dataset remains the same, but the initialisation of the parameters of the neural network are different. So one other thing we are witnessing from these experiments is how different initialisation results in different models using gradient descent - i.e. there are many local minimums in the parameter space!&lt;/p&gt;

&lt;h2 id=&quot;quadratic-data&quot;&gt;Quadratic data&lt;/h2&gt;
&lt;p&gt;I created some quadratic data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x*x + b*x + c + noise&lt;/code&gt; and tried to fit a neural network to it. Below are three examples. Note that as above, the dataset is staying the same each time, but the parameters are initialised differently each time and I play with the learning rates a bit, too.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;All these examples have similar overall behaviour. It is quick to get close-ish to the quadratic, and then the learning dramatically slows (but there is still learning going on throughout). Again we see how the different initialisations leads to different final models, showing how we are finding different local minimums.&lt;/p&gt;

&lt;h2 id=&quot;sinusoidal-data&quot;&gt;Sinusoidal data&lt;/h2&gt;
&lt;p&gt;I created some sinusoidal data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*sin(b*x+ c) + d&lt;/code&gt; and tried to fit a neural network to it. As before, same dataset but with different starting parameters.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The first one shows some oscillating behaviour, similar to the first sinusoidal case in the first post of this series. Looks like the behaviour is not as unlikely as I previously thought. The second two produce mediocre results, at best.&lt;/p&gt;

&lt;p&gt;Looking at the graphs, there seem to be ‘too many wiggles’, so I thought I’d try reducing the number of neurons in the hidden layer. The next three videos show what happens with eight neurons.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;These perform less well than with twenty neurons. The final video is intersting for its crazy behaviour at the start - the function seems to spin around and do all kinds of weird stuff before settling down. I do not know what to make of that.&lt;/p&gt;

&lt;p&gt;There is lots more experimentation that can be done, but I feel this is a good place to stop and move on to other things to investigate.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;There are various things I learnt doing this.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is a lot to be learnt by playing around with the ideas. Though (I think) I understand the theory of gradient descent and of vanilla neural networks, it is evident that the whole is greater than the sum of its parts. I under-estimated what I could have learnt by experimenting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It seems neural networks are only useful for interpolation - i.e. making predictions for data similar to the training data. They are hopeless at extrapolating: I think they necessarily have horizontal asymptotes in both x-directions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The initialisation of parameters matters a lot for neural networks.
    &lt;ul&gt;
      &lt;li&gt;My very first settings were useless, and were seemingly only capable of modelling a sigmoid function. For a while, I thought my code was buggy, but after some experimentation, it turned out the initial parameter settings were inappropriate.&lt;/li&gt;
      &lt;li&gt;Different initialisations resulted in vastly different final models. Presumably this is also the case with more complex tasks, but maybe it does not matter when doing a classification on real data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The range of values in the data impacts the choice of parameters. As I mentioned in the first post, I have read that normalising is important. The fact that the parameter settings are sensitive to the range of values in the dataset could be a big reason why normalising is important.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manually creating a neural network (even with only 1 layer) is a bit of faff; I should learn how to create one using the pytorch library.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;You can create some funky visuals with neural networks! Somebody more creative than I am could add some music to each of these to produce some interesting art. These animations may even be a weird alternative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Rorschach_test&quot;&gt;Rorschach inkblot tests&lt;/a&gt;: ‘How does this video make you feel? What do you see in this video?’&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD" /><published>2020-09-11T00:00:00-05:00</published><updated>2020-09-11T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;/blog/data%20science/python/2020/09/10/sgd1.html&quot;&gt;Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot;&gt;Michael Nielsen’s online book on neural networks&lt;/a&gt;.) Therefore, it should be possible for a neural network to model the datasets I created in the first post, and it should be interesting to see the visualisations of the learning taking place.&lt;/p&gt;

&lt;h2 id=&quot;linear-data&quot;&gt;Linear data&lt;/h2&gt;
&lt;p&gt;I created some linear data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x + b + noise&lt;/code&gt;, and then tried to fit a neural network to it.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This is enchanting. I never thought I’d see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again.&lt;/p&gt;

&lt;p&gt;You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-5&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-4&lt;/code&gt;) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well.&lt;/p&gt;

&lt;p&gt;I next tried increasing the learning rate. This next video is an example when the learning rate was &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; throughout.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the first one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate.&lt;/p&gt;

&lt;p&gt;I next tried introducing a cutoff point where the learning rate increases from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;3e-3&lt;/code&gt;. I have two examples of this.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;In both of these videos, there is a moment in the middle where things go a bit wacky for a few iterations and then quickly settle back down. This wacky moment occurs precisely when the increase in the learning rate kicks in. My explanation for this is that we are seeing the algorithm jump away from one local minimum and moving towards another.&lt;/p&gt;

&lt;p&gt;One thing I should have said is that in each of these videos, the training dataset remains the same, but the initialisation of the parameters of the neural network are different. So one other thing we are witnessing from these experiments is how different initialisation results in different models using gradient descent - i.e. there are many local minimums in the parameter space!&lt;/p&gt;

&lt;h2 id=&quot;quadratic-data&quot;&gt;Quadratic data&lt;/h2&gt;
&lt;p&gt;I created some quadratic data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x*x + b*x + c + noise&lt;/code&gt; and tried to fit a neural network to it. Below are three examples. Note that as above, the dataset is staying the same each time, but the parameters are initialised differently each time and I play with the learning rates a bit, too.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;All these examples have similar overall behaviour. It is quick to get close-ish to the quadratic, and then the learning dramatically slows (but there is still learning going on throughout). Again we see how the different initialisations leads to different final models, showing how we are finding different local minimums.&lt;/p&gt;

&lt;h2 id=&quot;sinusoidal-data&quot;&gt;Sinusoidal data&lt;/h2&gt;
&lt;p&gt;I created some sinusoidal data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*sin(b*x+ c) + d&lt;/code&gt; and tried to fit a neural network to it. As before, same dataset but with different starting parameters.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The first one shows some oscillating behaviour, similar to the first sinusoidal case in the first post of this series. Looks like the behaviour is not as unlikely as I previously thought. The second two produce mediocre results, at best.&lt;/p&gt;

&lt;p&gt;Looking at the graphs, there seem to be ‘too many wiggles’, so I thought I’d try reducing the number of neurons in the hidden layer. The next three videos show what happens with eight neurons.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;These perform less well than with twenty neurons. The final video is intersting for its crazy behaviour at the start - the function seems to spin around and do all kinds of weird stuff before settling down. I do not know what to make of that.&lt;/p&gt;

&lt;p&gt;There is lots more experimentation that can be done, but I feel this is a good place to stop and move on to other things to investigate.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;There are various things I learnt doing this.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is a lot to be learnt by playing around with the ideas. Though (I think) I understand the theory of gradient descent and of vanilla neural networks, it is evident that the whole is greater than the sum of its parts. I under-estimated what I could have learnt by experimenting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It seems neural networks are only useful for interpolation - i.e. making predictions for data similar to the training data. They are hopeless at extrapolating: I think they necessarily have horizontal asymptotes in both x-directions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The initialisation of parameters matters a lot for neural networks.
    &lt;ul&gt;
      &lt;li&gt;My very first settings were useless, and were seemingly only capable of modelling a sigmoid function. For a while, I thought my code was buggy, but after some experimentation, it turned out the initial parameter settings were inappropriate.&lt;/li&gt;
      &lt;li&gt;Different initialisations resulted in vastly different final models. Presumably this is also the case with more complex tasks, but maybe it does not matter when doing a classification on real data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The range of values in the data impacts the choice of parameters. As I mentioned in the first post, I have read that normalising is important. The fact that the parameter settings are sensitive to the range of values in the dataset could be a big reason why normalising is important.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manually creating a neural network (even with only 1 layer) is a bit of faff; I should learn how to create one using the pytorch library.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;You can create some funky visuals with neural networks! Somebody more creative than I am could add some music to each of these to produce some interesting art. These animations may even be a weird alternative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Rorschach_test&quot;&gt;Rorschach inkblot tests&lt;/a&gt;: ‘How does this video make you feel? What do you see in this video?’&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data" /><published>2020-09-10T00:00:00-05:00</published><updated>2020-09-10T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html&quot;&gt;Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html&quot;&gt;Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;At the end of &lt;a href=&quot;https://youtu.be/5L3Ao5KuCC4?t=7244&quot;&gt;Lecture 3 of the 2020 FastAI course&lt;/a&gt; and at the end of &lt;a href=&quot;https://youtu.be/ccMHJeQU4Qw?t=6394&quot;&gt;Lecture 2 of the 2018 FastAI course&lt;/a&gt;, there are visualisations of the gradient descent algorithm. I quite liked them, in particular the animation from the 2018 version, and I wanted to re-create them and on more complex examples.&lt;/p&gt;

&lt;p&gt;The animations I created are available below. Note that in all animations, the orange dots represent the training data, and the blue line represents the model’s predictions. I will go through them and give my thoughts. At the end I describe some insights I gained by doing this.&lt;/p&gt;

&lt;h2 id=&quot;linear-data&quot;&gt;Linear data&lt;/h2&gt;
&lt;p&gt;I created some linear data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x + b + noise&lt;/code&gt;, and then tried to use gradient descent to determine the coefficients.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_linear_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This animation is representative of the various examples I tried for linear data. Gradient descent is quick to get very close to the data, but then the learning dramatically slows down and it takes many iterations to improve further. (Note, you have to pay close attention to notice that there is still learning going on throughout the whole video).  Clearly, there is some optimisation that can be done with the learning rate; I did try to create a cutoff point where the learning rate gets bigger, but I am sure there are much better ways of doing things.&lt;/p&gt;

&lt;h2 id=&quot;quadratic-data&quot;&gt;Quadratic data&lt;/h2&gt;
&lt;p&gt;Next I created some quadratic data &lt;code class=&quot;highlighter-rouge&quot;&gt;y=a*x*x + b*x + c + noise&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_quadratic_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The pattern here is very similar to the pattern for the linear case: gradient descent quickly reaches a good model, and then the learning dramatically slows down. This is not too surprising, because though the final function is non-linear, this is still a linear-regression problem by treating &lt;code class=&quot;highlighter-rouge&quot;&gt;x*x&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; as separate features.&lt;/p&gt;

&lt;h2 id=&quot;sinusoidal-data&quot;&gt;Sinusoidal data&lt;/h2&gt;
&lt;p&gt;Next I created some sinusoidal data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*(sin(b*x + c)) + d&lt;/code&gt;. Things were more interesting here.&lt;/p&gt;

&lt;p&gt;The first video shows you what happened when I chose a learning rate that was too large (but not so large so as to have everything diverge to infinity):&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_sin_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Crazy, right! The model is oscillating back and forth, with the oscillations slowly getting larger with time. In Lecture 3 of the 2020 course, this behaviour is illustrated with the example of using gradient descent to minimise a quadratic function, but I never thought I would actually encounter this behaviour out in the wild.&lt;/p&gt;

&lt;p&gt;This second video shows what happens when I choose a smaller learning rate:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_sin_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;No craziness here, but it does not converge to an appropriate solution. I think the explanation for this is that the algorithm has found a local-minimum, and so the algorithm gets stuck and cannot improve.  This is qualitatively different to the linear and quadratic cases: since those were both instances of linear regression, it is known from theory that there is only one minimum so gradient descent will reach it. This sinusoidal case cannot be re-written as a linear regression problem, so there is not automatic guarantee of there being only one minimum point; from this experimentation, it looks like there multiple minimum points!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I learnt various things by doing this experiment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The learning rate is very important! I had to play around with the learning rates to get things to work.&lt;/li&gt;
  &lt;li&gt;The range of values in the training data seemed to have big impact on the which learning rates to use. I am not 100% sure about this, but I have read in places that it is important to normalise your data, and perhaps its effect on learning rates is a big reason.&lt;/li&gt;
  &lt;li&gt;I learnt how to create animations in matplotlib! And also how to include video files in this blog. :D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are various things I would like to try.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The next thing I will try is using the same datasets, but seeing if I can fit a neural network to the data.&lt;/li&gt;
  &lt;li&gt;Stochastic gradient descent. My hope is that it will avoid the local minimum problem in the sinusoidal case.&lt;/li&gt;
  &lt;li&gt;Creating a web-app out of this, so you can easily experiment for yourselves.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</summary></entry><entry><title type="html">FastAI Course, Part III, Frustrations with creating an image classifier</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html" rel="alternate" type="text/html" title="FastAI Course, Part III, Frustrations with creating an image classifier" /><published>2020-09-08T00:00:00-05:00</published><updated>2020-09-08T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html&quot;&gt;FastAI Course, Part II, Lesson 1 and sentiment analysis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/09/fastai1.html&quot;&gt;FastAI Course, Part I, Lessons 1 and 2&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After reading through Chapter 2 of the Fast AI course/book, it was time I tried putting the ideas into practice. I decided to create a classifier that distinguishes between squash, tennis and badminton rackets. Then using voila and binder, I created a little web app for it: check it out &lt;a href=&quot;https://mybinder.org/v2/gh/Lovkush-A/fastai_rackets/master?urlpath=%2Fvoila%2Frender%2Frackets.ipynb&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;a-comedy-of-errors&quot;&gt;A comedy of errors&lt;/h2&gt;
&lt;p&gt;This project was frustrating for me. I somehow managed to make numerous errors at every step which could not be straightforwardly de-bugged. I try to describe them here, but I have undoubtedly forgotten many of them.&lt;/p&gt;

&lt;h3 id=&quot;the-bing-api&quot;&gt;The Bing API&lt;/h3&gt;
&lt;p&gt;The instructions from the notes: “To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows.” Sounds simple. I went to the Microsoft website and logged on. This is what I saw:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_microsoft.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hmm, there’s nothing obvious here regarding Bing or APIs. I spent a whole bunch of time looking through all the menus, but to no avail. The instructions made it sound simple, and yet I couldn’t manage. I searched in the FastAI forums and a relevant &lt;a href=&quot;https://forums.fast.ai/t/getting-the-bing-image-search-key/67417&quot;&gt;post&lt;/a&gt; was easy to find. It explained the (non-trivial) steps needed. A big non-triviality is that you should be using a Microsoft &lt;em&gt;Azure&lt;/em&gt; account, not a Microsoft account. I think the book should make this clearer. It would have saved me (and presumably many others) a bunch of time.&lt;/p&gt;

&lt;h3 id=&quot;padding&quot;&gt;Padding&lt;/h3&gt;
&lt;p&gt;I downloaded the datasets, viewed some of the images, and deleted the files which were erroneous. Next up was to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;DataBlock&lt;/code&gt; with the appropriate transforms/data augmentation. To ensure the images have the correct aspect ratio, I decided that padding is the best option. I tried it, using &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode = 'zeros'&lt;/code&gt;. It looked alright, but most of the images have white backgrounds, so I thought maybe it would be better for the images to be padded with white space. I guessed that &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode = 'ones'&lt;/code&gt; would work, but alas it did not. So, I did what has been recommended numerous times, and went to the docs.&lt;/p&gt;

&lt;p&gt;This is what I found:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_pad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maybe I am in the minority, but I do not find this helpful. Here are some things that frustrate me about this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are the possibilities for &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;** kwargs&lt;/code&gt;? How would I find out?&lt;/li&gt;
  &lt;li&gt;What does ‘All possible mode as attributes to get tab-completion and typo-proofing’ mean?!  What does it have to do with padding images?&lt;/li&gt;
  &lt;li&gt;What is &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorBBox&lt;/code&gt;? Or &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorPoint&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;What’s the relevance of the change in font in the headings? Is it self-evident? Looking at it more closely, you have one font for classes and one font for methods. But do we need a different font for that? Changing heading formats usually indicates a change in the level of headings, but I don’t think that is what is happening here.&lt;/li&gt;
  &lt;li&gt;Most importantly, what are the different options for &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode&lt;/code&gt;? If you scroll further down, three examples are provided (zeros, reflection and border), but it is not clear if these are all the options. Maybe they are, maybe they aren’t. I don’t know.&lt;/li&gt;
  &lt;li&gt;Really, what does ‘All possible mode as attributes to get tab-completion and typo-proofing’ actually mean??&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps I could find out more by looking at the source code. After all, as is emphasised several times in the lectures, most of the source code is only a few lines and thus easy to understand.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CropPad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandTransform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;Center crop or pad an image to `size`&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PadMode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_process_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;store_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorBBox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_get_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crop_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yes, there are only several lines of code here. However, they are not helpful, at least not for me. In order to understand any of this, I need to go searching through the rest of the code base, which I am not inclined to do. I should not have to do this to find out if I can create white padding instead of black padding (or maybe I should?).&lt;/p&gt;

&lt;p&gt;Anyway, I stopped here with the presumption that it is not possible to get white padding.&lt;/p&gt;

&lt;h3 id=&quot;runtimeerror-dataloader-worker-pid-19862-is-killed-by-signal-killed&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RuntimeError: DataLoader worker (pid 19862) is killed by signal: Killed.&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;I made various other decisions for the DataBlock and created the DataLoaders. Now it was time to do the modelling! I ran the cell, and to my dismay, I got the error message shown in the heading. I didn’t understand it. I checked all my code carefully and did not spot anything wrong. Next I tried to run the examples from the book as is. The same error came up. This was bizarre!&lt;/p&gt;

&lt;p&gt;It was time for some Googling. I searched around, saw people with similar error messages, and tried their suggestions. None of them solved the problem. It was time to get to bed at this stage, so I decided I will ask about this in the forum, and have another stab the next day.&lt;/p&gt;

&lt;p&gt;Next day, I got ready to tackle the problem and opened up Gradient. But something was a little off:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_gradient.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The machine type was set to CPU! I did not remember selecting this, so was a bit confused. Anyway, I tried starting up the notebook, and the pop-up comes up to select the options. The default selection is the CPU. Hmm, that is odd. I tried to change it to GPU, and then I discovered the problem: there was no Free GPU available. It looks like the day before, I just assumed the settings would be the same as previous runs, so I did not check the machine type, and I must have been using the CPU option. This is almost certainly the cause of the &lt;code class=&quot;highlighter-rouge&quot;&gt;DataLoader worker&lt;/code&gt; error. At least I now know the issue.&lt;/p&gt;

&lt;p&gt;Time to move to Colab.&lt;/p&gt;

&lt;h3 id=&quot;google-colab&quot;&gt;Google Colab&lt;/h3&gt;
&lt;p&gt;This is just me, but I dislike the Colab interface. It is similar to Jupyter Notebook, but different in little ways that my brain finds off-putting. One example is that in Colab, the shortcut &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; to insert a new cell automatically enters the cell; this is not what I want, as I often want to create a bunch of cells at once, and then move into them.&lt;/p&gt;

&lt;p&gt;These are minor issues, but I thought I may as well include them, in case anybody else out there gets some comfort knowing they are not alone.&lt;/p&gt;

&lt;h3 id=&quot;re-training-model-post-cleaning&quot;&gt;Re-training model post cleaning&lt;/h3&gt;
&lt;p&gt;I continued with the project, and managed to get the classifier to work. I then cleaned the data using the easy-to-use widget created by FastAI. I then tried to do some further fine tuning, but I got an error message saying that a certain image file, &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt;, cannot be found.&lt;/p&gt;

&lt;p&gt;I had a little look at the code, and realised my error. I deleted some images while cleaning (including image &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt;), but did not re-run the cell that defines the dataloaders &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt; variable, where the information on the dataset is stored. So I re-ran the cell, and tried again. But I still got the error message.&lt;/p&gt;

&lt;p&gt;I wanted to investigate the &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt; variable to find out what is going on; specifically, I wanted to see which filenames are stored in the variable and check whether &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt; is listed there. Similar to the issues I had with the padding above, I did not find the documentation helpful. However, using &lt;code class=&quot;highlighter-rouge&quot;&gt;dir&lt;/code&gt; (a super handy function, by the way. It lists &lt;em&gt;all&lt;/em&gt; the properties and methods that an object has) and playing a bit, I was able to find the list of filenames stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt; was not there!&lt;/p&gt;

&lt;p&gt;This left only one possibility: that filenames are stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;learn&lt;/code&gt; variable. Based on my experience with other ML tools (e.g. scikitlearn), this is counter-intuitive to me. At this stage, I just wanted to get a working example as quickly as possible, so I did not investigate how I can do additional fine-tuning cycles on new datasets. Instead, I just created a fresh learner object, and trained that.&lt;/p&gt;

&lt;p&gt;I am not sure anybody can do anything about this kind of problem, as it stems from using tools as black-boxes, without understanding how they work. Another example of this I have had is in using conda: I am comfortable creating new environments, but I do not know how to rename a directory or move directories around, because that information is stored somewhere for conda to function, and I do not know how to change that information.&lt;/p&gt;

&lt;p&gt;Unfortunately, it does not change that the experience is frustrating - there is a simple task that I want to do, my naive approach does not work, and in order to learn how do it properly, I have to get a fairly comprehensive understanding of the tool. If anybody has any advice or suggestions, do please let me know!&lt;/p&gt;

&lt;h3 id=&quot;binder&quot;&gt;Binder&lt;/h3&gt;
&lt;p&gt;The next bunch of tasks went well, as far as I can remember. Exporting the learner, creating a local environment where I can create the jupyter notebook that will be turned into the web app, installing all the packages, creating the notebook, getting voila to work locally, and pushing it to github. I was literally one step away: get binder to work. The finish line was literally right in front of me.&lt;/p&gt;

&lt;p&gt;Oh boy was I wrong. I do not know how I managed to make such a mess of this step. From what I remember, I encountered two main problems, but my ineptitude dragged out the process far longer than it ought to have.&lt;/p&gt;

&lt;p&gt;The first mistake was not to change the &lt;code class=&quot;highlighter-rouge&quot;&gt;File&lt;/code&gt; setting to &lt;code class=&quot;highlighter-rouge&quot;&gt;URL&lt;/code&gt;, when you enter the path to the file in Binder. This setting was in my blindspot; I never even noticed that box. This is made worse because the voila documentation clearly states that you should do this; in fact, this is how I found the error, because I chose to re-read the instructions carefully, after my various other attempts failed.&lt;/p&gt;

&lt;p&gt;The second issue I had was with the requirements file that should be provided. I first tried using the file created by using &lt;code class=&quot;highlighter-rouge&quot;&gt;conda env export&lt;/code&gt;, but that did not work. Googling the error messages revealed that conda lists many device specific requirements, which will not work with the Linux machines that binder uses. Googling also showed the two-step process to get around this issue: create a modified version of the requirements by adding some tags to the &lt;code class=&quot;highlighter-rouge&quot;&gt;conda env export&lt;/code&gt; command, and then manually remove any requirements that still cause an error on Binder.&lt;/p&gt;

&lt;p&gt;This was it: I fixed the two problems and was minutes away from completion. I got binder going, and as it does, it was taking a few minutes to things set-up.  But things were taking very long. Something was not right. I just kept waiting, but eventually, the process quit with some other error message.&lt;/p&gt;

&lt;p&gt;I was so close, yet so far. I tried searching around on the forums, to see if anybody else had any issues on binder. I ended up finding a blog post and example github repo, so I tried to imitate that. The main takeaway was that they manually constructed a requirements file with 4 or 5 requirements, rather than using an automatically generated file from conda with many more requirements.&lt;/p&gt;

&lt;p&gt;After a few more attempts and ironing out several little mistakes (e.g. they used &lt;code class=&quot;highlighter-rouge&quot;&gt;fastai2&lt;/code&gt; in their requirements, which is now out-of-date), it finally worked.&lt;/p&gt;

&lt;h2 id=&quot;final-remarks&quot;&gt;Final remarks&lt;/h2&gt;
&lt;p&gt;This has been a stressful but enlightening experience. Just writing this blog made me re-live the emotions: the frustration when things seemed unnecessarily complicated, and the sense of achievement and relief when I finally reached the end.&lt;/p&gt;

&lt;p&gt;It may sound like I dislike FastAI and the course, but the opposite is true. Like I said in my first post on the FastAI course, I am impressed with the teaching and all the thought that has gone into it. I would strongly recommend this course to anybody interested in AI or Machine Learning. However, I think this may also be why I experience more frustration with it. I have such a high opinion of the course, that any negative stands out; I am holding the team to particularly high standards.&lt;/p&gt;

&lt;p&gt;To end, I hope this blog entry has been useful for you in some way. I welcome any comments or thoughts or disagreements; please feel free to let me know.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series FastAI Course, Part II, Lesson 1 and sentiment analysis FastAI Course, Part I, Lessons 1 and 2</summary></entry><entry><title type="html">Analysing the movies I’ve watched, Part V, Data visualisation II</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5.html" rel="alternate" type="text/html" title="Analysing the movies I've watched, Part V, Data visualisation II" /><published>2020-09-02T00:00:00-05:00</published><updated>2020-09-02T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/24/mymovies4.html&quot;&gt;Analysing the movies I’ve watched, Part IV, Data visualisation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/19/mymovies3.html&quot;&gt;Analysing the movies I’ve watched, Part III, Joining the tables&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/03/mymovies2.html&quot;&gt;Analysing the movies I’ve watched, Part II, Data cleaning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/02/mymovies1.html&quot;&gt;Analysing the movies I’ve watched, Part I, Data collection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a previous post, I described how the joining missed out half the data. After making various plots, is turns out the joining was sometimes erroneous too. Hence, I bit the bullet and decided I to manually do the joining, and get a complete and clean dataset. Learning the lessons from past attempts at manual processing, it was a fairly smooth process. (Furthermore, things were significantly sped up by using Selenium - which allowed me to automate the process of searching for a movie on imdb). In the end, it took roughly 3 hours to tidy the dataset and have the official imdb id attached to each entry in my database. And now for the charts!&lt;/p&gt;

&lt;h2 id=&quot;year-of-release&quot;&gt;Year of release&lt;/h2&gt;
&lt;p&gt;Below are charts showing information about the year of release for the movies I have watched. The first one is just a histogram, and the second shows a scatter-chart showing the year I watched the movie against the year the movie released.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_releaseyear.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/mymovies5_releaseyear2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not sure if there is anything surprising about any of these charts. One neat thing is that one could probably get an idea of my age based on the first chart. The second chart seems to show that the majority of movies I watch are relatively new.&lt;/p&gt;

&lt;h2 id=&quot;ratings&quot;&gt;Ratings&lt;/h2&gt;
&lt;p&gt;Below is a chart showing the average ratings (by users of imdb) for the movies I have watched, along with the distribution of ratings for all the movies in imdb.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is clear from this chart that I tend to watch ‘decent’ movies, as judged by the movies, and that I avoid ‘bad’ movies, but I also seem to avoid the ‘best’ movies.&lt;/p&gt;

&lt;p&gt;Below is a chart comparing the ratings for all the movies I have watched with the ratings of the movie I would recommend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is reassuring to see that, overall, the movies I recommend are generally better regarded than the movies I do not recommend.&lt;/p&gt;

&lt;p&gt;Lastly, here is a chart showing the variation in ratings by the year I watched the movie.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I cannot really discern any patterns here, that aren’t revealed by other charts. Patterns that I can see are that I watched a lot more movies (and greater variety of quality of  movies) in 2008 and 2009, and that the movies I recommend tend to have higher ratings than those I don’t.&lt;/p&gt;

&lt;p&gt;To end this section, here is a list of the most and least popular movies I have watched.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Rating on imdb&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;the shawshank redemption&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the dark knight&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12 angry men&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the return of the king&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pulp fiction&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fight club&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the fellowship of the ring&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;forrest gump&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;inception&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the matrix&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the two towers&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;goodfellas&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;star wars: episode v - the empire strikes back&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;se7en&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;city of god&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interstellar&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;saving private ryan&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the silence of the lambs&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;star wars: episode iv - a new hope&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;gladiator&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the departed&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;casablanca&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the usual suspects&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dear zachary: a letter to a son about his father&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;whiplash&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;joker&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;psycho&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;senna&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Rating on imdb&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;steel&lt;/td&gt;
      &lt;td&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;troll 2&lt;/td&gt;
      &lt;td&gt;2.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the foreigner&lt;/td&gt;
      &lt;td&gt;3.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batman &amp;amp; robin&lt;/td&gt;
      &lt;td&gt;3.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;half a person&lt;/td&gt;
      &lt;td&gt;3.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th part viii: jason takes manhattan&lt;/td&gt;
      &lt;td&gt;4.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;barely legal&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;elektra&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doe: dead or alive&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th: a new beginning&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;uncut gem&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;blair witch&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a teacher’s crime&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tekken: the motion picture&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;in their skin&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;daredevil&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;scary movie 2&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the circle&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th part vii: the new blood&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;little&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The one thing I feel slightly ashamed of is that I watched so many of the Friday the 13th movies. I think my reasoning was that if a series has that many movies, there must be something good about it. I kept going on, even though I thought the movies were bad, in the hope that there is a moment in the series where the movies become good. I did finally stop watching the series - it just took eight movies to make that decision…&lt;/p&gt;

&lt;h2 id=&quot;genres&quot;&gt;Genres&lt;/h2&gt;
&lt;p&gt;Below is a table showing a breakdown of the genres of movie I watched, ordered by which genre has the highest odds of me recommending it. Note that most movies had multiple genres.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Genre&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Frequency&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Frequency Recommended&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Percent recommended&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;thriller&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;126&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;37&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;war&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;sport&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;drama&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;319&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;87&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;sci-fi&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;crime&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;146&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;biography&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;47&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;music&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;romance&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;56&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;comedy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;210&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;42&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;mystery&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;animation&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;51&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;family&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;history&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fantasy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;documentary&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;31&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;adventure&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;188&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;horror&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;66&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;action&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;227&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;musical&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;news&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;western&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The entries that stand out to me are those with the largest frequencies. There is strong evidence that I am more likely to recommend a thriller or a drama than a comedy, and that I am unlikely to recommend adventure or action movies.&lt;/p&gt;

&lt;h2 id=&quot;directors&quot;&gt;Directors&lt;/h2&gt;
&lt;p&gt;Below is a list of the directors, along with the movies, whose movies I have most watched.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Steven Spielberg
 a.i. artificial intelligence
          catch me if you can
                         jaws
                jurassic park
              minority report
      raiders of the lost ark
          saving private ryan
     the adventures of tintin

Peter Jackson
                                         king kong
                 the hobbit: an unexpected journey
         the hobbit: the battle of the five armies
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers

Christopher Nolan
               dunkirk
             inception
          interstellar
               memento
       the dark knight
 the dark knight rises

Guy Ritchie
 lock, stock and two smoking barrels
                          rocknrolla
                     sherlock holmes
  sherlock holmes: a game of shadows
                              snatch
             the man from u.n.c.l.e.

Martin Scorsese
               cape fear
              goodfellas
                 silence
             taxi driver
            the departed
 the wolf of wall street

Quentin Tarantino
             inglourious basterds
                kill bill: vol. 1
 once upon a time... in hollywood
                     pulp fiction
                   reservoir dogs
                the hateful eight

Danny Boyle
    28 days later
       steve jobs
 t2 trainspotting
    trainspotting
        yesterday

Matthew Vaughn
                     kick-ass
 kingsman: the secret service
                   layer cake
                     stardust
           x-men: first class

David Fincher
                          fight club
                           gone girl
                               se7en
 the curious case of benjamin button
                  the social network

M. Night Shyamalan
             glass
 lady in the water
             split
   the sixth sense
       unbreakable

Darren Aronofsky
          black swan
             mother!
                noah
 requiem for a dream
        the wrestler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I tend not to pay any attention to who the director is, so for me, the interesting thing here is to see how movies that I would have said had no connection in fact have the same director. E.g. I did not realise I had watched that many movies by Spielberg. The other thing that stands out here is that all the directors are male.&lt;/p&gt;

&lt;h2 id=&quot;actors&quot;&gt;Actors&lt;/h2&gt;
&lt;p&gt;Below is a list of actors, along with the movies, that I have most watched.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Brad Pitt
                    12 years a slave
                              allied
                  burn after reading
                          fight club
                inglourious basterds
                 killing them softly
                            megamind
                           moneyball
    once upon a time... in hollywood
                               se7en
                              snatch
                       the big short
 the curious case of benjamin button
                                troy
                         war machine

Bruce Willis
                 alpha dog
                  die hard
                die hard 2
              die hard 4.0
 die hard with a vengeance
          fast food nation
                     glass
                    looper
          moonrise kingdom
              pulp fiction
                  sin city
           the sixth sense
               unbreakable

Samuel L. Jackson
                    die hard with a vengeance
                                        glass
                                       jumper
                 kingsman: the secret service
                                 pulp fiction
 star wars: episode iii - revenge of the sith
                            the hateful eight
                       the hitman's bodyguard
                              the incredibles
                               the negotiator
                                  unbreakable
                                unicorn store

Matt Damon
          adjustment bureau
                  contagion
          good will hunting
               jason bourne
                      ponyo
        saving private ryan
        the bourne identity
       the bourne ultimatum
               the departed
 the legend of bagger vance
                the martian
    the talented mr. ripley

Tom Hanks
     angels &amp;amp; demons
 catch me if you can
        forrest gump
 saving private ryan
          the circle
           toy story
         toy story 2
 toy story of terror
     you've got mail

Robert De Niro
     analyze this
        cape fear
    dirty grandpa
       goodfellas
    hide and seek
            joker
 meet the parents
   righteous kill
      taxi driver

Rachel McAdams
                         about time
                              aloha
                     doctor strange
                         mean girls
                            red eye
                    sherlock holmes
 sherlock holmes: a game of shadows
                      state of play
                   wedding crashers

Natalie Portman
                                 annihilation
                                   black swan
                                       closer
    star wars: episode i - the phantom menace
 star wars: episode ii - attack of the clones
 star wars: episode iii - revenge of the sith
                        the other boleyn girl
                                         thor
                               v for vendetta

Leonardo DiCaprio
                    blood diamond
              catch me if you can
                        inception
 once upon a time... in hollywood
                     the departed
         the man in the iron mask
                     the revenant
          the wolf of wall street

Ben Affleck
                               argo
 batman v superman: dawn of justice
                          daredevil
                          gone girl
                  good will hunting
                           paycheck
                      state of play
                     the accountant

Jake Gyllenhaal
      donnie darko
           jarhead
      nightcrawler
 nocturnal animals
         prisoners
             proof
       source code
    velvet buzzsaw

Ian McKellen
                                          stardust
                 the hobbit: an unexpected journey
         the hobbit: the battle of the five armies
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers
                                             x-men
                                  x2: x-men united

Christian Bale
          3:10 to yuma
       american psycho
        public enemies
  terminator salvation
         the big short
       the dark knight
 the dark knight rises
         the machinist

Morgan Freeman
      along came a spider
              deep impact
                     lucy
      million dollar baby
                    se7en
          the bucket list
 the shawshank redemption
                   wanted

Tom Cruise
          collateral
    edge of tomorrow
     minority report
 mission: impossible
            rain man
    the last samurai
            valkyrie
         vanilla sky

Johnny Depp
                   a nightmare on elm street
 fantastic beasts: the crimes of grindelwald
              fear and loathing in las vegas
     pirate of the caribbean: at world's end
  pirates of the caribbean: dead man's chest
                              public enemies
                                 the tourist

Ewan McGregor
                              angels &amp;amp; demons
                              black hawk down
    star wars: episode i - the phantom menace
 star wars: episode ii - attack of the clones
 star wars: episode iii - revenge of the sith
                             t2 trainspotting
                                trainspotting

Laurence Fishburne
 john wick: chapter 3 - parabellum
                        passengers
                         predators
                        the matrix
               the matrix reloaded
            the matrix revolutions
                        the signal

Robert Downey Jr.
             avengers: infinity war
                         iron man 2
                         iron man 3
                    sherlock holmes
 sherlock holmes: a game of shadows
             spider-man: homecoming
                       the avengers

Jude Law
       a.i. artificial intelligence
                             closer
                          contagion
                            gattaca
                    sherlock holmes
 sherlock holmes: a game of shadows
            the talented mr. ripley

Kevin Spacey
       a bug's life
    american beauty
        margin call
               moon
              se7en
     the negotiator
 the usual suspects

Orlando Bloom
                                       good doctor
           pirate of the caribbean: at world's end
        pirates of the caribbean: dead man's chest
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers
                                              troy

Scarlett Johansson
                   her
           jojo rabbit
   lost in translation
                  lucy
        marriage story
          the avengers
 the other boleyn girl

Angelina Jolie
         alexander
 girl, interrupted
     kung fu panda
   kung fu panda 2
       the tourist
            wanted

Paul Giamatti
         duplicity
 lady in the water
      private life
          sideways
   the illusionist
           win win

Joaquin Phoenix
                  gladiator
                        her
               hotel rwanda
                      joker
                     quills
 you were never really here

Jim Carrey
            ace ventura: pet detective
                       dumb and dumber
 eternal sunshine of the spotless mind
                              the mask
                       the truman show
                               yes man

Anthony Hopkins
                alexander
                     noah
                    proof
               red dragon
 the silence of the lambs
                     thor

Tom Hardy
               dunkirk
            layer cake
                 locke
    mad max: fury road
 the dark knight rises
          the revenant

Keanu Reeves
                         john wick
              john wick: chapter 2
 john wick: chapter 3 - parabellum
                        the matrix
               the matrix reloaded
            the matrix revolutions

Emma Watson
                         beauty and the beast
 harry potter and the deathly hallows: part 1
          harry potter and the goblet of fire
       harry potter and the half-blood prince
                                         noah
                                   the circle

Clive Owen
     children of men
              closer
           duplicity
          inside man
            sin city
 the bourne identity

Ethan Hawke
               boyhood
               gattaca
           lord of war
        predestination
 the magnificent seven
             the purge

John Malkovich
     being john malkovich
       burn after reading
           johnny english
            ripley's game
 the man in the iron mask
              warm bodies

Elijah Wood
                                       deep impact
                                        happy feet
                                            maniac
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers

Cameron Diaz
  being john malkovich
   shrek forever after
       shrek the third
              the mask
           vanilla sky
 what happens in vegas
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nice list of some of the more famous actors in the industry. One thing that surprised is how many movies I have watched that starred Brad Pitt - I did not realise it was that many! Once again, it is worth noting that females are under-represented here.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This project has probably been the one that I spent the most time on so far. I have learnt a whole bunch and managed to produce some nice visuals and summaries. Hopefully it was interesting to read!&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Analysing the movies I’ve watched, Part IV, Data visualisation Analysing the movies I’ve watched, Part III, Joining the tables Analysing the movies I’ve watched, Part II, Data cleaning Analysing the movies I’ve watched, Part I, Data collection</summary></entry><entry><title type="html">FastAI Course, Part II, Lesson 1 and sentiment analysis</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html" rel="alternate" type="text/html" title="FastAI Course, Part II, Lesson 1 and sentiment analysis" /><published>2020-08-30T00:00:00-05:00</published><updated>2020-08-30T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html&quot;&gt;FastAI Course, Part III, Frustrations with creating an image classifier&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/09/fastai1.html&quot;&gt;FastAI Course, Part I, Lessons 1 and 2&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, FastAI launched a new version of their course, using their revamped FastAI version 2 packages. As I had only completed the first two lessons of the previous version of the course, I thought I’d restart.  It is good I did re-start, because they have made big changes to how the course is organised and also to the syntax (e.g. in the old version, there was significant discussion about setting the learning rate using their ‘learning rate finder’, but so far, there has been no mention of this and it looks like that has been automated).  Towards the end of the first lesson, Jeremy showcases the different domains in which one can use Deep Learning. One of those was NLP. I have not yet done any NLP, so I thought this is a good chance to play around a little, without having to delve into any of the technical details.&lt;/p&gt;

&lt;p&gt;In this particular example, the NLP consists of sentiment analysis of movie reviews. Looking at the code, they use AWD LSTM architecture. A quick google search reveals that this is a fairly new algorithm, combining various ideas and tools into one. I am not yet at the stage to understand this though, but hopefully it will be discussed later in the course.&lt;/p&gt;

&lt;h2 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h2&gt;
&lt;p&gt;The input to the model is a string containing a movie review. The output is a probability/score between 0 and 1 indicating how much the model thinks the review is positive. My initial thought was to to see if the model had learnt about numbers, by seeing how it rates reviews of the form ‘x out of y’.&lt;/p&gt;

&lt;h3 id=&quot;x-out-of-5&quot;&gt;‘x out of 5’&lt;/h3&gt;
&lt;p&gt;The results of this experimentation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 out of 5: 0.5570
1 out of 5: 0.4760
2 out of 5: 0.3281
3 out of 5: 0.4038
4 out of 5: 0.4073
5 out of 5: 0.8569
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This was surprising to me:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The model thinks that the review ‘0 out of 5’ is likely a positive review&lt;/li&gt;
  &lt;li&gt;The model is roughly undecided about ‘1 out of 5’&lt;/li&gt;
  &lt;li&gt;The model thinks 2,3 or 4 out of 5 is negative&lt;/li&gt;
  &lt;li&gt;The model thinksthat ‘5 out of 5’ is positive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As with most of these ML models, this must be a reflection of the underlying dataset. I imagine very few people actually give a movie 0 or 1 out of 5, so the model doesn’t really know what to do with it, whereas ‘2 out of 5’ probably is more common and basically says the movie is bad, and ‘3 out of 5’ and ‘4 out of 5’ are more average-y ratings.&lt;/p&gt;

&lt;h3 id=&quot;x-out-of-10&quot;&gt;‘x out of 10’&lt;/h3&gt;
&lt;p&gt;The results of this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 out of 10: 0.5402
1 out of 10: 0.4478
2 out of 10: 0.2972
3 out of 10: 0.3741
4 out of 10: 0.3798
5 out of 10: 0.8475
6 out of 10: 0.9897
7 out of 10: 0.99996
8 out of 10: 0.99991
9 out of 10: 0.9998
10 out of 10:0.99996
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again the model is confused with the worst reviews, presumably because they just don’t occur in the training data. Then the model seems to deal with 2,3 and 4 out of 10 sensibly. But then it has big jump and thinks the rest are definitely positive, with anything above 7 out of 10 being considered positive with high confidence. Again, the explanation for these unusual evaluations of the reviews must be that they are uncommon in the dataset.&lt;/p&gt;

&lt;h3 id=&quot;variations-on-x-out-of-5&quot;&gt;Variations on ‘x out of 5’&lt;/h3&gt;
&lt;p&gt;I tried a few little variations on ‘x out of 5’ to see if it would make any impact: adding an exclamation mark at the end, adding the word ‘stars’ at the end, and adding the word ‘Only’ at the start.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;x&lt;/th&gt;
      &lt;th&gt;‘x out of 5’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘x out of 5!’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘x out of 5 stars’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘Only x out of 5’&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td&gt;0.5570&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7436&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7247&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0240&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;0.4760&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.6756&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.6538&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0247&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;0.3281&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.4901&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.4884&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td&gt;0.4038&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5670&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5634&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0189&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td&gt;0.4073&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5634&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5589&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;0.8569&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.9051&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8969&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5464&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Adding an exclamation mark makes the model think the reviews are more positive, which in my opinion is sensible.&lt;/li&gt;
  &lt;li&gt;Adding the word ‘stars’ seems to have very similar effect to adding an exclamation mark. I do not know if this is just a coincidence - I do not have intuition for this. I guess it must mean that if the word ‘star’ appeared in the training data, then it mostly meant it was a positive review.&lt;/li&gt;
  &lt;li&gt;Adding the word ‘only’ dramatically reduced the positivity score, which makes sense.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;miscellaneous-statements&quot;&gt;Miscellaneous statements&lt;/h3&gt;
&lt;p&gt;Lastly, I tried a variety of miscellaneous statements:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I liked some bits and I disliked other bits
0.9345

I disliked some bits and I liked other bits
0.8857

I liked most of it, but not all of it
0.9906

I disliked most of it, but not all of it
0.9782

I hated most of it, but not all of it
0.9486

I disliked the movie
0.7621

I hated the movie
0.6873

I cannot believe how bad the movie is
0.5535
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model is comically bad at judging whether the statement is positive. I started by trying a neutral statement, but the model thought it was great.  I then tried the favourable statement ‘I liked most of it…’ and the model thought it was positive (sensible), but still thought the statement was positive when I switched liked and disliked. Using the word ‘hated’ instead reduced it a bit.&lt;/p&gt;

&lt;p&gt;Maybe the model hasn’t even learnt the word ‘disliked’ and ‘hated’. So I just tried the simple statements ‘I dislike/hated the movie’, and the model does reduce its score, but still thinks these are overall positive. I end with a particularly strong negative review, which the model perceives as slightly positive.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;It is nice to play around with a model without any expectation to understand the inner-intricacies or working. My main takeaways are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model has learnt some sensible patterns, e.g. adding ‘only’ indicates a negative review, adding an exclamation mark indicates a positive review, etc.&lt;/li&gt;
  &lt;li&gt;The model fails spectacularly badly on several examples, which do not feel contrived to me. This makes me question the reliability of sentiment analysis. Maybe the examples I created are not representative of real-world examples.&lt;/li&gt;
  &lt;li&gt;If I were a decision-maker in some business, I would definitely be using manual analysis until I see better evidence.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Other posts in series FastAI Course, Part III, Frustrations with creating an image classifier FastAI Course, Part I, Lessons 1 and 2</summary></entry><entry><title type="html">Increasing the resolution of an image using an SRGAN</title><link href="https://lovkush-a.github.io/blog/python/neural%20network/2020/08/28/srgan1.html" rel="alternate" type="text/html" title="Increasing the resolution of an image using an SRGAN" /><published>2020-08-28T00:00:00-05:00</published><updated>2020-08-28T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/neural%20network/2020/08/28/srgan1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/neural%20network/2020/08/28/srgan1.html">&lt;!---
## Other posts in series




















































* [Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle](/blog/python/data%20science/2020/06/25/creditcard6.html)















* [Investigating Credit Card Fraud, Part V, Final Models](/blog/python/data%20science/2020/05/30/creditcard5.html)



* [Investigating Credit Card Fraud, Part IV, `n_estimators`](/blog/python/data%20science/2020/05/29/creditcard4.html)







* [Investigating Credit Card Fraud, Part III, Handmade Model](/blog/python/data%20science/2020/05/19/creditcard3.html)



* [Investigating Credit Card Fraud, Part II, Removing data](/blog/python/data%20science/2020/05/16/creditcard2.html)





* [Investigating Credit Card Fraud, Part I, First Models](/blog/python/data%20science/2020/05/14/creditcard1.html)










--&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I wanted to print out an image for a friend, and I wanted to first crop the image to only include a relatively small part of the photo (namely, the individuals in the photo. The original photo was 90% scenary).  However, after the cropping, the image became too small and would have been pixelated when printed.  I thought that a neural network ought to be able to solve this problem, because it could be trained on a manually created dataset (just take a bunch of images and use these as the &lt;em&gt;outputs&lt;/em&gt;, then reduce the quality of these images to create the &lt;em&gt;inputs&lt;/em&gt;). I decided to Google around to see what already had been done, and I found out about SRGANs.  At my current level of knowledge, I cannot understand the details of the architecture, so for now, my aim was just to see if I could use a pre-trained network to achieve my goal. After a bit more searching, I found &lt;a href=&quot;https://github.com/HasnainRaz/Fast-SRGAN&quot;&gt;this Github repo&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;steps-taken&quot;&gt;Steps taken&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Create a new directory and environment.&lt;/li&gt;
  &lt;li&gt;Download the github repo.&lt;/li&gt;
  &lt;li&gt;Install the requirements.&lt;/li&gt;
  &lt;li&gt;Create a directory &lt;code class=&quot;highlighter-rouge&quot;&gt;inputs&lt;/code&gt; for input images, and folder &lt;code class=&quot;highlighter-rouge&quot;&gt;outputs&lt;/code&gt; for where the output images will go.&lt;/li&gt;
  &lt;li&gt;Add images to the inputs directory.&lt;/li&gt;
  &lt;li&gt;Run the command `python infer.py –image_dir ‘inputs’ –output_dir ‘outputs’&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;Below are samples of the images, comparing before and after applying the neural network.&lt;/p&gt;

&lt;h3 id=&quot;example-1&quot;&gt;Example 1&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/srgan1_1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;example-2&quot;&gt;Example 2&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/srgan1_2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;example-3&quot;&gt;Example 3&lt;/h3&gt;
&lt;p&gt;&lt;img src=&quot;/blog/images/srgan1_3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is amazing! This feels like magic to me.&lt;/p&gt;

&lt;h2 id=&quot;limits&quot;&gt;Limits&lt;/h2&gt;
&lt;p&gt;I tried applying this to blurry images, expecting it to make the images less blurry. This did not happen:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/srgan1_4.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This reveals how limited my understanding is! After thinking and googling a bit, I think the issue is that resolution and blurriness/sharpness are two separate issues. Previously, I just lumped them together as ‘reasons an image can look bad’. After a bit more searching, it seems I might learn about anti-blurring / sharpening in the FastAI course that I am doing, so that is something to look forward to.&lt;/p&gt;

&lt;p&gt;Finally, I hope I will soon be at at stage where I can understand these intricate architectures, and be able to train my own cutting edge neural networks.&lt;/p&gt;</content><author><name></name></author><summary type="html"></summary></entry><entry><title type="html">Analysing the movies I’ve watched, Part IV, Data visualisation</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/08/24/mymovies4.html" rel="alternate" type="text/html" title="Analysing the movies I've watched, Part IV, Data visualisation" /><published>2020-08-24T00:00:00-05:00</published><updated>2020-08-24T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/08/24/mymovies4</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/08/24/mymovies4.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/09/02/mymovies5.html&quot;&gt;Analysing the movies I’ve watched, Part V, Data visualisation II&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/19/mymovies3.html&quot;&gt;Analysing the movies I’ve watched, Part III, Joining the tables&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/03/mymovies2.html&quot;&gt;Analysing the movies I’ve watched, Part II, Data cleaning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/02/mymovies1.html&quot;&gt;Analysing the movies I’ve watched, Part I, Data collection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;movies-per-year&quot;&gt;Movies per year&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_year.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The most striking thing is how many movies I watched in 2008 and 2009! More than an average of 2 per week. The uptick in 2016 and 2017 can be explained by my mum buying me an Odeon Unlimited card in September 2016. The slight rise in 2018 and 2019 compared to mid 2010s I think is explained by moving into a house where watching movies was a common social activity for the housemates. And 2020 is significantly above average (given it is not complete), and this is explained by the covid-19 pandemic.&lt;/p&gt;

&lt;h2 id=&quot;movies-by-source&quot;&gt;Movies by source&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_source.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The majority of my movie-watching experience is done on Netflix. To help see any other patterns, I tried grouping them together:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_source2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not sure if there is anything noteworthy to say about this. Note that the category ‘internet’ may or may not refer to streaming from dodgy websites.&lt;/p&gt;

&lt;h2 id=&quot;movies-by-source-and-year&quot;&gt;Movies by source and year&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_sourceyear.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Stackplots are awesome! Visually striking, and provides an overall sense of how my movie watching habits changed. Some patterns which are clear from this diagram:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;There is a clear decline in the use of ‘the internet’ to watch movies in 2017.&lt;/li&gt;
  &lt;li&gt;There is a clear rise in the use of Netflix, starting from around 2014.&lt;/li&gt;
  &lt;li&gt;There is a big bulge in cinema viewings in 2016 and 2017, corresponding to when I had an Odeon Unlimited card.&lt;/li&gt;
  &lt;li&gt;There is a big pink bulge, for a surge in ‘other online’ activity. A quick search in the dataframe shows this corresponds to me making a full use of free trials of NowTV.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;movies-by-month&quot;&gt;Movies by month&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_month.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Again, there are some patterns visible in this plot:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;There are generally peaks in December, the holiday period! Looks like I enjoy watching movies when I am back home during the Christmas break.&lt;/li&gt;
  &lt;li&gt;There are smaller peaks in several summer times, also likely due to me watching movies while at home.&lt;/li&gt;
  &lt;li&gt;You can see some peaks at the end of 2016 and start of 2017, corresponding to the odeon unlimited card.&lt;/li&gt;
  &lt;li&gt;You can see the increase in movie watching from March 2020, corresponding to covid-19.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I recently contributed to an open source project Darts, which does Time Series predictions. I was curious to see what patterns it would find. The following is obtained via an exponential smoothing:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies4_month2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The clearest pattern in the model’s prediction is that it predicts peaks in December. Given how small and error-filled the dataset is, I do not think there is much to read in the smaller peaks at other times of the year.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Even with a dataset as noisy as this one, it is still possible to obtain some nice visuals and uncover some overall patterns. My favourite chart is the stackplot showing how the source used to watch movies has changed over the years.&lt;/p&gt;

&lt;p&gt;Another nice thing about this project has been that it included various firsts for me: first stackplot, first time series model (admittedly basic), first pivotting in pandas (to create stack chart), first time grappling with Time formats to create precisely the plot I want (for the plot by month).&lt;/p&gt;

&lt;p&gt;Next time, I will see what patterns there in the subset of data for which I could join it with imdb data.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Analysing the movies I’ve watched, Part V, Data visualisation II Analysing the movies I’ve watched, Part III, Joining the tables Analysing the movies I’ve watched, Part II, Data cleaning Analysing the movies I’ve watched, Part I, Data collection</summary></entry><entry><title type="html">Analysing the movies I’ve watched, Part III, Joining the tables</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/08/19/mymovies3.html" rel="alternate" type="text/html" title="Analysing the movies I've watched, Part III, Joining the tables" /><published>2020-08-19T00:00:00-05:00</published><updated>2020-08-19T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/08/19/mymovies3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/08/19/mymovies3.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/09/02/mymovies5.html&quot;&gt;Analysing the movies I’ve watched, Part V, Data visualisation II&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/24/mymovies4.html&quot;&gt;Analysing the movies I’ve watched, Part IV, Data visualisation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/03/mymovies2.html&quot;&gt;Analysing the movies I’ve watched, Part II, Data cleaning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/02/mymovies1.html&quot;&gt;Analysing the movies I’ve watched, Part I, Data collection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the last post, I described the cleaning that I did, In particular, I had to manually correct over 140 titles in my data that did not occur in the imdb dataset. So I should be ready to join, right? No, of course not, why would things be so simple.&lt;/p&gt;

&lt;h2 id=&quot;the-main-problem&quot;&gt;The main problem&lt;/h2&gt;
&lt;p&gt;Many movie titles occur multiple times in the imdb database, and as far as I know, there is not an automated way to deal with this. To measure the extend of the problem, I added a column to count how many matches there are in the imdb database:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;num_rows_in_imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    returns the number of rows in the imdb dataset that have primaryTitle equal to movie_title.
    if print_rows is True, then also print the rows
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;primaryTitle&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;==&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;movie_title&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;num_rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;print_rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;temp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_rows&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;rows_in_imdb&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nan&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rows_in_imdb'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;movie_title&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;num_rows_in_imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Doing this revealed that most of the entries had multiple occuences in the imdb database! This was not good. I looked into a couple of examples by searching on the imdb website. It looked like many of the repetitions were from matches with TV episodes.  So I repeated the above process but with only the movies from the imdb dataset:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read_csv&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'title.basics.tsv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;sep&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\t&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;na_values&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\\&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;N'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                   &lt;span class=&quot;n&quot;&gt;usecols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tconst'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'primaryTitle'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'startYear'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'titleType'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;imdb&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;titleType&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;isin&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'short'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'movie'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'tvMovie'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;By removing all the TV episodes, the number of matches decreased, but we still have a large problem. Only 351 of my entries have precisely one match. That is a low number indeed.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I have decided to leave the dataset in the mess that it is and move onto the analysis stage. I will look for patterns within my dataframe first, and then look at those 351 entries which should be able to join with the imdb dataset for any patterns there.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Analysing the movies I’ve watched, Part V, Data visualisation II Analysing the movies I’ve watched, Part IV, Data visualisation Analysing the movies I’ve watched, Part II, Data cleaning Analysing the movies I’ve watched, Part I, Data collection</summary></entry></feed>