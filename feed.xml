<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://lovkush-a.github.io/blog/feed.xml" rel="self" type="application/atom+xml" /><link href="https://lovkush-a.github.io/blog/" rel="alternate" type="text/html" /><updated>2020-10-16T18:19:09-05:00</updated><id>https://lovkush-a.github.io/blog/feed.xml</id><title type="html">Lovkush Agarwal</title><subtitle>A blog for my data science learning and projects</subtitle><entry><title type="html">Visualising L1 and L2 regularisation</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg.html" rel="alternate" type="text/html" title="Visualising L1 and L2 regularisation" /><published>2020-10-11T00:00:00-05:00</published><updated>2020-10-11T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/10/11/l1l2reg.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In this &lt;a href=&quot;https://medium.com/@davidsotunbo/ridge-and-lasso-regression-an-illustration-and-explanation-using-sklearn-in-python-4853cd543898&quot;&gt;medium post comparing L1 and L2 regularisation&lt;/a&gt; there is an image showing how L1 regularisation is more likely to make one of the parameters equal to zero than L2 regularisation.&lt;/p&gt;

&lt;p&gt;One of my &lt;a href=&quot;https://faculty.ai/fellowship/&quot;&gt;co-fellows at Faculty&lt;/a&gt; pointed out that this image is not convincing, because it could just be a case of a cherry-picked cost function. As I had never made any effort to properly understanding L1 versus L2 regularisation previously, this was good motivation for me to better to understand.&lt;/p&gt;

&lt;p&gt;The results are bunch of visuals that are below.&lt;/p&gt;

&lt;h2 id=&quot;varying-cost-function-with-parameters-restricted-to-l1-or-l2-balls&quot;&gt;Varying cost function with parameters restricted to L1 or L2 balls&lt;/h2&gt;

&lt;h3 id=&quot;parameters-restricted-to-l1-ball&quot;&gt;Parameters restricted to L1 ball&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_l11.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;parameters-restricted-to-l2-ball&quot;&gt;Parameters restricted to L2 ball&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_l21.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the plots above:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The circular things that are moving around are contour plots of cost functions. They are all convex.&lt;/li&gt;
  &lt;li&gt;The shaded regions are L1 and L2 balls, i.e. all points where the L1 or L2 norm of the parameters are less than some fixed radius r.&lt;/li&gt;
  &lt;li&gt;The red dot is the parameter which minimizes the cost function, given the restriction of being within the ball.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;What can be seen is that restricting the parameters to an L1-ball results in one of the two paramters being zero, most of the time.  The L2-ball has no preference for values and the red dot just tracks the cost function.&lt;/p&gt;

&lt;p&gt;This matches the general descriptions I have seen of L1 regularisation in various blog posts and articles.&lt;/p&gt;

&lt;h2 id=&quot;varying-cost-functions-with-l1-or-l2-regularisations&quot;&gt;Varying cost functions with L1 or L2 regularisations&lt;/h2&gt;
&lt;p&gt;An issue with the above plots is that I have forced my parameters to be within an L1 or L2 ball, but in regularisation, the parameters can have any value, but there is simply a regularisation term added to incentivise the model to reduce the L1 or L2 norm.&lt;/p&gt;

&lt;p&gt;(Having this forced restriction corresponds to have a regularisation term that is zero if the parameters are inside the ball, and infinity if the parameters are outside the ball. So normal regularisation can be thought of as being a smoothed out version of this forced restriction.)&lt;/p&gt;

&lt;p&gt;To check that the insights gained in the above plots do work when we have regularisation I create a couple more plots.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I created a cost function &lt;code class=&quot;highlighter-rouge&quot;&gt;(x-x0)^2 + (y-y0))^2 + r*norm((x,y))&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;r&lt;/code&gt; is a regularisation constant.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;norm&lt;/code&gt; is the L1 norm in the first plot and the L2 norm in the second plot.&lt;/li&gt;
  &lt;li&gt;I determined the coordinates &lt;code class=&quot;highlighter-rouge&quot;&gt;(x', y')&lt;/code&gt; that minimised the cost function above.&lt;/li&gt;
  &lt;li&gt;I added those coordinates to a scatterplot.&lt;/li&gt;
  &lt;li&gt;I then varied &lt;code class=&quot;highlighter-rouge&quot;&gt;x0&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;y0&lt;/code&gt;, producing many cost functions, and plotting the resulting &lt;code class=&quot;highlighter-rouge&quot;&gt;(x', y')&lt;/code&gt; coordinates in the scatterplot.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;optimal-parameters-with-l1-regularisation&quot;&gt;Optimal parameters with L1 regularisation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_l12.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimal-parameters-with-l2-regularisation&quot;&gt;Optimal parameters with L2 regularisation&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_l22.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;We can see in the plots above that the pattern continues. L1 regularisation will force parameters to zero, and L2 regularisation does not have any preferred direction - L2 just wants the length of the vector to be smaller.&lt;/p&gt;

&lt;h2 id=&quot;max-norm&quot;&gt;Max norm&lt;/h2&gt;
&lt;p&gt;So you can check your understanding, imagine how the plots above would look if we replaced the L1and L2 norms with the max norm, &lt;code class=&quot;highlighter-rouge&quot;&gt;max_norm((x,y)) = max(|x|, |y|)&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;No really, take a couple of minutes to think this through. You learn the most by actively engaging with the ideas rather than passively reading somebody else’s thoughts.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;Well, here are the two plots. Minor note, the plots are for the L10 norm, but it is a good enough approximation.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_linf1.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/l1l2reg_linf2.gif&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;Like I said in my recent post on Bayes Rule, there is always a deeper level of understanding available, even for simple concepts. I am glad I produced these plots. I have read before that L1-regularisation ought to have parameters go to zero, but I never really understood it, but now I feel I have some feel for it.  Also, it was good python practice.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case" /><published>2020-10-01T00:00:00-05:00</published><updated>2020-10-01T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/10/01/sgd4.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html&quot;&gt;Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and &lt;strong&gt;S&lt;/strong&gt;GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html&quot;&gt;Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/09/10/sgd1.html&quot;&gt;Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_linear_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After the first post of the series, I believed that the issue with fitting sinusoidal data with a sinusoidal model using Gradient Descent is that it gets stuck in a local minimum where the amplitude is small. I hoped that adding stochasticity would fix it. In Part III, I described the process of adding stochasticity, and that still did not help. Also in Part III, I described how I eventually realised that the issue was with the learning rate.&lt;/p&gt;

&lt;p&gt;I will recap my investigations into fitting sinusoidal data using sinusoidal models with SGD.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In Part I, I described my attempt at using GD and my belief that the failure to fit was due to the model getting stuck in a local minimum where the amplitude is small. I hoped that SGD would fix this problem.&lt;/li&gt;
  &lt;li&gt;I tried using SGD but it did not help (described in Part III).&lt;/li&gt;
  &lt;li&gt;I then tried using regularisation to solve the amplitude issue (described below).&lt;/li&gt;
  &lt;li&gt;I had the idea of investigating the loss function in detail. However, I decided it would be better to separate off this detailed investigation into sinusoidal models into a separate post (this one), and have a post discussing only the stochasticity (Part III).&lt;/li&gt;
  &lt;li&gt;While writing up Part III, for the sake of completeness, I investigated the learning rate. This solved the issue!&lt;/li&gt;
  &lt;li&gt;However, even though I solved the issues, I thought it would still be worthwhile to write-up my experiments with regularisation and also to carry out the investigation into the loss function. So here we are!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;regularisation&quot;&gt;Regularisation&lt;/h2&gt;
&lt;p&gt;Based on the examples I had tried, the amplitude would always tend to zero. Hence, I thought it would be worth adding a regularisation term that punishes having small amplitudes.&lt;/p&gt;

&lt;p&gt;The loss function was &lt;code class=&quot;highlighter-rouge&quot;&gt;loss = mse(y_est, y)&lt;/code&gt;. After the regularisation, it became &lt;code class=&quot;highlighter-rouge&quot;&gt;loss = mse(y_est, y) - parameters_est[0]&lt;/code&gt;.  Why did I choose this regularisation?&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;I believed that the amplitude would naturally tend to small values. Thus, I want to punish small values and encourage large values.&lt;/li&gt;
  &lt;li&gt;Smaller losses should correspond to better models. Therefore, the larger the amplitude, the smaller the loss should be.&lt;/li&gt;
  &lt;li&gt;Subtracting the amplitude from the loss achieves this. (Note that the first element of &lt;code class=&quot;highlighter-rouge&quot;&gt;parameters_est&lt;/code&gt; is the amplitude).&lt;/li&gt;
  &lt;li&gt;By differentiating, this regularisation causes the amplitude to increase by a constant amount each step, so there is a constant upward pressure on the amplitude.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Below is the first result of introducing this regularisation.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;As you can see, there are moments where the model gets close to the data. This got my hopes up, and made me feel like I was onto something.&lt;/p&gt;

&lt;p&gt;I tried various other things to see if I could make it better. I tried changing the weight of the regularisation term. I tried adding other regularisation terms (because in the experiments, it looked like there was now a tendency for the frequency to keep increasing). I can’t remember if I tried other things or not. Suffice it to say, I made no progress.&lt;/p&gt;

&lt;p&gt;Below is an animation of an experiment which involved changing the weight of the regularisation term. I include it only because I thought it was particularly funky and visually interesting.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;visualising-the-loss-function&quot;&gt;Visualising the loss function&lt;/h2&gt;
&lt;p&gt;After failing to get regularisation to work, I decided I should try to visualise the loss function, and find out exactly where the local minima were, and hopefully better understand why things were not working.&lt;/p&gt;

&lt;p&gt;The process I followed was:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create data to be fit. That was just &lt;code class=&quot;highlighter-rouge&quot;&gt;y = sin(x)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Create generic sinusoidal models &lt;code class=&quot;highlighter-rouge&quot;&gt;y_est = a*sin(b*x + c) + d&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Vary the parameters &lt;code class=&quot;highlighter-rouge&quot;&gt;a,b,c,d&lt;/code&gt; and calculate the loss, &lt;code class=&quot;highlighter-rouge&quot;&gt;mse(y, y_est)&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Plot graphs to visualise the loss function&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To begin, I set &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;d&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;0&lt;/code&gt; and varied &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt;. &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; is the amplitude and &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; is the frequency (multiplied by &lt;code class=&quot;highlighter-rouge&quot;&gt;2*pi&lt;/code&gt;) or the coefficient of &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt;. The reason for fixing &lt;code class=&quot;highlighter-rouge&quot;&gt;c&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;d&lt;/code&gt; is that it was the amplitude and the frequency which were giving the most trouble.&lt;/p&gt;

&lt;p&gt;The first animation below shows a sequence of charts. Each individual chart shows how the loss varies with frequency, and from chart to chart the amplitude is changing.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_loss_vs_freq.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;As can be seen from this, there are many local minima, so the model might get stuck in the wrong one. Eye-balling the chart, if the initial frequency is below 0.5 or above 1.7, then gradient descent will push the frequency away from the optimal value of 1. It is not clear why there should be a tendency for the frequency to increase, as we saw in the SGD examples in Part III.&lt;/p&gt;

&lt;p&gt;The next animation is the opposite. For each individual chart, we see how the loss varies with amplitude, and from chart to chart we are modifying the frequency.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_loss_vs_amp.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Fantastic! This I feel like I can understand. For the majority of frequencies, the optimal value for amplitude is zero and the amplitude will just slide its way to that value. Only for a narrow range of frequencies is the optimal value of the amplitude non-zero.&lt;/p&gt;

&lt;p&gt;To summarise, based on these two animations, here is what I would predict:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Regardless of amplitude, there is a narrow band of frequencies which would result in SGD finding the global minimum. Otherwise, you will get stuck in some other local minimum.&lt;/li&gt;
  &lt;li&gt;For ‘small’ and ‘large’ frequencies, the amplitude will want to decay to zero. For a certain range of frequncies, the amplitude will tend towards a sensible value.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As I am writing this up and thinking things through, I am starting to wonder about my conclusion in Part III about the sinusoidal model. In Part III, I concluded that the issue all along was having an inappropriate learning rate, but the two animations above suggest there is more to it. Did I just get lucky and stumble upon starting parameters which fit the criteria I described above, and hence that is why I got the sinusoidal model to fit?  There’s only one way to find out, which is to do more experimentation!&lt;/p&gt;

&lt;h2 id=&quot;investigating-parameter-initialisation&quot;&gt;Investigating parameter initialisation&lt;/h2&gt;
&lt;p&gt;The steps for the investigation are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Create data to be fit and generic model, as above.&lt;/li&gt;
  &lt;li&gt;Initialise the estimated parameters: &lt;code class=&quot;highlighter-rouge&quot;&gt;a=1, b=?, c=0, d=0&lt;/code&gt;. We will be varying the value of initial value of &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Do SGD and visualise the learning&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I start by trying a large value for the frequency, 5.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;So, as predicted, the frequency gets stuck in some sub-optimal value and the amplitude tends to zero. It looks like I did just get lucky in Part III.&lt;/p&gt;

&lt;p&gt;Frequency of 2 and 1.5 is similar:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f15.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Frequency of 1.2:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f12.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;We get the model converging to the data! Though this is to be expected, it is still satisfying to see it actually work. With a bit of manual experimentation, the cut-off between these two behaviours is roughly 1.46.&lt;/p&gt;

&lt;p&gt;How about lower frequencies? A frequency of 0.6 converges to the correct model:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f06.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;And a frequency of 0.5 converges to a different minima:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd4_sin_f05.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Again, this is consistent with the frequncy vs loss charts above, where you can see there are local minima to the left of the global minimum.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This has been a bit of a topsy-turvy learning experience. I am still surprised at how much I learnt from this basic example. And having struggled with this simple example, I better appreciate how impressive it is to get complicated neural networks to learn.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and SGD Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</summary></entry><entry><title type="html">An intuitive but unknown version of Bayes’ Theorem</title><link href="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html" rel="alternate" type="text/html" title="An intuitive but unknown version of Bayes' Theorem" /><published>2020-09-24T00:00:00-05:00</published><updated>2020-09-24T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes</id><content type="html" xml:base="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;One of the big lessons I have learnt is that no matter how well (you think) you understand something, there is almost always a deeper or clearer level of understanding available. Even for trivially simple things, like addition. Today, I will describe my experience with this phenomena in the context of Baye’s Theorem, by going through four different ways I have of conceptualising it. The final way  is surprisingly simple but largely unknown; I learnt about it from this &lt;a href=&quot;https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/#bayesian-updating&quot;&gt;80000 Hours’ interview of Spencer Greenberg&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Recall that Bayes Theorem states that:
&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(A | B) = \frac{P(A) P(B|A)}{P(B)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.53em;vertical-align:-0.52em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.01em;&quot;&gt;&lt;span style=&quot;top:-2.655em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.485em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.52em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;h2 id=&quot;algebraic&quot;&gt;Algebraic&lt;/h2&gt;
&lt;p&gt;For a long time, the main way I thought about Baye’s Theorem was that it was just a consequence of some algebraic re-arrangement:&lt;/p&gt;

&lt;span class=&quot;katex-display&quot;&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace linebreak=&quot;newline&quot;&gt;&lt;/mspace&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mspace linebreak=&quot;newline&quot;&gt;&lt;/mspace&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(B \cap A) = P(A \cap B) \\
P(B) P(A|B) = P(A) P(B|A) \\
P(A|B) = \frac{P(A) P(B|A)}{P(B)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;∩&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mbin&quot;&gt;∩&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2222222222222222em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace newline&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace newline&quot;&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:2.363em;vertical-align:-0.936em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.427em;&quot;&gt;&lt;span style=&quot;top:-2.314em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.677em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.936em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is a derivation of the formula&lt;/li&gt;
  &lt;li&gt;It is easy to follow this argument&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Provides no insight or understanding&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Unfortunately, this is how I treated a lot of mathematics during my teenage and undergraduate years. The purpose of a proof is to establish the truth of something, and once you know it is true, you are free to use it. Somehow, I never stepped back and asked myself if there was more understanding available.&lt;/p&gt;

&lt;h2 id=&quot;changing-the-universe--some-algebra&quot;&gt;‘Changing the universe’ + some algebra&lt;/h2&gt;
&lt;p&gt;Often when dealing with probabilities, you have to determine all the possibilities of the situation (‘the universe of possibilities’) and then determine which of those corresponds to the event you are interested in.&lt;/p&gt;

&lt;p&gt;For example, to determine the probability of getting two heads when you toss a fair coin twice, you might say that there are four total options (that are all equally likely), and one of those is what we are interested in, so the probability is &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;4&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\frac{1}{4}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.190108em;vertical-align:-0.345em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.845108em;&quot;&gt;&lt;span style=&quot;top:-2.6550000000000002em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;4&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.394em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.345em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;How does this relate to Bayes? Well, the way I think about conditional probability is that when calculating the probability of A given B, I am changing the universe so that the possibilities are precisely those that correspond to B. Once I am in this restricted universe, I continue reasoning as normal.&lt;/p&gt;

&lt;p&gt;For example, what is the probability of getting two heads given that at least one of them (but we don’t know which) is heads. In this case, the universe of possibilities is reduced to three options (we have ruled out the option of TT), so the probability is now &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\frac{1}{3}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.190108em;vertical-align:-0.345em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.845108em;&quot;&gt;&lt;span style=&quot;top:-2.6550000000000002em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;3&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.394em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;1&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.345em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Converting this to algebra, we get:
&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo&gt;∩&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) P(B|A)}{P(B)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.53em;vertical-align:-0.52em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.01em;&quot;&gt;&lt;span style=&quot;top:-2.655em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.485em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mbin mtight&quot;&gt;∩&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.52em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.53em;vertical-align:-0.52em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.01em;&quot;&gt;&lt;span style=&quot;top:-2.655em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.485em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.52em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is a derivation of the formula&lt;/li&gt;
  &lt;li&gt;Slightly more insight than the original, e.g. we divide by $P(B)$ because $B$ has become the ‘universe of possibilities’.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Still not particularly insightful&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;adjusting-a-first-estimate&quot;&gt;Adjusting a first estimate&lt;/h2&gt;
&lt;p&gt;A third way I have of thinking about Baye’s Theorem (which if I remember correctly, only arose after being exposed to the fourth way below) starts with the idea that a reasonable default belief or first estimate is:&lt;/p&gt;

&lt;p&gt;&lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(A \vert B) = P(A)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;.&lt;/p&gt;

&lt;p&gt;Why is this a reasonable first estimate?&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The probability of $A$ happening given some information ought to be related to the probability of $A$ happening without any information.&lt;/li&gt;
  &lt;li&gt;If $B$ has no influence on $A$ whatsoever, then this first estimate &lt;em&gt;is&lt;/em&gt; exactly correct. (And this is the basis of the formal definition of two events being independent.)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Now that we have a first estimate, we need adjust it to get the exact probability. This adjustment factor is the role of the term &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;\frac{P(B \vert A)}{P(B)}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1.53em;vertical-align:-0.52em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;&lt;span class=&quot;mopen nulldelimiter&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mfrac&quot;&gt;&lt;span class=&quot;vlist-t vlist-t2&quot;&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:1.01em;&quot;&gt;&lt;span style=&quot;top:-2.655em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.23em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;frac-line&quot; style=&quot;border-bottom-width:0.04em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span style=&quot;top:-3.485em;&quot;&gt;&lt;span class=&quot;pstrut&quot; style=&quot;height:3em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;sizing reset-size6 size3 mtight&quot;&gt;&lt;span class=&quot;mord mtight&quot;&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen mtight&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord mtight&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault mtight&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose mtight&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-s&quot;&gt;​&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;vlist-r&quot;&gt;&lt;span class=&quot;vlist&quot; style=&quot;height:0.52em;&quot;&gt;&lt;span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;mclose nulldelimiter&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;. Why is this a sensible adjustment factor? Let us see when we increase or decrease our first estimate.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;If &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(B \vert A) &amp;gt; P(B)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;&amp;gt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, then the adjustment factor is bigger than 1, so we increase our first estimate. This makes sense, or at least feels intuitive: if $B$ is more likely when $A$ occurs than when $A$ does not occur, then we should increase our estimate of $A$ occuring.&lt;/li&gt;
  &lt;li&gt;If &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(B \vert A) &amp;lt; P(B)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mrel&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;mspace&quot; style=&quot;margin-right:0.2777777777777778em;&quot;&gt;&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;, then the adjustment factor is smaller than 1, so we decrease our first estimate. The makes sense, following similar reasoning to the point above.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;I hope that makes sense - please let me know if it does not, it is somewhat vague and fuzzy. The strange thing about this is that this is the whole point of ‘priors’ and ‘posteriors’ in Bayesian updating. Yet somehow, I never made the intuitive leap that &lt;span class=&quot;katex&quot;&gt;&lt;span class=&quot;katex-mathml&quot;&gt;&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;A&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;∣&lt;/mi&gt;&lt;mi&gt;B&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;annotation encoding=&quot;application/x-tex&quot;&gt;P(A \vert B)&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;&lt;/span&gt;&lt;span class=&quot;katex-html&quot; aria-hidden=&quot;true&quot;&gt;&lt;span class=&quot;base&quot;&gt;&lt;span class=&quot;strut&quot; style=&quot;height:1em;vertical-align:-0.25em;&quot;&gt;&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.13889em;&quot;&gt;P&lt;/span&gt;&lt;span class=&quot;mopen&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot;&gt;A&lt;/span&gt;&lt;span class=&quot;mord&quot;&gt;∣&lt;/span&gt;&lt;span class=&quot;mord mathdefault&quot; style=&quot;margin-right:0.05017em;&quot;&gt;B&lt;/span&gt;&lt;span class=&quot;mclose&quot;&gt;)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; ought to be $P(A)$ with some kind of adjustment. I cannot remember what I thought when I first learnt about Bayesian stats at university, but my guess is that I treated it more mechanically: “If we have these various bits of information, which for formality sake we label with fancy terms like ‘prior distribution’ and ‘likelihood function’, then we can use Baye’s Theorem to calculate this new thing which we label ‘posterior distribution’”.&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Provides insight into what Bayes’ Theorem is actually saying&lt;/li&gt;
  &lt;li&gt;Corresponds to Bayesian statistics / Bayesian updating&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Does not provide a derivation of the formula&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bayes-via-odds-the-unknown-version-of-bayes-theorem&quot;&gt;Bayes via odds: the unknown version of Bayes’ Theorem&lt;/h2&gt;
&lt;p&gt;There are two main ideas in this version of Bayes’ Theorem.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Replacing probabilities with odds&lt;/li&gt;
  &lt;li&gt;Adjusting Bayes’ Theorem to odds&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Instead of giving a general derivation or discussion, I think the easiest way to illustrate and show-off this version of Bayes’ Theorem is by walking through some explicit examples. Before that though, we should first get used to thinking in terms of odds.&lt;/p&gt;

&lt;h3 id=&quot;odds&quot;&gt;Odds&lt;/h3&gt;
&lt;p&gt;When describing uncertainty, the usual way is to list all the possible options along with their probabilities. With odds, we instead assign &lt;em&gt;relative probabilities&lt;/em&gt;, and collectively these relative probabilities are called the odds.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Example. Instead of saying there is a probability of 0.5 of getting H and 0.5 of getting T, we say the odds are 1:1 of getting H and T.&lt;/li&gt;
  &lt;li&gt;Example. Instead of saying the probabilities are 0.25, 0.25 and 0.5 of getting two heads, two tails or one of each, we say the odds are 1:1:2.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Note, I do not actually know what the official phrasing is. I have just come up with something that feels OK and whose meaning is hopefully clear.)&lt;/p&gt;

&lt;p&gt;As a little exercise to check your understanding, determine how you would convert between probabilities and odds.&lt;/p&gt;

&lt;p&gt;No really, I recommend you do this. The best way to learn is by actively doing something with the ideas, rather than passively reading.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;Here is how I would describe the conversions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Given some odds, you determine the probabilities by dividing all the relative probabilities by the sum of the relative probabilities. This process is known as &lt;em&gt;normalising&lt;/em&gt; and the sum of the relative probabilities is the &lt;em&gt;normalising factor&lt;/em&gt; or &lt;em&gt;normalising constant&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;Given some probabilities, there are infinitely many different odds you could create. Say we had probabilities $p_1, p_2$ and $p_3$, then the odds would be any multiple of these three numbers: $ap_1:ap_2:ap_3$. Of course, you should pick $a$ to create the most intuitive list of relative probabilities.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Do not worry if this feels strange. That is to be expected with a different way of looking at something that we are so used to looking at in a certain way.&lt;/p&gt;

&lt;h3 id=&quot;example-1-disease-given-a-positive-test-result&quot;&gt;Example 1. Disease given a positive test result&lt;/h3&gt;
&lt;p&gt;This is a famous question that demonstrates how bad our intuitions are when it comes to reasoning about uncertainty and probabilities. Here are the assumptions:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Without any other information, there is a 1 in a 1000 chance somebody has the disease, independent of everybody else.&lt;/li&gt;
  &lt;li&gt;There is a test for the disease which is 99% effective. Explicitly, 99% of people who have the disease will get a positive test result, and 99% of people who do not have the disease will get a negative test result.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;(Do not think about how one could know these facts. E.g. how can you know the effectiveness of a test if there is not already a test which is 100% effective?)&lt;/p&gt;

&lt;p&gt;The question: you take the test and get a positive result. What is the probability that you have the disease?&lt;/p&gt;

&lt;p&gt;The most common instinctive answer is 99%, because we know the test is 99% effective. However, the error here is getting mixed up between the probability of having the disease given that you have a positive test versus the probability of getting a positive test result given that you have the disease.&lt;/p&gt;

&lt;p&gt;Before continuing, try to answer the question using standard tools and Bayes’ Theorem as usual.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;p&gt;OK, I assume you have tried it. Or at least, that you have done it before. If it was me, I would draw a probability tree and then do the various calculations.&lt;/p&gt;

&lt;p&gt;Now, here is how to answer this question using the magical alternative: Bayes’ via odds.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine the odds without any information, i.e. the prior odds.
    &lt;ul&gt;
      &lt;li&gt;They are 1:999 of having the disease versus not having the disease. (Why is it 999 and not 1000?)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Determine the probabilities of getting a positive test result, conditioned on the possibilities from Step 1.
    &lt;ul&gt;
      &lt;li&gt;If I have the disease, I have 0.99 chance of getting positive result.&lt;/li&gt;
      &lt;li&gt;If I do not have the disease, it is a 0.01 chance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiply the corresponding numbers from Steps 1 and 2 together. Re-scale so numbers are nice.
    &lt;ul&gt;
      &lt;li&gt;From Step 1 we had 1:999. From Step 2 we had 0.99:0.01&lt;/li&gt;
      &lt;li&gt;Multiplying them together gives 0.99:9.99&lt;/li&gt;
      &lt;li&gt;Re-scaling gives 99:999&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Marvel at the fact we’re done. Those final numbers you worked out are the posterior odds!&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;!!! This is amazing! I still find it surreal how straightforward the odds perspective makes the whole process. It is basically magic. (Though, if you did do the question using traditional means, you should see how all the numbers and calculations match up).&lt;/p&gt;

&lt;p&gt;Furthermore, this process makes explicit the effect of the new information. The prior belief is that we have the disease with roughly 1:1000 odds, and the posterior belief is that we have the disease with roughly 1:10 odds. The odds have increased by a factor of 100. By looking at the calculations (which is easy to do because they are so short), we see that this factor of 100 arises as the ratio of values in Step 2. I have a 99% chance of a positive result if I have the disease, and a 1% chance if I do not have the disease, so I should put 99 times more weight on having the disease once I know I have a positive test result.&lt;/p&gt;

&lt;h3 id=&quot;example-2-the-monty-hall-problem&quot;&gt;Example 2. The Monty-Hall Problem&lt;/h3&gt;
&lt;p&gt;Another famous question. I assume you know about it, but here is a brief description of the set-up. You are on a game show. There are three doors. Behind two of the doors are goats and behind the third door is a prize (which you value more than a goat). You initially guess that the prize is behind Door 1. The gameshow host is nice, and reveals that Door 3 has a goat behind it. You are then offered to switch your choice from Door 1 to Door 2.&lt;/p&gt;

&lt;p&gt;Question: Should you stay on Door 1, switch to Door 2, or does it make no difference (statistically speaking)?&lt;/p&gt;

&lt;p&gt;This question is a real brain-burner because, counter-intuitively, the answer is that you should switch. The most common thought is that once Door 3 is ruled out, there is a 50:50 chance of the prize being behind Door 1 or Door 2, so it makes no difference if you switch or not.&lt;/p&gt;

&lt;p&gt;Here, I will illustrate how using odds, we can gain some understanding of this situation.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Determine the odds without any information (i.e. before Door 3 was revealed).
    &lt;ul&gt;
      &lt;li&gt;The odds are 1:1:1 for the prize being behind Doors 1, 2 and 3 respectively.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Determine the probabilities of Door 3 being revealed, conditioned on the events in Step 1.
    &lt;ul&gt;
      &lt;li&gt;If the prize is behind Door 1, then there is a 50% chance of Door 3 being opened by the host. (The host picks randomly if they have a choice).&lt;/li&gt;
      &lt;li&gt;If the prize is behind Door 2, then there is a 100% chance of Door 3 being opened by the host. (The host will never open the door you originally guessed.)&lt;/li&gt;
      &lt;li&gt;It the prize is behind Door 3, then there is 0% chance of Door 3 being opened by the host. (The host always reveals a goat)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Multiply the corresponding values to generate the posterior odds.
    &lt;ul&gt;
      &lt;li&gt;0.5:1:0&lt;/li&gt;
      &lt;li&gt;Re-scaling we get, 1:2:0.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The prize is twice as likely to behind Door 2 as it is Door 1, so you should switch.&lt;/p&gt;

&lt;p&gt;Note, I do not think this is the most intuitive explanation of the Monty-Hall Problem. But I hope it is an illustration of the incredible ease of this odds-based Bayes’ Theorem.&lt;/p&gt;

&lt;h3 id=&quot;exercises&quot;&gt;Exercises&lt;/h3&gt;
&lt;p&gt;As I said above, the best way to learn is by actively engaging with the ideas, rather than passively reading. Hence, here are some exercises to try out.&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Formulate a statement of Bayes’ Theorem in this odds framework. Then prove it.&lt;/li&gt;
  &lt;li&gt;A bag contains 1000 coins - 999 are fair coins but one of them is dodgy and has two heads. You take one out randomly, toss it five times-in-a-row, and get heads each time. What is the probability you have the dodgy coin? How many heads in a row would you have to observe so that you think it is more likely you have a dodgy coin than not?&lt;/li&gt;
  &lt;li&gt;You meet somebody. They tell you they have two kids and that at least one of them is born on a Tuesday. What is the probability the other is born on Friday?&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;pros-and-cons&quot;&gt;Pros and Cons&lt;/h3&gt;
&lt;p&gt;Pros&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Easy to use&lt;/li&gt;
  &lt;li&gt;Better matches how Bayesian reasoning is used in real-life. You want to update all probabilities, not just a single one.&lt;/li&gt;
  &lt;li&gt;The ease of use makes explicit exactly how the new information changes the relative probabilities.&lt;/li&gt;
  &lt;li&gt;It reveals that $P(B)$ in the standard formulation has no information, it is just a normalising constant.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is not a derivation.&lt;/li&gt;
  &lt;li&gt;It requires an unfamiliar change in perspective.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;final-remarks&quot;&gt;Final remarks&lt;/h2&gt;
&lt;p&gt;I hope you gained some new insights into Bayes’ Theorem. If you have any other perspectives on it, please let me know! As I said at the start, it is always surprising how things that you feel you understand contain hidden layers and deeper insights.&lt;/p&gt;

&lt;p&gt;Lastly, I recommend you listen to the &lt;a href=&quot;https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/#bayesian-updating&quot;&gt;80000 Hours’ interview of Spencer Greenberg&lt;/a&gt;. First, you will hear Spencer’s description and perspectives on this odds framework. Second, you will learn a whole bunch of other insightful things: Spencer researches thinking and rationality and tries to develop software tools that take make use of his insights. For example, Spencer discusses their research on when people are over- or under-confident.&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction One of the big lessons I have learnt is that no matter how well (you think) you understand something, there is almost always a deeper or clearer level of understanding available. Even for trivially simple things, like addition. Today, I will describe my experience with this phenomena in the context of Baye’s Theorem, by going through four different ways I have of conceptualising it. The final way is surprisingly simple but largely unknown; I learnt about it from this 80000 Hours’ interview of Spencer Greenberg.</summary></entry><entry><title type="html">Squash rankings, Part I, Scraping wikipedia and data analysis</title><link href="https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1.html" rel="alternate" type="text/html" title="Squash rankings, Part I, Scraping wikipedia and data analysis" /><published>2020-09-17T00:00:00-05:00</published><updated>2020-09-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;I am going through the CS109 Harvard lectures on data science. I just watched a couple of lectures on web-scraping with BeautifulSoup, so I wanted to practice. I decided to scrape squash ranking data from wikipedia, as I am a avid fan of the sport. On wikipedia, the best information I could find was the top 10 players at the end of each year for the past 25 years or so.&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;
&lt;p&gt;The results of the scraping process are in the following tables, summarising some key stats for the players. The tables are ordered by players’ average rank in the dataset. It is worth emphasising that the data only contains information on Top 10 rankings and only at the end of each year; this will skew the data in various ways.&lt;/p&gt;

&lt;h3 id=&quot;summary-for-female-players&quot;&gt;Summary for female players&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|                  player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year |
|------------------------:|-------------:|---------------:|----------:|-----------:|--------------:|------------:|
|         Michelle Martin |     1.400000 |              5 |         1 |          2 |          1994 |        1998 |
|       Sarah Fitz-Gerald |     2.333333 |              9 |         1 |          5 |          1994 |        2002 |
|             Nicol David |     2.357143 |             14 |         1 |          6 |          2004 |        2017 |
|       Raneem El Weleily |     2.444444 |              9 |         1 |          7 |          2011 |        2019 |
|          Leilani Rorani |     2.750000 |              4 |         1 |          7 |          1998 |        2001 |
|        Nour El Sherbini |     3.142857 |              7 |         1 |          6 |          2012 |        2019 |
|         Rachael Grinham |     3.909091 |             11 |         1 |          8 |          2001 |        2011 |
|             Carol Owens |     4.100000 |             10 |         1 |          8 |          1994 |        2003 |
|           Laura Massaro |     4.111111 |              9 |         2 |          9 |          2010 |        2018 |
|          Cassie Jackman |     4.545455 |             11 |         2 |          8 |          1994 |        2004 |
|         Natalie Grinham |     4.666667 |              9 |         2 |          9 |          2003 |        2013 |
|        Natalie Grainger |     5.100000 |             10 |         3 |          7 |          1999 |        2009 |
|              Sue Wright |     5.250000 |              4 |         4 |          8 |          1994 |        1998 |
|           Jenny Duncalf |     5.375000 |              8 |         2 |          9 |          2005 |        2013 |
|            Nouran Gohar |     5.400000 |              5 |         3 |          9 |          2015 |        2019 |
|           Linda Elriani |     5.555556 |              9 |         3 |          9 |          1997 |        2005 |
|          Suzanne Horner |     5.625000 |              8 |         2 |          9 |          1994 |        2001 |
|            Tania Bailey |     5.666667 |              6 |         5 |          9 |          1999 |        2007 |
|        Vanessa Atkinson |     5.875000 |              8 |         1 |         10 |          2002 |        2010 |
|           Camille Serme |     5.888889 |              9 |         3 |         10 |          2010 |        2019 |
|           Nour El Tayeb |     6.000000 |              5 |         3 |          8 |          2014 |        2019 |
|             Kasey Brown |     6.000000 |              2 |         5 |          7 |          2010 |        2011 |
|              Liz Irving |     6.200000 |              5 |         3 |         10 |          1994 |        1998 |
|           Alison Waters |     6.400000 |             10 |         3 |         10 |          2008 |        2018 |
|             Joelle King |     6.666667 |              6 |         4 |         10 |          2012 |        2019 |
|         Vicky Botwright |     7.000000 |              4 |         5 |          9 |          2003 |        2007 |
|            Amanda Sobhy |     7.000000 |              2 |         7 |          7 |          2016 |        2019 |
|        Sarah-Jane Perry |     7.000000 |              3 |         6 |          8 |          2017 |        2019 |
|            Low Wee Wern |     7.333333 |              3 |         7 |          8 |          2012 |        2014 |
|          Madeline Perry |     7.428571 |              7 |         4 |         10 |          2006 |        2013 |
|          Sabine Schoene |     7.500000 |              4 |         6 |          9 |          1995 |        1998 |
|       Omneya Abdel Kawy |     7.900000 |             10 |         4 |         10 |          2004 |        2016 |
|            Fiona Geaves |     8.444444 |              9 |         5 |         10 |          1994 |        2004 |
| Laura Lengthorn-Massaro |     8.500000 |              2 |         8 |          9 |          2008 |        2009 |
|                Annie Au |     8.750000 |              4 |         8 |         10 |          2011 |        2015 |
|             Tesni Evans |     9.000000 |              2 |         9 |          9 |          2018 |        2019 |
|          Rebecca Macree |     9.250000 |              4 |         8 |         10 |          2001 |        2004 |
|         Stephanie Brind |     9.250000 |              4 |         7 |         10 |          1999 |        2003 |
|            Claire Nitch |     9.333333 |              3 |         9 |         10 |          1994 |        1996 |
|             Jane Martin |     9.500000 |              2 |         9 |         10 |          1994 |        1995 |
|        Hania El Hammamy |    10.000000 |              1 |        10 |         10 |          2019 |        2019 |
|         Shelley Kitchen |    10.000000 |              2 |        10 |         10 |          2007 |        2008 |
|         Dipika Pallikal |    10.000000 |              1 |        10 |         10 |          2012 |        2012 |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;summary-for-male-players&quot;&gt;Summary for male players&lt;/h3&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;|                 player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year |
|-----------------------:|-------------:|---------------:|----------:|-----------:|--------------:|------------:|
|            Peter Nicol |     2.400000 |             10 |         1 |          8 |          1996 |        2005 |
|              Ali Farag |     3.250000 |              4 |         1 |          7 |          2016 |        2019 |
|           Jansher Khan |     3.333333 |              3 |         1 |          8 |          1996 |        1998 |
|         Jonathon Power |     3.666667 |              9 |         1 |          9 |          1997 |        2005 |
|    Mohamed El Shorbagy |     3.700000 |             10 |         1 |         10 |          2010 |        2019 |
|            Ramy Ashour |     3.909091 |             11 |         1 |          7 |          2006 |        2016 |
|       Grégory Gaultier |     4.000000 |             15 |         1 |         10 |          2003 |        2018 |
|           Ahmed Barada |     4.250000 |              4 |         2 |          7 |          1997 |        2000 |
|           Rodney Eyles |     4.333333 |              3 |         2 |          7 |          1996 |        1998 |
|           Nick Matthew |     4.642857 |             14 |         1 |         10 |          2004 |        2017 |
|           David Palmer |     4.727273 |             11 |         1 |          9 |          2000 |        2011 |
|            Amr Shabana |     4.727273 |             11 |         1 |          9 |          2004 |        2014 |
|           Paul Johnson |     5.000000 |              3 |         4 |          7 |          1998 |        2000 |
|        Stewart Boswell |     5.000000 |              2 |         4 |          6 |          2001 |        2002 |
|         Thierry Lincou |     5.200000 |             10 |         1 |          9 |          2001 |        2010 |
|        James Willstrop |     5.272727 |             11 |         1 |         10 |          2005 |        2017 |
|       Anthony Ricketts |     5.500000 |              4 |         3 |          7 |          2002 |        2006 |
|          Karim Darwish |     5.600000 |             10 |         1 |          9 |          2003 |        2013 |
|      Karim Abdel Gawad |     5.666667 |              6 |         2 |          9 |          2015 |        2019 |
|           Martin Heath |     5.666667 |              3 |         5 |          6 |          1998 |        2000 |
|             Del Harris |     6.000000 |              2 |         6 |          6 |          1996 |        1997 |
|             Dan Jenson |     6.000000 |              1 |         6 |          6 |          1998 |        1998 |
|     Marwan El Shorbagy |     6.250000 |              4 |         5 |          9 |          2016 |        2019 |
|             John White |     6.571429 |              7 |         2 |         10 |          1999 |        2007 |
|              Paul Coll |     6.666667 |              3 |         5 |          8 |          2017 |        2019 |
|            Omar Mosaad |     6.666667 |              3 |         4 |          8 |          2012 |        2016 |
|            Tarek Momen |     6.800000 |              5 |         4 |         10 |          2014 |        2019 |
|           Lee Beachill |     6.800000 |              5 |         1 |         10 |          2002 |        2006 |
| Miguel Ángel Rodríguez |     7.000000 |              3 |         5 |         10 |          2015 |        2019 |
|            Diego Elias |     7.000000 |              1 |         7 |          7 |          2019 |        2019 |
|       Stefan Casteleyn |     7.000000 |              1 |         7 |          7 |          1999 |        1999 |
|            David Evans |     7.000000 |              2 |         4 |         10 |          2000 |        2001 |
|            Simon Parke |     7.000000 |              5 |         3 |         10 |          1996 |        2000 |
|          Craig Rowland |     7.000000 |              1 |         7 |          7 |          1996 |        1996 |
|           Chris Walker |     7.000000 |              2 |         4 |         10 |          1996 |        1997 |
|           Brett Martin |     7.000000 |              2 |         5 |          9 |          1996 |        1997 |
|            Borja Golán |     7.000000 |              2 |         7 |          7 |          2013 |        2014 |
|           Peter Barker |     7.142857 |              7 |         5 |          9 |          2008 |        2014 |
|           Anthony Hill |     7.333333 |              3 |         5 |          9 |          1996 |        1999 |
|           Simon Rösner |     7.500000 |              6 |         3 |         10 |          2014 |        2019 |
|           Ong Beng Hee |     7.666667 |              3 |         7 |          8 |          2001 |        2003 |
|          Mark Chaloner |     8.666667 |              3 |         8 |         10 |          1996 |        2002 |
|     Laurens Jan Anjema |     9.000000 |              1 |         9 |          9 |          2010 |        2010 |
|             Alex Gough |     9.000000 |              2 |         9 |          9 |          1999 |        2000 |
|          Wael El Hindi |     9.000000 |              3 |         8 |         10 |          2007 |        2009 |
|      Mathieu Castagnet |     9.000000 |              2 |         9 |          9 |          2015 |        2016 |
|             Paul Price |     9.500000 |              2 |         9 |         10 |          2000 |        2001 |
|             Derek Ryan |    10.000000 |              1 |        10 |         10 |          1998 |        1998 |
|    Mohd Azlan Iskandar |    10.000000 |              1 |        10 |         10 |          2011 |        2011 |
|     Mohamed Abouelghar |    10.000000 |              1 |        10 |         10 |          2018 |        2018 |
|            Daryl Selby |    10.000000 |              2 |        10 |         10 |          2012 |        2013 |
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;patterns-and-observations&quot;&gt;Patterns and observations&lt;/h2&gt;
&lt;p&gt;It is satisfying to be able to compare the various big names in squash.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;From this, the most outstanding player is Nicol David. They have the largest number of years in the top 10 of any female player by a big margin, and she has the 3rd best average rating. Only two players have a higher average rating: Sarah Fitz-Gerald’s average is for practical purposes the same (2.33 vs 2.35) and Michelle Martin’s is probably skewed by the fact the data only goes as far back as 1994.&lt;/li&gt;
  &lt;li&gt;A stand-out statistic is that Gaultier has been in the Top 10 for 15 years! Though it is mentioned by commentators frequently, it is only when I compare it to other players’ durations do I fully appreciate how incredible the achievement is. Nick Matthew is not far behind with 14 years.&lt;/li&gt;
  &lt;li&gt;Another surprise for me is how high Ramy Ashour is, compared to other players. It is well-recognised that he is the best player of his generation by a large margin, but he has also been plagued by injury for most of it, too. I would have predicted that it would have had a noticable dent on his stats. It is scary to think how much better his stats would have been if he did not have injuries!&lt;/li&gt;
  &lt;li&gt;Ali Farag’s average is very high. Though I do not want to diminish this achievement, I think this is a reflection of how the modern game has fewer elite players, whereas ten years ago, 7 or 8 of the top 10 players had all achieved a World Ranking of 1.&lt;/li&gt;
  &lt;li&gt;It is interesting to see general patterns. E.g. the players who have spent the most years in the top 10 are also players with higher averages and higher maximum ranks. For the males, this pattern is stark: if they have spent at least 9 years in the Top-10 rankings then they have reached the No. 1 spot.  For the females, the pattern is not as clear. This might suggeset that female squash has had a few players that have dominated the top spot, with the remaining players competiting for the other spots in the top 10.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Like I said earlier, the data is skewed in various ways.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The data only includes rankings at the end of each year. This will hide variations throughout the year. If you use monthly data instead, I expect the patterns will be clearer, and the best players will stand out even more from the good players.&lt;/li&gt;
  &lt;li&gt;The data only includes rankings that are in the top 10. For the absolute best players, this is not a loss of much data, but for the players in the 5-10 range, significant data is missing about their ranking history.&lt;/li&gt;
  &lt;li&gt;The data only goes back to the early 90s. This skews data by missing out the achievements of previous great players. The main one is Jahangir Khan, who had a 500+ match winning streak in the 80s!&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;next-steps&quot;&gt;Next steps&lt;/h2&gt;
&lt;p&gt;I will see if I can find more detailed ranking information, so I can get a more fine-grained analysis of the players. A project I have in the back of my mind is to create my own ranking system based on match history, and see if I can create a system which is more predictive than the current system. I think this should be possible, but again, I will need to see if I can obtain the relevant data.&lt;/p&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;requests&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;bs4&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;IPython.core.display&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;HTML&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;pandas&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;numpy&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'https://en.wikipedia.org/wiki/Official_Women&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%27&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s_Squash_World_Ranking'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;'https://en.wikipedia.org/wiki/Official_Men&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;%27&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;s_Squash_World_Ranking'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;is_not_numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;float&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;s&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;ValueError&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;table_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;table&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'tr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'th'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]]&lt;/span&gt;
    
    &lt;span class=&quot;n&quot;&gt;n_cols&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'td'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;    
    
    &lt;span class=&quot;n&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;col&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'td'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
             &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_not_numeric&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;col&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;replace&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;''&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;
            &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
            &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;row&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;rows&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;
           &lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;DataFrame&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;headers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;index&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;11&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;url_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;&quot;&quot;
    headers contains id needed to cut html into two pieces
    &quot;&quot;&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;get&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;text&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;start&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id=&quot;Year_end_world_top_10_players'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;end&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'id=&quot;Year-end_number_1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;tables&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;BeautifulSoup&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;html&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;start&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;end&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'html.parser'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;find_all&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'table'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;concat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;table_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;t&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;t&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tables&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;axis&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stack&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;reset_index&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'player'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_stack&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#     years_in_top10 = df.player.value_counts()
&lt;/span&gt;    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;groupby&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'player'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
               &lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;agg&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;({&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;mean&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'count'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt;
                     &lt;span class=&quot;s&quot;&gt;'year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;np&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
                    &lt;span class=&quot;p&quot;&gt;})&lt;/span&gt;
              &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;columns&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'average_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'years_in_top10'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'best_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'worst_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
                       &lt;span class=&quot;s&quot;&gt;'earliest_year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'latest_year'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;sort_values&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;by&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'average_rank'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;inplace&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;players&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;df_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;df_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;url_to_pandas&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;urls&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;range&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)]&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;players_f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;players_m&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;player_summaries&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;df_m&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;</content><author><name></name></author><summary type="html">Other posts in series</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and **S**GD</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and **S**GD" /><published>2020-09-17T00:00:00-05:00</published><updated>2020-09-17T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/10/01/sgd4.html&quot;&gt;Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html&quot;&gt;Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/09/10/sgd1.html&quot;&gt;Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous posts, I used gradient descent to model linear, quadratic and sinusoidal data. In the first post, the linear and quadratic models could be fit, but the sinusoidal data could not be fit.  In the second post, we saw how neural networks kind of fit the data, but not very well.&lt;/p&gt;

&lt;p&gt;This time, I will add the stochasticity by introducing mini-batches. My hope is that I will be able to fit the sinusoidal data that I could not fit in the first post. I will discuss this example at the end, because it is the most interesting&lt;/p&gt;

&lt;h2 id=&quot;models-with-neural-networks&quot;&gt;Models with neural networks&lt;/h2&gt;
&lt;p&gt;Here are animations of a neural network trying to fit using stochastic gradient descent.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_linearnn.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_quadraticnn.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sinnn.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The end results look similar to the end results without using mini-batches. The big difference is that the network converges much faster.&lt;/p&gt;

&lt;h2 id=&quot;fitting-sinusoidal-data-using-a-sinusoidal-model&quot;&gt;Fitting sinusoidal data using a sinusoidal model&lt;/h2&gt;
&lt;p&gt;Here is a representative example of my first attempts using SGD on sinusoidal data. Note that in all of the animations in this section, the same dataset and initial parameters were used, so the comparisons are more rigorous.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sin1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;To my dismay, we seem to have the same problem as with normal gradient descent. The system gets stuck in some local minimum where the amplitude is small. It still continues to ‘learn’ though.&lt;/p&gt;

&lt;p&gt;At this point, I experimented a little (e.g. with some ‘regularisation’), which I plan to describe in a separate post (spoiler alert - they didn’t work). While planning this blogpost, I re-watched the animation above and thought that maybe the learning rate is too big. I presumably tried playing with the learning rate already but for the sake of completeness, I thought it would be good to produce a series of animations to show you that varying the learning rate does not help.&lt;/p&gt;

&lt;p&gt;The learning rate in the animation above was 0.1.  Below is an animation for a learning rate of 0.01.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sin2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;! That was unexpected! It managed to find good parameters, but then jumped to some inferior local minimum. I managed to achieve something similar to this using the regularisation mentioned above (and which I will describe in a later post), but I was not expecting to see this by changing the learning rate. Clearly my memory is off and I had not experimented with the learning rate. I thought I would re-run the calculations to see if the same behaviour would occur again. (Note that the initial parameters are the same in all these animations, but there is still randomness from how the mini-batches are selected.)&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sin2b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;So we get similar behaviour. For some time it looks like we are getting close to a good model but then it jumps away to some other set of parameters.&lt;/p&gt;

&lt;p&gt;Looks like I should make the learning rate smaller, and see if that prevents jumping away from the correct model. The next animation is for a learning rate of 0.001.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sin3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;!!! Wow! Given that I had failed after several hours of trying, this was basically magic to me. The model gently and slides its way into position, increases its amplitude, then stays there. Fantastic!&lt;/p&gt;

&lt;p&gt;Now a big question arises. Were the learning rates in the first post of this series too high, and that was the reason for the struggles with sinusoidal models? There’s only one way to find out, and that’s by doing the experiment. Below is the resulting animation.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd3_sin3b.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;!!! All this time, it was as simple as changing the learning rate. How did I miss this?! What is noteworthy is how slow the learning is in gradient descent as compared to stochastic gradient descent.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;There are two big lessons I learnt from this.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;The first is to somehow take good notes of what I have tried and to be systematic. In my previous job teaching maths to STEM Foundation Year students, my colleague who taught Laboratory Skills was trying to explain the purpose of a lab-book to students: it should be a record of what you did, why you did it, what you observed, etc. so that somebody else (in particular, future-you) can read it and re-live your experience. It looks like I have only now learnt this lesson my colleague was trying to teach. Better late than never, I suppose.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The second is that the main benefit of stochastic gradient descent seems to be in efficiency/speed. I have read in places that it can help with preventing local minimums, but I am still unsure of this latter point.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD" /><published>2020-09-11T00:00:00-05:00</published><updated>2020-09-11T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/10/01/sgd4.html&quot;&gt;Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html&quot;&gt;Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and &lt;strong&gt;S&lt;/strong&gt;GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/09/10/sgd1.html&quot;&gt;Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in &lt;a href=&quot;http://neuralnetworksanddeeplearning.com/chap4.html&quot;&gt;Michael Nielsen’s online book on neural networks&lt;/a&gt;.) Therefore, it should be possible for a neural network to model the datasets I created in the first post, and it should be interesting to see the visualisations of the learning taking place.&lt;/p&gt;

&lt;h2 id=&quot;linear-data&quot;&gt;Linear data&lt;/h2&gt;
&lt;p&gt;I created some linear data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x + b + noise&lt;/code&gt;, and then tried to fit a neural network to it.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This is enchanting. I never thought I’d see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again.&lt;/p&gt;

&lt;p&gt;You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-5&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-4&lt;/code&gt;) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well.&lt;/p&gt;

&lt;p&gt;I next tried increasing the learning rate. This next video is an example when the learning rate was &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; throughout.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the first one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate.&lt;/p&gt;

&lt;p&gt;I next tried introducing a cutoff point where the learning rate increases from &lt;code class=&quot;highlighter-rouge&quot;&gt;1e-3&lt;/code&gt; to &lt;code class=&quot;highlighter-rouge&quot;&gt;3e-3&lt;/code&gt;. I have two examples of this.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_linearnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;In both of these videos, there is a moment in the middle where things go a bit wacky for a few iterations and then quickly settle back down. This wacky moment occurs precisely when the increase in the learning rate kicks in. My explanation for this is that we are seeing the algorithm jump away from one local minimum and moving towards another.&lt;/p&gt;

&lt;p&gt;One thing I should have said is that in each of these videos, the training dataset remains the same, but the initialisation of the parameters of the neural network are different. So one other thing we are witnessing from these experiments is how different initialisation results in different models using gradient descent - i.e. there are many local minimums in the parameter space!&lt;/p&gt;

&lt;h2 id=&quot;quadratic-data&quot;&gt;Quadratic data&lt;/h2&gt;
&lt;p&gt;I created some quadratic data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x*x + b*x + c + noise&lt;/code&gt; and tried to fit a neural network to it. Below are three examples. Note that as above, the dataset is staying the same each time, but the parameters are initialised differently each time and I play with the learning rates a bit, too.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_quadraticnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;All these examples have similar overall behaviour. It is quick to get close-ish to the quadratic, and then the learning dramatically slows (but there is still learning going on throughout). Again we see how the different initialisations leads to different final models, showing how we are finding different local minimums.&lt;/p&gt;

&lt;h2 id=&quot;sinusoidal-data&quot;&gt;Sinusoidal data&lt;/h2&gt;
&lt;p&gt;I created some sinusoidal data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*sin(b*x+ c) + d&lt;/code&gt; and tried to fit a neural network to it. As before, same dataset but with different starting parameters.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_3.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The first one shows some oscillating behaviour, similar to the first sinusoidal case in the first post of this series. Looks like the behaviour is not as unlikely as I previously thought. The second two produce mediocre results, at best.&lt;/p&gt;

&lt;p&gt;Looking at the graphs, there seem to be ‘too many wiggles’, so I thought I’d try reducing the number of neurons in the hidden layer. The next three videos show what happens with eight neurons.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_4.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_5.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd2_sinnn_6.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;These perform less well than with twenty neurons. The final video is interesting for its crazy behaviour at the start - the function seems to spin around and do all kinds of weird stuff before settling down. I do not know what to make of that.&lt;/p&gt;

&lt;p&gt;There is lots more experimentation that can be done, but I feel this is a good place to stop and move on to other things to investigate.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;There are various things I learnt doing this.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;There is a lot to be learnt by playing around with the ideas. Though (I think) I understand the theory of gradient descent and of vanilla neural networks, it is evident that the whole is greater than the sum of its parts. I under-estimated what I could have learnt by experimenting.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;It seems neural networks are only useful for interpolation - i.e. making predictions for data similar to the training data. They are hopeless at extrapolating: I think they necessarily have horizontal asymptotes in both x-directions.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The initialisation of parameters matters a lot for neural networks.
    &lt;ul&gt;
      &lt;li&gt;My very first settings were useless, and were seemingly only capable of modelling a sigmoid function. For a while, I thought my code was buggy, but after some experimentation, it turned out the initial parameter settings were inappropriate.&lt;/li&gt;
      &lt;li&gt;Different initialisations resulted in vastly different final models. Presumably this is also the case with more complex tasks, but maybe it does not matter when doing a classification on real data.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The range of values in the data impacts the choice of parameters. As I mentioned in the first post, I have read that normalising is important. The fact that the parameter settings are sensitive to the range of values in the dataset could be a big reason why normalising is important.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Manually creating a neural network (even with only 1 layer) is a bit of faff; I should learn how to create one using the pytorch library.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;You can create some funky visuals with neural networks! Somebody more creative than I am could add some music to each of these to produce some interesting art. These animations may even be a weird alternative to &lt;a href=&quot;https://en.wikipedia.org/wiki/Rorschach_test&quot;&gt;Rorschach inkblot tests&lt;/a&gt;: ‘How does this video make you feel? What do you see in this video?’&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and SGD Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</summary></entry><entry><title type="html">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</title><link href="https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1.html" rel="alternate" type="text/html" title="Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data" /><published>2020-09-10T00:00:00-05:00</published><updated>2020-09-10T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/python/2020/10/01/sgd4.html&quot;&gt;Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html&quot;&gt;Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and &lt;strong&gt;S&lt;/strong&gt;GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html&quot;&gt;Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;At the end of &lt;a href=&quot;https://youtu.be/5L3Ao5KuCC4?t=7244&quot;&gt;Lecture 3 of the 2020 FastAI course&lt;/a&gt; and at the end of &lt;a href=&quot;https://youtu.be/ccMHJeQU4Qw?t=6394&quot;&gt;Lecture 2 of the 2018 FastAI course&lt;/a&gt;, there are visualisations of the gradient descent algorithm. I quite liked them, in particular the animation from the 2018 version, and I wanted to re-create them and on more complex examples.&lt;/p&gt;

&lt;p&gt;The animations I created are available below. Note that in all animations, the orange dots represent the training data, and the blue line represents the model’s predictions. I will go through them and give my thoughts. At the end I describe some insights I gained by doing this.&lt;/p&gt;

&lt;h2 id=&quot;linear-data&quot;&gt;Linear data&lt;/h2&gt;
&lt;p&gt;I created some linear data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*x + b + noise&lt;/code&gt;, and then tried to use gradient descent to determine the coefficients.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_linear_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;This animation is representative of the various examples I tried for linear data. Gradient descent is quick to get very close to the data, but then the learning dramatically slows down and it takes many iterations to improve further. (Note, you have to pay close attention to notice that there is still learning going on throughout the whole video).  Clearly, there is some optimisation that can be done with the learning rate; I did try to create a cutoff point where the learning rate gets bigger, but I am sure there are much better ways of doing things.&lt;/p&gt;

&lt;h2 id=&quot;quadratic-data&quot;&gt;Quadratic data&lt;/h2&gt;
&lt;p&gt;Next I created some quadratic data &lt;code class=&quot;highlighter-rouge&quot;&gt;y=a*x*x + b*x + c + noise&lt;/code&gt;.&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_quadratic_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;The pattern here is very similar to the pattern for the linear case: gradient descent quickly reaches a good model, and then the learning dramatically slows down. This is not too surprising, because though the final function is non-linear, this is still a linear-regression problem by treating &lt;code class=&quot;highlighter-rouge&quot;&gt;x*x&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;x&lt;/code&gt; as separate features.&lt;/p&gt;

&lt;h2 id=&quot;sinusoidal-data&quot;&gt;Sinusoidal data&lt;/h2&gt;
&lt;p&gt;Next I created some sinusoidal data &lt;code class=&quot;highlighter-rouge&quot;&gt;y = a*(sin(b*x + c)) + d&lt;/code&gt;. Things were more interesting here.&lt;/p&gt;

&lt;p&gt;The first video shows you what happened when I chose a learning rate that was too large (but not so large so as to have everything diverge to infinity):&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_sin_1.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;Crazy, right! The model is oscillating back and forth, with the oscillations slowly getting larger with time. In Lecture 3 of the 2020 course, this behaviour is illustrated with the example of using gradient descent to minimise a quadratic function, but I never thought I would actually encounter this behaviour out in the wild.&lt;/p&gt;

&lt;p&gt;This second video shows what happens when I choose a smaller learning rate:&lt;/p&gt;

&lt;figure class=&quot;video_container&quot;&gt;
  &lt;video controls=&quot;true&quot; allowfullscreen=&quot;true&quot;&gt;
    &lt;source src=&quot;/blog/images/sgd1_sin_2.mp4&quot; type=&quot;video/mp4&quot; /&gt;
  &lt;/video&gt;
&lt;/figure&gt;

&lt;p&gt;No craziness here, but it does not converge to an appropriate solution. I think the explanation for this is that the algorithm has found a local-minimum, and so the algorithm gets stuck and cannot improve.  This is qualitatively different to the linear and quadratic cases: since those were both instances of linear regression, it is known from theory that there is only one minimum so gradient descent will reach it. This sinusoidal case cannot be re-written as a linear regression problem, so there is not automatic guarantee of there being only one minimum point; from this experimentation, it looks like there multiple minimum points!&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;I learnt various things by doing this experiment.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The learning rate is very important! I had to play around with the learning rates to get things to work.&lt;/li&gt;
  &lt;li&gt;The range of values in the training data seemed to have big impact on the which learning rates to use. I am not 100% sure about this, but I have read in places that it is important to normalise your data, and perhaps its effect on learning rates is a big reason.&lt;/li&gt;
  &lt;li&gt;I learnt how to create animations in matplotlib! And also how to include video files in this blog. :D&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;There are various things I would like to try.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The next thing I will try is using the same datasets, but seeing if I can fit a neural network to the data.&lt;/li&gt;
  &lt;li&gt;Stochastic gradient descent. My hope is that it will avoid the local minimum problem in the sinusoidal case.&lt;/li&gt;
  &lt;li&gt;Creating a web-app out of this, so you can easily experiment for yourselves.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;the-code&quot;&gt;The code&lt;/h2&gt;
&lt;p&gt;The code for this project is in this &lt;a href=&quot;https://github.com/Lovkush-A/pytorch_sgd&quot;&gt;GitHub repository&lt;/a&gt;.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and SGD Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</summary></entry><entry><title type="html">FastAI Course, Part III, Frustrations with creating an image classifier</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html" rel="alternate" type="text/html" title="FastAI Course, Part III, Frustrations with creating an image classifier" /><published>2020-09-08T00:00:00-05:00</published><updated>2020-09-08T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html&quot;&gt;FastAI Course, Part II, Lesson 1 and sentiment analysis&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/09/fastai1.html&quot;&gt;FastAI Course, Part I, Lessons 1 and 2&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;After reading through Chapter 2 of the Fast AI course/book, it was time I tried putting the ideas into practice. I decided to create a classifier that distinguishes between squash, tennis and badminton rackets. Then using voila and binder, I created a little web app for it: check it out &lt;a href=&quot;https://mybinder.org/v2/gh/Lovkush-A/fastai_rackets/master?urlpath=%2Fvoila%2Frender%2Frackets.ipynb&quot;&gt;here&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;a-comedy-of-errors&quot;&gt;A comedy of errors&lt;/h2&gt;
&lt;p&gt;This project was frustrating for me. I somehow managed to make numerous errors at every step which could not be straightforwardly de-bugged. I try to describe them here, but I have undoubtedly forgotten many of them.&lt;/p&gt;

&lt;h3 id=&quot;the-bing-api&quot;&gt;The Bing API&lt;/h3&gt;
&lt;p&gt;The instructions from the notes: “To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows.” Sounds simple. I went to the Microsoft website and logged on. This is what I saw:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_microsoft.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Hmm, there’s nothing obvious here regarding Bing or APIs. I spent a whole bunch of time looking through all the menus, but to no avail. The instructions made it sound simple, and yet I couldn’t manage. I searched in the FastAI forums and a relevant &lt;a href=&quot;https://forums.fast.ai/t/getting-the-bing-image-search-key/67417&quot;&gt;post&lt;/a&gt; was easy to find. It explained the (non-trivial) steps needed. A big non-triviality is that you should be using a Microsoft &lt;em&gt;Azure&lt;/em&gt; account, not a Microsoft account. I think the book should make this clearer. It would have saved me (and presumably many others) a bunch of time.&lt;/p&gt;

&lt;h3 id=&quot;padding&quot;&gt;Padding&lt;/h3&gt;
&lt;p&gt;I downloaded the datasets, viewed some of the images, and deleted the files which were erroneous. Next up was to create a &lt;code class=&quot;highlighter-rouge&quot;&gt;DataBlock&lt;/code&gt; with the appropriate transforms/data augmentation. To ensure the images have the correct aspect ratio, I decided that padding is the best option. I tried it, using &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode = 'zeros'&lt;/code&gt;. It looked alright, but most of the images have white backgrounds, so I thought maybe it would be better for the images to be padded with white space. I guessed that &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode = 'ones'&lt;/code&gt; would work, but alas it did not. So, I did what has been recommended numerous times, and went to the docs.&lt;/p&gt;

&lt;p&gt;This is what I found:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_pad.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Maybe I am in the minority, but I do not find this helpful. Here are some things that frustrate me about this:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What are the possibilities for &lt;code class=&quot;highlighter-rouge&quot;&gt;*args&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;** kwargs&lt;/code&gt;? How would I find out?&lt;/li&gt;
  &lt;li&gt;What does ‘All possible mode as attributes to get tab-completion and typo-proofing’ mean?!  What does it have to do with padding images?&lt;/li&gt;
  &lt;li&gt;What is &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorBBox&lt;/code&gt;? Or &lt;code class=&quot;highlighter-rouge&quot;&gt;TensorPoint&lt;/code&gt;?&lt;/li&gt;
  &lt;li&gt;What’s the relevance of the change in font in the headings? Is it self-evident? Looking at it more closely, you have one font for classes and one font for methods. But do we need a different font for that? Changing heading formats usually indicates a change in the level of headings, but I don’t think that is what is happening here.&lt;/li&gt;
  &lt;li&gt;Most importantly, what are the different options for &lt;code class=&quot;highlighter-rouge&quot;&gt;pad_mode&lt;/code&gt;? If you scroll further down, three examples are provided (zeros, reflection and border), but it is not clear if these are all the options. Maybe they are, maybe they aren’t. I don’t know.&lt;/li&gt;
  &lt;li&gt;Really, what does ‘All possible mode as attributes to get tab-completion and typo-proofing’ actually mean??&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Perhaps I could find out more by looking at the source code. After all, as is emphasised several times in the lectures, most of the source code is only a few lines and thus easy to understand.&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;nc&quot;&gt;CropPad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;RandTransform&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;&quot;Center crop or pad an image to `size`&quot;&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;order&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;PadMode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Zeros&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;size&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_process_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;store_attr&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;**&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;kwargs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;encodes&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorBBox&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;TensorPoint&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;_get_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;//&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;2&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;crop_pad&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;size&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tl&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;orig_sz&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;pad_mode&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Yes, there are only several lines of code here. However, they are not helpful, at least not for me. In order to understand any of this, I need to go searching through the rest of the code base, which I am not inclined to do. I should not have to do this to find out if I can create white padding instead of black padding (or maybe I should?).&lt;/p&gt;

&lt;p&gt;Anyway, I stopped here with the presumption that it is not possible to get white padding.&lt;/p&gt;

&lt;h3 id=&quot;runtimeerror-dataloader-worker-pid-19862-is-killed-by-signal-killed&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;RuntimeError: DataLoader worker (pid 19862) is killed by signal: Killed.&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;I made various other decisions for the DataBlock and created the DataLoaders. Now it was time to do the modelling! I ran the cell, and to my dismay, I got the error message shown in the heading. I didn’t understand it. I checked all my code carefully and did not spot anything wrong. Next I tried to run the examples from the book as is. The same error came up. This was bizarre!&lt;/p&gt;

&lt;p&gt;It was time for some Googling. I searched around, saw people with similar error messages, and tried their suggestions. None of them solved the problem. It was time to get to bed at this stage, so I decided I will ask about this in the forum, and have another stab the next day.&lt;/p&gt;

&lt;p&gt;Next day, I got ready to tackle the problem and opened up Gradient. But something was a little off:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/fastai3_gradient.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The machine type was set to CPU! I did not remember selecting this, so was a bit confused. Anyway, I tried starting up the notebook, and the pop-up comes up to select the options. The default selection is the CPU. Hmm, that is odd. I tried to change it to GPU, and then I discovered the problem: there was no Free GPU available. It looks like the day before, I just assumed the settings would be the same as previous runs, so I did not check the machine type, and I must have been using the CPU option. This is almost certainly the cause of the &lt;code class=&quot;highlighter-rouge&quot;&gt;DataLoader worker&lt;/code&gt; error. At least I now know the issue.&lt;/p&gt;

&lt;p&gt;Time to move to Colab.&lt;/p&gt;

&lt;h3 id=&quot;google-colab&quot;&gt;Google Colab&lt;/h3&gt;
&lt;p&gt;This is just me, but I dislike the Colab interface. It is similar to Jupyter Notebook, but different in little ways that my brain finds off-putting. One example is that in Colab, the shortcut &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt; to insert a new cell automatically enters the cell; this is not what I want, as I often want to create a bunch of cells at once, and then move into them.&lt;/p&gt;

&lt;p&gt;These are minor issues, but I thought I may as well include them, in case anybody else out there gets some comfort knowing they are not alone.&lt;/p&gt;

&lt;h3 id=&quot;re-training-model-post-cleaning&quot;&gt;Re-training model post cleaning&lt;/h3&gt;
&lt;p&gt;I continued with the project, and managed to get the classifier to work. I then cleaned the data using the easy-to-use widget created by FastAI. I then tried to do some further fine tuning, but I got an error message saying that a certain image file, &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt;, cannot be found.&lt;/p&gt;

&lt;p&gt;I had a little look at the code, and realised my error. I deleted some images while cleaning (including image &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt;), but did not re-run the cell that defines the dataloaders &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt; variable, where the information on the dataset is stored. So I re-ran the cell, and tried again. But I still got the error message.&lt;/p&gt;

&lt;p&gt;I wanted to investigate the &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt; variable to find out what is going on; specifically, I wanted to see which filenames are stored in the variable and check whether &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt; is listed there. Similar to the issues I had with the padding above, I did not find the documentation helpful. However, using &lt;code class=&quot;highlighter-rouge&quot;&gt;dir&lt;/code&gt; (a super handy function, by the way. It lists &lt;em&gt;all&lt;/em&gt; the properties and methods that an object has) and playing a bit, I was able to find the list of filenames stored in &lt;code class=&quot;highlighter-rouge&quot;&gt;dls&lt;/code&gt;; &lt;code class=&quot;highlighter-rouge&quot;&gt;00000149.png&lt;/code&gt; was not there!&lt;/p&gt;

&lt;p&gt;This left only one possibility: that filenames are stored in the &lt;code class=&quot;highlighter-rouge&quot;&gt;learn&lt;/code&gt; variable. Based on my experience with other ML tools (e.g. scikitlearn), this is counter-intuitive to me. At this stage, I just wanted to get a working example as quickly as possible, so I did not investigate how I can do additional fine-tuning cycles on new datasets. Instead, I just created a fresh learner object, and trained that.&lt;/p&gt;

&lt;p&gt;I am not sure anybody can do anything about this kind of problem, as it stems from using tools as black-boxes, without understanding how they work. Another example of this I have had is in using conda: I am comfortable creating new environments, but I do not know how to rename a directory or move directories around, because that information is stored somewhere for conda to function, and I do not know how to change that information.&lt;/p&gt;

&lt;p&gt;Unfortunately, it does not change that the experience is frustrating - there is a simple task that I want to do, my naive approach does not work, and in order to learn how do it properly, I have to get a fairly comprehensive understanding of the tool. If anybody has any advice or suggestions, do please let me know!&lt;/p&gt;

&lt;h3 id=&quot;binder&quot;&gt;Binder&lt;/h3&gt;
&lt;p&gt;The next bunch of tasks went well, as far as I can remember. Exporting the learner, creating a local environment where I can create the jupyter notebook that will be turned into the web app, installing all the packages, creating the notebook, getting voila to work locally, and pushing it to github. I was literally one step away: get binder to work. The finish line was literally right in front of me.&lt;/p&gt;

&lt;p&gt;Oh boy was I wrong. I do not know how I managed to make such a mess of this step. From what I remember, I encountered two main problems, but my ineptitude dragged out the process far longer than it ought to have.&lt;/p&gt;

&lt;p&gt;The first mistake was not to change the &lt;code class=&quot;highlighter-rouge&quot;&gt;File&lt;/code&gt; setting to &lt;code class=&quot;highlighter-rouge&quot;&gt;URL&lt;/code&gt;, when you enter the path to the file in Binder. This setting was in my blindspot; I never even noticed that box. This is made worse because the voila documentation clearly states that you should do this; in fact, this is how I found the error, because I chose to re-read the instructions carefully, after my various other attempts failed.&lt;/p&gt;

&lt;p&gt;The second issue I had was with the requirements file that should be provided. I first tried using the file created by using &lt;code class=&quot;highlighter-rouge&quot;&gt;conda env export&lt;/code&gt;, but that did not work. Googling the error messages revealed that conda lists many device specific requirements, which will not work with the Linux machines that binder uses. Googling also showed the two-step process to get around this issue: create a modified version of the requirements by adding some tags to the &lt;code class=&quot;highlighter-rouge&quot;&gt;conda env export&lt;/code&gt; command, and then manually remove any requirements that still cause an error on Binder.&lt;/p&gt;

&lt;p&gt;This was it: I fixed the two problems and was minutes away from completion. I got binder going, and as it does, it was taking a few minutes to things set-up.  But things were taking very long. Something was not right. I just kept waiting, but eventually, the process quit with some other error message.&lt;/p&gt;

&lt;p&gt;I was so close, yet so far. I tried searching around on the forums, to see if anybody else had any issues on binder. I ended up finding a blog post and example github repo, so I tried to imitate that. The main takeaway was that they manually constructed a requirements file with 4 or 5 requirements, rather than using an automatically generated file from conda with many more requirements.&lt;/p&gt;

&lt;p&gt;After a few more attempts and ironing out several little mistakes (e.g. they used &lt;code class=&quot;highlighter-rouge&quot;&gt;fastai2&lt;/code&gt; in their requirements, which is now out-of-date), it finally worked.&lt;/p&gt;

&lt;h2 id=&quot;final-remarks&quot;&gt;Final remarks&lt;/h2&gt;
&lt;p&gt;This has been a stressful but enlightening experience. Just writing this blog made me re-live the emotions: the frustration when things seemed unnecessarily complicated, and the sense of achievement and relief when I finally reached the end.&lt;/p&gt;

&lt;p&gt;It may sound like I dislike FastAI and the course, but the opposite is true. Like I said in my first post on the FastAI course, I am impressed with the teaching and all the thought that has gone into it. I would strongly recommend this course to anybody interested in AI or Machine Learning. However, I think this may also be why I experience more frustration with it. I have such a high opinion of the course, that any negative stands out; I am holding the team to particularly high standards.&lt;/p&gt;

&lt;p&gt;To end, I hope this blog entry has been useful for you in some way. I welcome any comments or thoughts or disagreements; please feel free to let me know.&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series FastAI Course, Part II, Lesson 1 and sentiment analysis FastAI Course, Part I, Lessons 1 and 2</summary></entry><entry><title type="html">Analysing the movies I’ve watched, Part V, Data visualisation II</title><link href="https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5.html" rel="alternate" type="text/html" title="Analysing the movies I've watched, Part V, Data visualisation II" /><published>2020-09-02T00:00:00-05:00</published><updated>2020-09-02T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5</id><content type="html" xml:base="https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/24/mymovies4.html&quot;&gt;Analysing the movies I’ve watched, Part IV, Data visualisation&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/19/mymovies3.html&quot;&gt;Analysing the movies I’ve watched, Part III, Joining the tables&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/03/mymovies2.html&quot;&gt;Analysing the movies I’ve watched, Part II, Data cleaning&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/python/data%20science/2020/08/02/mymovies1.html&quot;&gt;Analysing the movies I’ve watched, Part I, Data collection&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;In a previous post, I described how the joining missed out half the data. After making various plots, is turns out the joining was sometimes erroneous too. Hence, I bit the bullet and decided I to manually do the joining, and get a complete and clean dataset. Learning the lessons from past attempts at manual processing, it was a fairly smooth process. (Furthermore, things were significantly sped up by using Selenium - which allowed me to automate the process of searching for a movie on imdb). In the end, it took roughly 3 hours to tidy the dataset and have the official imdb id attached to each entry in my database. And now for the charts!&lt;/p&gt;

&lt;h2 id=&quot;year-of-release&quot;&gt;Year of release&lt;/h2&gt;
&lt;p&gt;Below are charts showing information about the year of release for the movies I have watched. The first one is just a histogram, and the second shows a scatter-chart showing the year I watched the movie against the year the movie released.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_releaseyear.png&quot; alt=&quot;image&quot; /&gt;
&lt;img src=&quot;/blog/images/mymovies5_releaseyear2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Not sure if there is anything surprising about any of these charts. One neat thing is that one could probably get an idea of my age based on the first chart. The second chart seems to show that the majority of movies I watch are relatively new.&lt;/p&gt;

&lt;h2 id=&quot;ratings&quot;&gt;Ratings&lt;/h2&gt;
&lt;p&gt;Below is a chart showing the average ratings (by users of imdb) for the movies I have watched, along with the distribution of ratings for all the movies in imdb.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings1.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is clear from this chart that I tend to watch ‘decent’ movies, as judged by imdb users, and that I avoid ‘bad’ movies, but I also seem to avoid the ‘best’ movies.&lt;/p&gt;

&lt;p&gt;Below is a chart comparing the ratings for all the movies I have watched with the ratings of the movie I would recommend.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings2.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;It is reassuring to see that, overall, the movies I recommend are generally better regarded than the movies I do not recommend.&lt;/p&gt;

&lt;p&gt;Lastly, here is a chart showing the variation in ratings by the year I watched the movie.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/blog/images/mymovies5_ratings3.png&quot; alt=&quot;image&quot; /&gt;&lt;/p&gt;

&lt;p&gt;I cannot really discern any patterns here, that aren’t revealed by other charts. Patterns that I can see are that I watched a lot more movies (and greater variety of quality of  movies) in 2008 and 2009, and that the movies I recommend tend to have higher ratings than those I don’t.&lt;/p&gt;

&lt;p&gt;To end this section, here is a list of the most and least popular movies I have watched.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Rating on imdb&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;the shawshank redemption&lt;/td&gt;
      &lt;td&gt;9.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the dark knight&lt;/td&gt;
      &lt;td&gt;9.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;12 angry men&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the return of the king&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;pulp fiction&lt;/td&gt;
      &lt;td&gt;8.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;fight club&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the fellowship of the ring&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;forrest gump&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;inception&lt;/td&gt;
      &lt;td&gt;8.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the matrix&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the lord of the rings: the two towers&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;goodfellas&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;star wars: episode v - the empire strikes back&lt;/td&gt;
      &lt;td&gt;8.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;se7en&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;city of god&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;interstellar&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;saving private ryan&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the silence of the lambs&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;star wars: episode iv - a new hope&lt;/td&gt;
      &lt;td&gt;8.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;gladiator&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the departed&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;casablanca&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the usual suspects&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;dear zachary: a letter to a son about his father&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;whiplash&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;joker&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;psycho&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;senna&lt;/td&gt;
      &lt;td&gt;8.5&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Movie&lt;/th&gt;
      &lt;th&gt;Rating on imdb&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;steel&lt;/td&gt;
      &lt;td&gt;2.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;troll 2&lt;/td&gt;
      &lt;td&gt;2.9&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the foreigner&lt;/td&gt;
      &lt;td&gt;3.4&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;batman &amp;amp; robin&lt;/td&gt;
      &lt;td&gt;3.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;half a person&lt;/td&gt;
      &lt;td&gt;3.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th part viii: jason takes manhattan&lt;/td&gt;
      &lt;td&gt;4.6&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;barely legal&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;elektra&lt;/td&gt;
      &lt;td&gt;4.7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;doe: dead or alive&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th: a new beginning&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;uncut gem&lt;/td&gt;
      &lt;td&gt;4.8&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;blair witch&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;a teacher’s crime&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;tekken: the motion picture&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;in their skin&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;daredevil&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;scary movie 2&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;the circle&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;friday the 13th part vii: the new blood&lt;/td&gt;
      &lt;td&gt;5.3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;little&lt;/td&gt;
      &lt;td&gt;5.4&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The one thing I feel slightly ashamed of is that I watched so many of the Friday the 13th movies. I think my reasoning was that if a series has that many movies, there must be something good about it. I kept going on, even though I thought the movies were bad, in the hope that there is a moment in the series where the movies become good. I did finally stop watching the series - it just took eight movies to make that decision…&lt;/p&gt;

&lt;h2 id=&quot;genres&quot;&gt;Genres&lt;/h2&gt;
&lt;p&gt;Below is a table showing a breakdown of the genres of movie I watched, ordered by which genre has the highest odds of me recommending it. Note that most movies had multiple genres.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Genre&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Frequency&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Frequency Recommended&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;Percent recommended&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;thriller&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;126&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;37&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;war&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;14&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.29&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;sport&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.28&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;drama&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;319&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;87&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.27&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;sci-fi&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;86&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;22&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.26&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;crime&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;146&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;36&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.25&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;biography&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;47&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;music&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;18&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.22&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;romance&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;56&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;11&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;comedy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;210&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;42&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.20&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;mystery&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;80&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;15&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.19&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;animation&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;51&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;8&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;family&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;25&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.16&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;history&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;26&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;fantasy&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;70&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;10&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.14&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;documentary&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;31&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.13&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;adventure&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;188&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.12&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;horror&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;66&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;7&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.11&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;action&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;227&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;23&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.10&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;musical&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;6&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;news&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;western&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.00&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The entries that stand out to me are those with the largest frequencies. There is strong evidence that I am more likely to recommend a thriller or a drama than a comedy, and that I am unlikely to recommend adventure or action movies.&lt;/p&gt;

&lt;h2 id=&quot;directors&quot;&gt;Directors&lt;/h2&gt;
&lt;p&gt;Below is a list of the directors, along with the movies, whose movies I have most watched.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Steven Spielberg
 a.i. artificial intelligence
          catch me if you can
                         jaws
                jurassic park
              minority report
      raiders of the lost ark
          saving private ryan
     the adventures of tintin

Peter Jackson
                                         king kong
                 the hobbit: an unexpected journey
         the hobbit: the battle of the five armies
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers

Christopher Nolan
               dunkirk
             inception
          interstellar
               memento
       the dark knight
 the dark knight rises

Guy Ritchie
 lock, stock and two smoking barrels
                          rocknrolla
                     sherlock holmes
  sherlock holmes: a game of shadows
                              snatch
             the man from u.n.c.l.e.

Martin Scorsese
               cape fear
              goodfellas
                 silence
             taxi driver
            the departed
 the wolf of wall street

Quentin Tarantino
             inglourious basterds
                kill bill: vol. 1
 once upon a time... in hollywood
                     pulp fiction
                   reservoir dogs
                the hateful eight

Danny Boyle
    28 days later
       steve jobs
 t2 trainspotting
    trainspotting
        yesterday

Matthew Vaughn
                     kick-ass
 kingsman: the secret service
                   layer cake
                     stardust
           x-men: first class

David Fincher
                          fight club
                           gone girl
                               se7en
 the curious case of benjamin button
                  the social network

M. Night Shyamalan
             glass
 lady in the water
             split
   the sixth sense
       unbreakable

Darren Aronofsky
          black swan
             mother!
                noah
 requiem for a dream
        the wrestler
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I tend not to pay any attention to who the director is, so for me, the interesting thing here is to see how movies that I would have said had no connection in fact have the same director. E.g. I did not realise I had watched that many movies by Spielberg. The other thing that stands out here is that all the directors are male.&lt;/p&gt;

&lt;h2 id=&quot;actors&quot;&gt;Actors&lt;/h2&gt;
&lt;p&gt;Below is a list of actors, along with the movies, that I have most watched.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Brad Pitt
                    12 years a slave
                              allied
                  burn after reading
                          fight club
                inglourious basterds
                 killing them softly
                            megamind
                           moneyball
    once upon a time... in hollywood
                               se7en
                              snatch
                       the big short
 the curious case of benjamin button
                                troy
                         war machine

Bruce Willis
                 alpha dog
                  die hard
                die hard 2
              die hard 4.0
 die hard with a vengeance
          fast food nation
                     glass
                    looper
          moonrise kingdom
              pulp fiction
                  sin city
           the sixth sense
               unbreakable

Samuel L. Jackson
                    die hard with a vengeance
                                        glass
                                       jumper
                 kingsman: the secret service
                                 pulp fiction
 star wars: episode iii - revenge of the sith
                            the hateful eight
                       the hitman's bodyguard
                              the incredibles
                               the negotiator
                                  unbreakable
                                unicorn store

Matt Damon
          adjustment bureau
                  contagion
          good will hunting
               jason bourne
                      ponyo
        saving private ryan
        the bourne identity
       the bourne ultimatum
               the departed
 the legend of bagger vance
                the martian
    the talented mr. ripley

Tom Hanks
     angels &amp;amp; demons
 catch me if you can
        forrest gump
 saving private ryan
          the circle
           toy story
         toy story 2
 toy story of terror
     you've got mail

Robert De Niro
     analyze this
        cape fear
    dirty grandpa
       goodfellas
    hide and seek
            joker
 meet the parents
   righteous kill
      taxi driver

Rachel McAdams
                         about time
                              aloha
                     doctor strange
                         mean girls
                            red eye
                    sherlock holmes
 sherlock holmes: a game of shadows
                      state of play
                   wedding crashers

Natalie Portman
                                 annihilation
                                   black swan
                                       closer
    star wars: episode i - the phantom menace
 star wars: episode ii - attack of the clones
 star wars: episode iii - revenge of the sith
                        the other boleyn girl
                                         thor
                               v for vendetta

Leonardo DiCaprio
                    blood diamond
              catch me if you can
                        inception
 once upon a time... in hollywood
                     the departed
         the man in the iron mask
                     the revenant
          the wolf of wall street

Ben Affleck
                               argo
 batman v superman: dawn of justice
                          daredevil
                          gone girl
                  good will hunting
                           paycheck
                      state of play
                     the accountant

Jake Gyllenhaal
      donnie darko
           jarhead
      nightcrawler
 nocturnal animals
         prisoners
             proof
       source code
    velvet buzzsaw

Ian McKellen
                                          stardust
                 the hobbit: an unexpected journey
         the hobbit: the battle of the five armies
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers
                                             x-men
                                  x2: x-men united

Christian Bale
          3:10 to yuma
       american psycho
        public enemies
  terminator salvation
         the big short
       the dark knight
 the dark knight rises
         the machinist

Morgan Freeman
      along came a spider
              deep impact
                     lucy
      million dollar baby
                    se7en
          the bucket list
 the shawshank redemption
                   wanted

Tom Cruise
          collateral
    edge of tomorrow
     minority report
 mission: impossible
            rain man
    the last samurai
            valkyrie
         vanilla sky

Johnny Depp
                   a nightmare on elm street
 fantastic beasts: the crimes of grindelwald
              fear and loathing in las vegas
     pirate of the caribbean: at world's end
  pirates of the caribbean: dead man's chest
                              public enemies
                                 the tourist

Ewan McGregor
                              angels &amp;amp; demons
                              black hawk down
    star wars: episode i - the phantom menace
 star wars: episode ii - attack of the clones
 star wars: episode iii - revenge of the sith
                             t2 trainspotting
                                trainspotting

Laurence Fishburne
 john wick: chapter 3 - parabellum
                        passengers
                         predators
                        the matrix
               the matrix reloaded
            the matrix revolutions
                        the signal

Robert Downey Jr.
             avengers: infinity war
                         iron man 2
                         iron man 3
                    sherlock holmes
 sherlock holmes: a game of shadows
             spider-man: homecoming
                       the avengers

Jude Law
       a.i. artificial intelligence
                             closer
                          contagion
                            gattaca
                    sherlock holmes
 sherlock holmes: a game of shadows
            the talented mr. ripley

Kevin Spacey
       a bug's life
    american beauty
        margin call
               moon
              se7en
     the negotiator
 the usual suspects

Orlando Bloom
                                       good doctor
           pirate of the caribbean: at world's end
        pirates of the caribbean: dead man's chest
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers
                                              troy

Scarlett Johansson
                   her
           jojo rabbit
   lost in translation
                  lucy
        marriage story
          the avengers
 the other boleyn girl

Angelina Jolie
         alexander
 girl, interrupted
     kung fu panda
   kung fu panda 2
       the tourist
            wanted

Paul Giamatti
         duplicity
 lady in the water
      private life
          sideways
   the illusionist
           win win

Joaquin Phoenix
                  gladiator
                        her
               hotel rwanda
                      joker
                     quills
 you were never really here

Jim Carrey
            ace ventura: pet detective
                       dumb and dumber
 eternal sunshine of the spotless mind
                              the mask
                       the truman show
                               yes man

Anthony Hopkins
                alexander
                     noah
                    proof
               red dragon
 the silence of the lambs
                     thor

Tom Hardy
               dunkirk
            layer cake
                 locke
    mad max: fury road
 the dark knight rises
          the revenant

Keanu Reeves
                         john wick
              john wick: chapter 2
 john wick: chapter 3 - parabellum
                        the matrix
               the matrix reloaded
            the matrix revolutions

Emma Watson
                         beauty and the beast
 harry potter and the deathly hallows: part 1
          harry potter and the goblet of fire
       harry potter and the half-blood prince
                                         noah
                                   the circle

Clive Owen
     children of men
              closer
           duplicity
          inside man
            sin city
 the bourne identity

Ethan Hawke
               boyhood
               gattaca
           lord of war
        predestination
 the magnificent seven
             the purge

John Malkovich
     being john malkovich
       burn after reading
           johnny english
            ripley's game
 the man in the iron mask
              warm bodies

Elijah Wood
                                       deep impact
                                        happy feet
                                            maniac
 the lord of the rings: the fellowship of the ring
     the lord of the rings: the return of the king
             the lord of the rings: the two towers

Cameron Diaz
  being john malkovich
   shrek forever after
       shrek the third
              the mask
           vanilla sky
 what happens in vegas
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Nice list of some of the more famous actors in the industry. One thing that surprised is how many movies I have watched that starred Brad Pitt - I did not realise it was that many! Once again, it is worth noting that females are under-represented here.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;This project has probably been the one that I spent the most time on so far. I have learnt a whole bunch and managed to produce some nice visuals and summaries. Hopefully it was interesting to read!&lt;/p&gt;</content><author><name></name></author><summary type="html">Other posts in series Analysing the movies I’ve watched, Part IV, Data visualisation Analysing the movies I’ve watched, Part III, Joining the tables Analysing the movies I’ve watched, Part II, Data cleaning Analysing the movies I’ve watched, Part I, Data collection</summary></entry><entry><title type="html">FastAI Course, Part II, Lesson 1 and sentiment analysis</title><link href="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html" rel="alternate" type="text/html" title="FastAI Course, Part II, Lesson 1 and sentiment analysis" /><published>2020-08-30T00:00:00-05:00</published><updated>2020-08-30T00:00:00-05:00</updated><id>https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2</id><content type="html" xml:base="https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html">&lt;h2 id=&quot;other-posts-in-series&quot;&gt;Other posts in series&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html&quot;&gt;FastAI Course, Part III, Frustrations with creating an image classifier&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;/blog/data%20science/neural%20network/python/2020/08/09/fastai1.html&quot;&gt;FastAI Course, Part I, Lessons 1 and 2&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Recently, FastAI launched a new version of their course, using their revamped FastAI version 2 packages. As I had only completed the first two lessons of the previous version of the course, I thought I’d restart.  It is good I did re-start, because they have made big changes to how the course is organised and also to the syntax (e.g. in the old version, there was significant discussion about setting the learning rate using their ‘learning rate finder’, but so far, there has been no mention of this and it looks like that has been automated).  Towards the end of the first lesson, Jeremy showcases the different domains in which one can use Deep Learning. One of those was NLP. I have not yet done any NLP, so I thought this is a good chance to play around a little, without having to delve into any of the technical details.&lt;/p&gt;

&lt;p&gt;In this particular example, the NLP consists of sentiment analysis of movie reviews. Looking at the code, they use AWD LSTM architecture. A quick google search reveals that this is a fairly new algorithm, combining various ideas and tools into one. I am not yet at the stage to understand this though, but hopefully it will be discussed later in the course.&lt;/p&gt;

&lt;h2 id=&quot;experimentation&quot;&gt;Experimentation&lt;/h2&gt;
&lt;p&gt;The input to the model is a string containing a movie review. The output is a probability/score between 0 and 1 indicating how much the model thinks the review is positive. My initial thought was to to see if the model had learnt about numbers, by seeing how it rates reviews of the form ‘x out of y’.&lt;/p&gt;

&lt;h3 id=&quot;x-out-of-5&quot;&gt;‘x out of 5’&lt;/h3&gt;
&lt;p&gt;The results of this experimentation:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 out of 5: 0.5570
1 out of 5: 0.4760
2 out of 5: 0.3281
3 out of 5: 0.4038
4 out of 5: 0.4073
5 out of 5: 0.8569
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This was surprising to me:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The model thinks that the review ‘0 out of 5’ is likely a positive review&lt;/li&gt;
  &lt;li&gt;The model is roughly undecided about ‘1 out of 5’&lt;/li&gt;
  &lt;li&gt;The model thinks 2,3 or 4 out of 5 is negative&lt;/li&gt;
  &lt;li&gt;The model thinksthat ‘5 out of 5’ is positive.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;As with most of these ML models, this must be a reflection of the underlying dataset. I imagine very few people actually give a movie 0 or 1 out of 5, so the model doesn’t really know what to do with it, whereas ‘2 out of 5’ probably is more common and basically says the movie is bad, and ‘3 out of 5’ and ‘4 out of 5’ are more average-y ratings.&lt;/p&gt;

&lt;h3 id=&quot;x-out-of-10&quot;&gt;‘x out of 10’&lt;/h3&gt;
&lt;p&gt;The results of this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;0 out of 10: 0.5402
1 out of 10: 0.4478
2 out of 10: 0.2972
3 out of 10: 0.3741
4 out of 10: 0.3798
5 out of 10: 0.8475
6 out of 10: 0.9897
7 out of 10: 0.99996
8 out of 10: 0.99991
9 out of 10: 0.9998
10 out of 10:0.99996
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Again the model is confused with the worst reviews, presumably because they just don’t occur in the training data. Then the model seems to deal with 2,3 and 4 out of 10 sensibly. But then it has big jump and thinks the rest are definitely positive, with anything above 7 out of 10 being considered positive with high confidence. Again, the explanation for these unusual evaluations of the reviews must be that they are uncommon in the dataset.&lt;/p&gt;

&lt;h3 id=&quot;variations-on-x-out-of-5&quot;&gt;Variations on ‘x out of 5’&lt;/h3&gt;
&lt;p&gt;I tried a few little variations on ‘x out of 5’ to see if it would make any impact: adding an exclamation mark at the end, adding the word ‘stars’ at the end, and adding the word ‘Only’ at the start.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;x&lt;/th&gt;
      &lt;th&gt;‘x out of 5’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘x out of 5!’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘x out of 5 stars’&lt;/th&gt;
      &lt;th style=&quot;text-align: right&quot;&gt;‘Only x out of 5’&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0&lt;/td&gt;
      &lt;td&gt;0.5570&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7436&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.7247&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0240&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;1&lt;/td&gt;
      &lt;td&gt;0.4760&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.6756&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.6538&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0247&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;2&lt;/td&gt;
      &lt;td&gt;0.3281&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.4901&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.4884&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0196&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;3&lt;/td&gt;
      &lt;td&gt;0.4038&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5670&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5634&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0189&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;4&lt;/td&gt;
      &lt;td&gt;0.4073&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5634&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5589&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.0179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;5&lt;/td&gt;
      &lt;td&gt;0.8569&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.9051&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.8969&lt;/td&gt;
      &lt;td style=&quot;text-align: right&quot;&gt;0.5464&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;ul&gt;
  &lt;li&gt;Adding an exclamation mark makes the model think the reviews are more positive, which in my opinion is sensible.&lt;/li&gt;
  &lt;li&gt;Adding the word ‘stars’ seems to have very similar effect to adding an exclamation mark. I do not know if this is just a coincidence - I do not have intuition for this. I guess it must mean that if the word ‘star’ appeared in the training data, then it mostly meant it was a positive review.&lt;/li&gt;
  &lt;li&gt;Adding the word ‘only’ dramatically reduced the positivity score, which makes sense.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;miscellaneous-statements&quot;&gt;Miscellaneous statements&lt;/h3&gt;
&lt;p&gt;Lastly, I tried a variety of miscellaneous statements:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;I liked some bits and I disliked other bits
0.9345

I disliked some bits and I liked other bits
0.8857

I liked most of it, but not all of it
0.9906

I disliked most of it, but not all of it
0.9782

I hated most of it, but not all of it
0.9486

I disliked the movie
0.7621

I hated the movie
0.6873

I cannot believe how bad the movie is
0.5535
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The model is comically bad at judging whether the statement is positive. I started by trying a neutral statement, but the model thought it was great.  I then tried the favourable statement ‘I liked most of it…’ and the model thought it was positive (sensible), but still thought the statement was positive when I switched liked and disliked. Using the word ‘hated’ instead reduced it a bit.&lt;/p&gt;

&lt;p&gt;Maybe the model hasn’t even learnt the word ‘disliked’ and ‘hated’. So I just tried the simple statements ‘I dislike/hated the movie’, and the model does reduce its score, but still thinks these are overall positive. I end with a particularly strong negative review, which the model perceives as slightly positive.&lt;/p&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;
&lt;p&gt;It is nice to play around with a model without any expectation to understand the inner-intricacies or working. My main takeaways are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The model has learnt some sensible patterns, e.g. adding ‘only’ indicates a negative review, adding an exclamation mark indicates a positive review, etc.&lt;/li&gt;
  &lt;li&gt;The model fails spectacularly badly on several examples, which do not feel contrived to me. This makes me question the reliability of sentiment analysis. Maybe the examples I created are not representative of real-world examples.&lt;/li&gt;
  &lt;li&gt;If I were a decision-maker in some business, I would definitely be using manual analysis until I see better evidence.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><summary type="html">Other posts in series FastAI Course, Part III, Frustrations with creating an image classifier FastAI Course, Part I, Lessons 1 and 2</summary></entry></feed>