<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Similarity trees and NaN trees</h1><p class="page-description">I summarise the two main ideas in Sathe and Aggarwal's paper Similarity Forests.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2021-04-17T00:00:00-05:00" itemprop="datePublished">
        Apr 17, 2021
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      3 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#notation">Notation</a></li>
<li class="toc-entry toc-h2"><a href="#trees-via-similarities">Trees via similarities</a></li>
<li class="toc-entry toc-h2"><a href="#trees-with-nan-values">Trees with NaN values</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>I summarise the two main ideas of the paper <a href="http://saketsathe.net/downloads/simforest.pdf">Similarity Forests</a> by Sathe and Aggarwal: trees via similarities and trees with <code class="highlighter-rouge">NaN</code> values.</p>

<h2 id="notation">
<a class="anchor" href="#notation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Notation</h2>
<p>This is a supervised learning setting so the dataset consists of a set of tuples <code class="highlighter-rouge">(X_i, y_i)</code> where:</p>
<ul>
  <li>
<code class="highlighter-rouge">i</code> ranges from <code class="highlighter-rouge">1</code> to <code class="highlighter-rouge">N</code>, so <code class="highlighter-rouge">N</code> is the number of datapoints</li>
  <li>Each <code class="highlighter-rouge">X_i</code> is a <code class="highlighter-rouge">D</code>-dimensional vector, so <code class="highlighter-rouge">D</code> is the number of features. The <code class="highlighter-rouge">d</code>th entry of <code class="highlighter-rouge">X_i</code> will be denoted <code class="highlighter-rouge">X_id</code>
</li>
  <li>Each <code class="highlighter-rouge">y_i</code> is a scalar and is the target or prediction variable</li>
</ul>

<h2 id="trees-via-similarities">
<a class="anchor" href="#trees-via-similarities" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trees via similarities</h2>
<p>In a standard decision tree, the decision or rule at each node takes the following form:</p>
<ul>
  <li>Let <code class="highlighter-rouge">d</code> be one of the possible feature columns</li>
  <li>Let <code class="highlighter-rouge">a</code> be a constant</li>
  <li>Then <code class="highlighter-rouge">X_i</code> is in the left or right child node depending on whether <code class="highlighter-rouge">X_id</code> is smaller than or bigger than <code class="highlighter-rouge">a</code>
</li>
</ul>

<p>At each step, the <code class="highlighter-rouge">d</code> and <code class="highlighter-rouge">a</code> are chosen to (greedily) minimise some loss function, e.g. Gini impurity.</p>

<p>For this process to be possible, we need to have access to the individual entries <code class="highlighter-rouge">X_id</code>. What Sathe and Aggarwal show is that you can construct a decision tree without access to these individual entries <code class="highlighter-rouge">X_id</code> as long as you have access to some form of similarity between your datapoints. This is analagous to the kernel trick for SVMs: you only need to know how to calculate the dot products between data points; you do not need to calculate the vector representation of each of the data points.</p>

<p>So suppose we have access to the similarities between all data points <code class="highlighter-rouge">S</code>, where <code class="highlighter-rouge">S_ij</code> is equal to the similarity between <code class="highlighter-rouge">i</code>th and <code class="highlighter-rouge">j</code>th datapoints. Then the decision rule at each node takes the following form:</p>
<ul>
  <li>Let <code class="highlighter-rouge">j</code> and <code class="highlighter-rouge">k</code> be distinct values from <code class="highlighter-rouge">1</code> to <code class="highlighter-rouge">N</code>, corresponding to two of the datapoints</li>
  <li>Let <code class="highlighter-rouge">a</code> be a constant</li>
  <li>Then <code class="highlighter-rouge">X_i</code> is in the left or right child node depending on whether <code class="highlighter-rouge">S_ij - S_ik</code> is smaller than or bigger than <code class="highlighter-rouge">a</code>
</li>
</ul>

<p>The intuition for this is that you are projecting all your datapoints onto the line joining <code class="highlighter-rouge">X_j</code> and <code class="highlighter-rouge">X_k</code>, and then picking a point on the line to split the datapoints in two. For a derivation of why <code class="highlighter-rouge">S_ij - S_ik</code> corresponds to this projection, and a diagram that helps visualise this projection, you can read the original paper.</p>

<p>And that’s it! There are various details I have skipped over (e.g. how to choose <code class="highlighter-rouge">j</code> and <code class="highlighter-rouge">k</code> at each node), but using <code class="highlighter-rouge">S_ij - S_ik</code> instead of <code class="highlighter-rouge">X_id</code> is the central idea.</p>

<h2 id="trees-with-nan-values">
<a class="anchor" href="#trees-with-nan-values" aria-hidden="true"><span class="octicon octicon-link"></span></a>Trees with <code class="highlighter-rouge">NaN</code> values</h2>
<p>What happens if <code class="highlighter-rouge">X_id</code> is a NaN value? The standard algorithm will not work. In the paper, they describe a possible way of dealing with this. I do not know if this paper is the first to come up with this idea, but it is where I found out about it so I am giving them the credit.</p>

<p>Before describing their solution, spend a couple of minutes thinking about how you might adjust the decision tree algorithm to deal with NaN values (without imputation).</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>No really, I recommend you give it some thought. You will learn more by actively engaging and thinking for yourself, rather than passively reading.</p>

<p>…</p>

<p>…</p>

<p>…</p>

<p>The idea in their paper is quite neat. In the standard decision tree, each node has two children nodes. To deal with with NaN values, just introduce a third child containing the NaN entries! From here, there are various ways of dealing with these NaN-nodes. In the paper, Sathe and Aggarwal make all these nodes be leaf nodes and the prediction at these NaN-nodes is equal to the modal class of its parent. Another option is to just treat these NaN-nodes like any other node, and continue to split on them.  I imagine the strategy one chooses will depend on the particular situation one is in, and whether there are any patterns in the NaN values.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>
<p>To summarise, the paper contains two main new ideas:</p>
<ul>
  <li>To create decision trees from similarities, use <code class="highlighter-rouge">S_ij - S_ik</code> instead of <code class="highlighter-rouge">X_id</code> to decide how to split at each node.</li>
  <li>To deal with NaN values, create a third NaN-node.</li>
</ul>

  </div><a class="u-url" href="/blog/data%20science/2021/04/17/trees_variants.html" hidden></a>
</article>