<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case</h1><p class="page-description">I end this series by describing some experiments I did with the sinusoidal case, before I realised that the learning rate was too big. Spoiler alert: turns out my first instincts from Part I were correct all along...</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-10-01T00:00:00-05:00" itemprop="datePublished">
        Oct 1, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#other-posts-in-series">Other posts in series</a></li>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#regularisation">Regularisation</a></li>
<li class="toc-entry toc-h2"><a href="#visualising-the-loss-function">Visualising the loss function</a></li>
<li class="toc-entry toc-h2"><a href="#investigating-parameter-initialisation">Investigating parameter initialisation</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><h2 id="other-posts-in-series">
<a class="anchor" href="#other-posts-in-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and <strong>S</strong>GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/python/2020/09/10/sgd1.html">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</a></p>
  </li>
</ul>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>I will recap my investigations into fitting sinusoidal data using sinusoidal models with SGD.</p>
<ul>
  <li>In Part I, I described my attempt at using GD and my belief that the failure to fit was due to the model getting stuck in a local minimum where the amplitude is small. I hoped that SGD would fix this problem.</li>
  <li>I tried using SGD but it did not help (described in Part III).</li>
  <li>I then tried using regularisation to solve the amplitude issue (described below).</li>
  <li>I had the idea of investigating the loss function in detail. However, I decided it would be better to separate off this detailed investigation into sinusoidal models into a separate post (this one), and have a post discussing only the stochasticity (Part III).</li>
  <li>While writing up Part III, for the sake of completeness, I investigated the learning rate. This solved the issue!</li>
  <li>However, even though I solved the issues, I thought it would still be worthwhile to write-up my experiments with regularisation and also to carry out the investigation into the loss function. So here we are!</li>
</ul>

<h2 id="regularisation">
<a class="anchor" href="#regularisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Regularisation</h2>
<p>Based on the examples I had tried, the amplitude would always tend to zero. Hence, I thought it would be worth adding a regularisation term that punishes having small amplitudes.</p>

<p>The loss function was <code class="highlighter-rouge">loss = mse(y_est, y)</code>. After the regularisation, it became <code class="highlighter-rouge">loss = mse(y_est, y) - parameters_est[0]</code>.  Why did I choose this regularisation?</p>

<ul>
  <li>I believed that the amplitude would naturally tend to small values. Thus, I want to punish small values and encourage large values.</li>
  <li>Smaller losses should correspond to better models. Therefore, the larger the amplitude, the smaller the loss should be.</li>
  <li>Subtracting the amplitude from the loss achieves this. (Note that the first element of <code class="highlighter-rouge">parameters_est</code> is the amplitude).</li>
  <li>By differentiating, this regularisation causes the amplitude to increase by a constant amount each step, so there is a constant upward pressure on the amplitude.</li>
</ul>

<p>Below is the first result of introducing this regularisation.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin1.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>As you can see, there are moments where the model gets close to the data. This got my hopes up, and made me feel like I was onto something.</p>

<p>I tried various other things to see if I could make it better. I tried changing the weight of the regularisation term. I tried adding other regularisation terms (because in the experiments, it looked like there was now a tendency for the frequency to keep increasing). I can’t remember if I tried other things or not. Suffice it to say, I made no progress.</p>

<p>Below is an animation of an experiment which involved changing the weight of the regularisation term. I include it only because I thought it was particularly funky and visually interesting.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin2.mp4" type="video/mp4"></source>
  </video>
</figure>

<h2 id="visualising-the-loss-function">
<a class="anchor" href="#visualising-the-loss-function" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualising the loss function</h2>
<p>After failing to get regularisation to work, I decided I should try to visualise the loss function, and find out exactly where the local minima were, and hopefully better understand why things were not working.</p>

<p>The process I followed was:</p>
<ul>
  <li>Create data to be fit. That was just <code class="highlighter-rouge">y = sin(x)</code>
</li>
  <li>Create generic sinusoidal models <code class="highlighter-rouge">y_est = a*sin(b*x + c) + d</code>
</li>
  <li>Vary the parameters <code class="highlighter-rouge">a,b,c,d</code> and calculate the loss, <code class="highlighter-rouge">mse(y, y_est)</code>
</li>
  <li>Plot graphs to visualise the loss function</li>
</ul>

<p>To begin, I set <code class="highlighter-rouge">c</code> and <code class="highlighter-rouge">d</code> to <code class="highlighter-rouge">0</code> and varied <code class="highlighter-rouge">a</code> and <code class="highlighter-rouge">b</code>. <code class="highlighter-rouge">a</code> is the amplitude and <code class="highlighter-rouge">b</code> is the frequency (multiplied by <code class="highlighter-rouge">2*pi</code>) or the coefficient of <code class="highlighter-rouge">x</code>. The reason for fixing <code class="highlighter-rouge">c</code> and <code class="highlighter-rouge">d</code> is that it was the amplitude and the frequency which were giving the most trouble.</p>

<p>The first animation below shows a sequence of charts. Each individual chart shows how the loss varies with frequency, and from chart to chart the amplitude is changing.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_loss_vs_freq.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>As can be seen from this, there are many local minima, so the model might get stuck in the wrong one. Eye-balling the chart, if the initial frequency is below 0.5 or above 1.7, then gradient descent will push the frequency away from the optimal value of 1. It is now clear why there should be a tendency for the frequency to increase, as we saw in the SGD examples in Part III.</p>

<p>The next animation is the opposite. For each individual chart, we see how the loss varies with amplitude, and from chart to chart we are modifying the frequency.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_loss_vs_amp.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>Fantastic! This I feel like I can understand. For the majority of frequencies, the optimal value for amplitude is zero and the amplitude will just slide its way to that value. Only for a narrow range of frequencies is the optimal value of the amplitude non-zero.</p>

<p>To summarise, based on these two animations, here is what I would predict:</p>
<ul>
  <li>Regardless of amplitude, there is a narrow band of frequencies which would result in SGD finding the global minimum. Otherwise, you will get stuck in some other local minimum.</li>
  <li>For ‘small’ and ‘large’ frequencies, the amplitude will want to decay to zero. For a certain range of frequncies, the amplitude will tend towards a sensible value.</li>
</ul>

<p>As I am writing this up and thinking things through, I am starting to wonder about my conclusion in Part III about the sinusoidal model. In Part III, I concluded that the issue all along was having an inappropriate learning rate, but the two animations above suggest there is more to it. Did I just get lucky and stumble upon starting parameters which fit the criteria I described above, and hence that is why I got the sinusoidal model to fit?  There’s only one way to find out, which is to do more experimentation!</p>

<h2 id="investigating-parameter-initialisation">
<a class="anchor" href="#investigating-parameter-initialisation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Investigating parameter initialisation</h2>
<p>The steps for the investigation are as follows:</p>
<ul>
  <li>Create data to be fit and generic model, as above.</li>
  <li>Initialise the estimated parameters: <code class="highlighter-rouge">a=1, b=?, c=0, d=0</code>. We will be varying the value of initial value of <code class="highlighter-rouge">b</code>
</li>
  <li>Do SGD and visualise the learning</li>
</ul>

<p>I start by trying a large value for the frequency, 5.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f5.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>So, as predicted, the frequency gets stuck in some sub-optimal value and the amplitude tends to zero. It looks like I did just get lucky in Part III.</p>

<p>Frequency of 2 and 1.5 is similar:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f2.mp4" type="video/mp4"></source>
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f15.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>Frequency of 1.2:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f12.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>We get the model converging to the data! Though this is to be expected, it is still satisfying to see it actually work. With a bit of manual experimentation, the cut-off between these two behaviours is roughly 1.46.</p>

<p>How about lower frequencies? A frequency of 0.6 converges to the correct model:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f06.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>And a frequency of 0.5 converges to a different minima:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd4_sin_f05.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>Again, this is consistent with the frequncy vs loss charts above, where you can see there are local minima to the left of the global minimum.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>
<p>This has been a bit of a topsy-turvy learning experience. I am still surprised at how much I learnt from this basic example. And having struggled with this simple example, I better appreciate how impressive it is to get complicated neural networks to learn.</p>

  </div><a class="u-url" href="/blog/data%20science/python/2020/10/01/sgd4.html" hidden></a>
</article>