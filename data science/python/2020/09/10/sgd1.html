<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</h1><p class="page-description">After seeing the visualisations of gradient descent in the FastAI course, I thought I'd try to create my own. I start by looking at gradient descent applied to linear, quadratic and sinusoidal data.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-10T00:00:00-05:00" itemprop="datePublished">
        Sep 10, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#other-posts-in-series">Other posts in series</a></li>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#linear-data">Linear data</a></li>
<li class="toc-entry toc-h2"><a href="#quadratic-data">Quadratic data</a></li>
<li class="toc-entry toc-h2"><a href="#sinusoidal-data">Sinusoidal data</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
<li class="toc-entry toc-h2"><a href="#the-code">The code</a></li>
</ul><h2 id="other-posts-in-series">
<a class="anchor" href="#other-posts-in-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/data%20science/python/2020/10/01/sgd4.html">Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and <strong>S</strong>GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</a></p>
  </li>
</ul>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>At the end of <a href="https://youtu.be/5L3Ao5KuCC4?t=7244">Lecture 3 of the 2020 FastAI course</a> and at the end of <a href="https://youtu.be/ccMHJeQU4Qw?t=6394">Lecture 2 of the 2018 FastAI course</a>, there are visualisations of the gradient descent algorithm. I quite liked them, in particular the animation from the 2018 version, and I wanted to re-create them and on more complex examples.</p>

<p>The animations I created are available below. Note that in all animations, the orange dots represent the training data, and the blue line represents the model’s predictions. I will go through them and give my thoughts. At the end I describe some insights I gained by doing this.</p>

<h2 id="linear-data">
<a class="anchor" href="#linear-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Linear data</h2>
<p>I created some linear data <code class="highlighter-rouge">y = a*x + b + noise</code>, and then tried to use gradient descent to determine the coefficients.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd1_linear_1.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>This animation is representative of the various examples I tried for linear data. Gradient descent is quick to get very close to the data, but then the learning dramatically slows down and it takes many iterations to improve further. (Note, you have to pay close attention to notice that there is still learning going on throughout the whole video).  Clearly, there is some optimisation that can be done with the learning rate; I did try to create a cutoff point where the learning rate gets bigger, but I am sure there are much better ways of doing things.</p>

<h2 id="quadratic-data">
<a class="anchor" href="#quadratic-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Quadratic data</h2>
<p>Next I created some quadratic data <code class="highlighter-rouge">y=a*x*x + b*x + c + noise</code>.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd1_quadratic_1.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>The pattern here is very similar to the pattern for the linear case: gradient descent quickly reaches a good model, and then the learning dramatically slows down. This is not too surprising, because though the final function is non-linear, this is still a linear-regression problem by treating <code class="highlighter-rouge">x*x</code> and <code class="highlighter-rouge">x</code> as separate features.</p>

<h2 id="sinusoidal-data">
<a class="anchor" href="#sinusoidal-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Sinusoidal data</h2>
<p>Next I created some sinusoidal data <code class="highlighter-rouge">y = a*(sin(b*x + c)) + d</code>. Things were more interesting here.</p>

<p>The first video shows you what happened when I chose a learning rate that was too large (but not so large so as to have everything diverge to infinity):</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd1_sin_1.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>Crazy, right! The model is oscillating back and forth, with the oscillations slowly getting larger with time. In Lecture 3 of the 2020 course, this behaviour is illustrated with the example of using gradient descent to minimise a quadratic function, but I never thought I would actually encounter this behaviour out in the wild.</p>

<p>This second video shows what happens when I choose a smaller learning rate:</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd1_sin_2.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>No craziness here, but it does not converge to an appropriate solution. I think the explanation for this is that the algorithm has found a local-minimum, and so the algorithm gets stuck and cannot improve.  This is qualitatively different to the linear and quadratic cases: since those were both instances of linear regression, it is known from theory that there is only one minimum so gradient descent will reach it. This sinusoidal case cannot be re-written as a linear regression problem, so there is not automatic guarantee of there being only one minimum point; from this experimentation, it looks like there multiple minimum points!</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>
<p>I learnt various things by doing this experiment.</p>

<ul>
  <li>The learning rate is very important! I had to play around with the learning rates to get things to work.</li>
  <li>The range of values in the training data seemed to have big impact on the which learning rates to use. I am not 100% sure about this, but I have read in places that it is important to normalise your data, and perhaps its effect on learning rates is a big reason.</li>
  <li>I learnt how to create animations in matplotlib! And also how to include video files in this blog. :D</li>
</ul>

<p>There are various things I would like to try.</p>

<ul>
  <li>The next thing I will try is using the same datasets, but seeing if I can fit a neural network to the data.</li>
  <li>Stochastic gradient descent. My hope is that it will avoid the local minimum problem in the sinusoidal case.</li>
  <li>Creating a web-app out of this, so you can easily experiment for yourselves.</li>
</ul>

<h2 id="the-code">
<a class="anchor" href="#the-code" aria-hidden="true"><span class="octicon octicon-link"></span></a>The code</h2>
<p>The code for this project is in this <a href="https://github.com/Lovkush-A/pytorch_sgd">GitHub repository</a>.</p>

  </div><a class="u-url" href="/blog/data%20science/python/2020/09/10/sgd1.html" hidden></a>
</article>