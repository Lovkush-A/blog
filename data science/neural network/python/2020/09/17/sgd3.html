<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and **S**GD</h1><p class="page-description">I add the stochasticity in Stochastic Gradient Descent, by using mini-batches. In my previous post, I was hoping this would solve my local minimum with sinusoidal data. To my dismay, it did not help. However, I discover what the problem was all along.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-17T00:00:00-05:00" itemprop="datePublished">
        Sep 17, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#neural network">neural network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#other-posts-in-series">Other posts in series</a></li>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#models-with-neural-networks">Models with neural networks</a></li>
<li class="toc-entry toc-h2"><a href="#fitting-sinusoidal-data-using-a-sinusoidal-model">Fitting sinusoidal data using a sinusoidal model</a></li>
<li class="toc-entry toc-h2"><a href="#conclusions">Conclusions</a></li>
<li class="toc-entry toc-h2"><a href="#the-code">The code</a></li>
</ul><h2 id="other-posts-in-series">
<a class="anchor" href="#other-posts-in-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/data%20science/python/2020/10/01/sgd4.html">Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD</a></p>
  </li>
  <li>
    <p><a href="/blog/data%20science/python/2020/09/10/sgd1.html">Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data</a></p>
  </li>
</ul>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>In the previous posts, I used gradient descent to model linear, quadratic and sinusoidal data. In the first post, the linear and quadratic models could be fit, but the sinusoidal data could not be fit.  In the second post, we saw how neural networks kind of fit the data, but not very well.</p>

<p>This time, I will add the stochasticity by introducing mini-batches. My hope is that I will be able to fit the sinusoidal data that I could not fit in the first post. I will discuss this example at the end, because it is the most interesting</p>

<h2 id="models-with-neural-networks">
<a class="anchor" href="#models-with-neural-networks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Models with neural networks</h2>
<p>Here are animations of a neural network trying to fit using stochastic gradient descent.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_linearnn.mp4" type="video/mp4"></source>
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_quadraticnn.mp4" type="video/mp4"></source>
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sinnn.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>The end results look similar to the end results without using mini-batches. The big difference is that the network converges much faster.</p>

<h2 id="fitting-sinusoidal-data-using-a-sinusoidal-model">
<a class="anchor" href="#fitting-sinusoidal-data-using-a-sinusoidal-model" aria-hidden="true"><span class="octicon octicon-link"></span></a>Fitting sinusoidal data using a sinusoidal model</h2>
<p>Here is a representative example of my first attempts using SGD on sinusoidal data. Note that in all of the animations in this section, the same dataset and initial parameters were used, so the comparisons are more rigorous.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sin1.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>To my dismay, we seem to have the same problem as with normal gradient descent. The system gets stuck in some local minimum where the amplitude is small. It still continues to ‘learn’ though.</p>

<p>At this point, I experimented a little (e.g. with some ‘regularisation’), which I plan to describe in a separate post (spoiler alert - they didn’t work). While planning this blogpost, I re-watched the animation above and thought that maybe the learning rate is too big. I presumably tried playing with the learning rate already but for the sake of completeness, I thought it would be good to produce a series of animations to show you that varying the learning rate does not help.</p>

<p>The learning rate in the animation above was 0.1.  Below is an animation for a learning rate of 0.01.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sin2.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>! That was unexpected! It managed to find good parameters, but then jumped to some inferior local minimum. I managed to achieve something similar to this using the regularisation mentioned above (and which I will describe in a later post), but I was not expecting to see this by changing the learning rate. Clearly my memory is off and I had not experimented with the learning rate. I thought I would re-run the calculations to see if the same behaviour would occur again. (Note that the initial parameters are the same in all these animations, but there is still randomness from how the mini-batches are selected.)</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sin2b.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>So we get similar behaviour. For some time it looks like we are getting close to a good model but then it jumps away to some other set of parameters.</p>

<p>Looks like I should make the learning rate smaller, and see if that prevents jumping away from the correct model. The next animation is for a learning rate of 0.001.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sin3.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>!!! Wow! Given that I had failed after several hours of trying, this was basically magic to me. The model gently and slides its way into position, increases its amplitude, then stays there. Fantastic!</p>

<p>Now a big question arises. Were the learning rates in the first post of this series too high, and that was the reason for the struggles with sinusoidal models? There’s only one way to find out, and that’s by doing the experiment. Below is the resulting animation.</p>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="/blog/images/sgd3_sin3b.mp4" type="video/mp4"></source>
  </video>
</figure>

<p>!!! All this time, it was as simple as changing the learning rate. How did I miss this?! What is noteworthy is how slow the learning is in gradient descent as compared to stochastic gradient descent.</p>

<h2 id="conclusions">
<a class="anchor" href="#conclusions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusions</h2>
<p>There are two big lessons I learnt from this.</p>

<ul>
  <li>
    <p>The first is to somehow take good notes of what I have tried and to be systematic. In my previous job teaching maths to STEM Foundation Year students, my colleague who taught Laboratory Skills was trying to explain the purpose of a lab-book to students: it should be a record of what you did, why you did it, what you observed, etc. so that somebody else (in particular, future-you) can read it and re-live your experience. It looks like I have only now learnt this lesson my colleague was trying to teach. Better late than never, I suppose.</p>
  </li>
  <li>
    <p>The second is that the main benefit of stochastic gradient descent seems to be in efficiency/speed. I have read in places that it can help with preventing local minimums, but I am still unsure of this latter point.</p>
  </li>
</ul>

<h2 id="the-code">
<a class="anchor" href="#the-code" aria-hidden="true"><span class="octicon octicon-link"></span></a>The code</h2>
<p>The code for this project is in this <a href="https://github.com/Lovkush-A/pytorch_sgd">GitHub repository</a>. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask.</p>

  </div><a class="u-url" href="/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html" hidden></a>
</article>