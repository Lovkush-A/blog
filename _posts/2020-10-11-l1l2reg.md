---
toc: true
layout: post
description: I create various charts to help visualise the difference between L1 and L2 regularisation. The pattern is clear and L1 regularisation does tend to force parameters to zero.
categories: [data science, python]
title: Visualising L1 and L2 regularisation
---
## Other posts in series
{% for post in site.posts %}
{% if (post.title contains "L1 and L2") and (post.title != page.title) %}
* [{{ post.title }}]({{ site.baseurl }}{{ post.url }})
{% endif %}
{% endfor %}

## Introduction
In this [medium post comparing L1 and L2 regularisation](https://medium.com/@davidsotunbo/ridge-and-lasso-regression-an-illustration-and-explanation-using-sklearn-in-python-4853cd543898) there is an image showing how L1 regularisation is more likely to make one of the parameters equal to zero than L2 regularisation.

One of my co-fellows at Faculty pointed out that this image is not convincing, because it could just be a case of a cherry-picked cost function. As I had never made any effort to properly understanding L1 versus L2 regularisation previously, this was good motivation for me to better to understand.

The results are bunch of visuals that are below.

## Varying cost function with parameters restricted to L1 or L2 balls

![image]({{ site.baseurl }}/images/l1l2reg_l11.gif)
