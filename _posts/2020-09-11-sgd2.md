---
toc: true
layout: post
description: I continue my project to visual and understand gradient descent. This time I try to fit a neural network to linear, quadratic and sinusoidal data. 
categories: [data science, neural network, python]
title: Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD
---
## Other posts in series
{% for post in site.posts %}
{% if (post.title contains "Stochastic Gradient Descent, Part") and (post.title != page.title) %}
* [{{ post.title }}]({{ site.baseurl }}{{ post.url }})
{% endif %}
{% endfor %}


## Introduction
In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in [Michael Nielsen's online book on neural networks](http://neuralnetworksanddeeplearning.com/chap4.html).) Therefore, I wanted to visualise what happened if we trie to do just that!

## Linear data
I created some linear data `y = a*x + b + noise`, and then tried to fit a neural network to it.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_1.mp4" type="video/mp4">
  </video>
</figure>

This is enchanting. I never thought I'd see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again.

You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from `1e-5` to `1e-4`) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well.

I next tried increasing the learning rate. This next video is an example when the learning rate was `1e-3` throughout.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_3.mp4" type="video/mp4">
  </video>
</figure>

Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the firt one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate.

I next tried introducing a cutoff point where the learning rate increases from `1e-3` to `3e-3`. I have two examples of this.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_5.mp4" type="video/mp4">
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_6.mp4" type="video/mp4">
  </video>
</figure>
