---
toc: true
layout: post
description: I continue my project to visual and understand gradient descent. This time I try to fit a neural network to linear, quadratic and sinusoidal data. 
categories: [data science, neural network, python]
title: Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD
---
## Other posts in series
{% for post in site.posts %}
{% if (post.title contains "Stochastic Gradient Descent, Part") and (post.title != page.title) %}
* [{{ post.title }}]({{ site.baseurl }}{{ post.url }})
{% endif %}
{% endfor %}


## Introduction
In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in [Michael Nielsen's online book on neural networks](http://neuralnetworksanddeeplearning.com/chap4.html).) Therefore, I wanted to visualise what happened if we trie to do just that! I created a function (see code below) which behaves like a neural network with one hidden layer containing twenty neurons.

## Linear data
I created some linear data `y = a*x + b + noise`, and then tried to fit a neural network to it.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_1.mp4" type="video/mp4">
  </video>
</figure>

This is enchanting. I never thought I'd see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again.

You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from `1e-5` to `1e-4`) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well.

I next tried increasing the learning rate. This next video is an example when the learning rate was `1e-3` throughout.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_3.mp4" type="video/mp4">
  </video>
</figure>

Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the first one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate.

I next tried introducing a cutoff point where the learning rate increases from `1e-3` to `3e-3`. I have two examples of this.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_5.mp4" type="video/mp4">
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_linearnn_6.mp4" type="video/mp4">
  </video>
</figure>

In both of these videos, there is a moment in the middle where things go a bit wacky for a few iterations and then quickly settle back down. This wacky moment occurs precisely when the increase in the learning rate kicks in. My explanation for this is that we are seeing the algorithm jump away from one local minimum and moving towards another.

One thing I should have said is that in each of these videos, the training dataset remains the same, but the initialisation of the parameters of the neural network are different. So one other thing we are witnessing from these experiments is how different initialisation results in different models using gradient descent - i.e. there are many local minimums in the parameter space!



## Quadratic data
I created some quadratic data `y = a*x*x + b*x + c + noise` and tried to fit a neural network to it. Below are three examples. Note that as above, the dataset is staying the same each time, but the parameters are initialised differently each time and I play with the learning rates a bit, too.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_quadraticnn_1.mp4" type="video/mp4">
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_quadraticnn_2.mp4" type="video/mp4">
  </video>
</figure>

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd2_quadraticnn_4.mp4" type="video/mp4">
  </video>
</figure>

All these examples have similar overall behaviour. It is quick to get close-ish to the quadratic, and then the learning dramatically slows (but there is still learning going on throughout). Again we see how the different initialisations leads to different final models, showing how we are finding different local minimums.



