---
toc: true
layout: post
description: I end this series by describing some experiments I did with the sinusoidal case, before I realised that the learning rate was too big.
categories: [data science, python]
title: Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case
---
## Other posts in series
{% for post in site.posts %}
{% if (post.title contains "Stochastic Gradient Descent, Part") and (post.title != page.title) %}
* [{{ post.title }}]({{ site.baseurl }}{{ post.url }})
{% endif %}
{% endfor %}


<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd1_linear_1.mp4" type="video/mp4">
  </video>
</figure>

## Introduction
After the post of the series, I believed that the issue with fitting sinusoidal data with sinusoidal model using Gradient Descent is that it gets stuck in a local minimum where the amplitude is small. I hoped that adding stochasticity would fix it. In Part III, I describe the process of adding stochasticity, and that still did not help. Also in Part III, I describe how I eventually realised that the issue was with the learning rate.

In this post, I will describe a couple of things I tried, before trying to lower the learning rate.

## Regularisation
Based on the examples I had tried, the amplitude would always tend to zero and stabilise there. Hence, I thought it would be worth trying adding a regularisation term that punishes having small amplitudes!

The loss function was `loss = mse(y_est, y)`. After the regularisation, it became `loss = mse(y_est, y) - parameters_est[0]`.  Why did I choose this regularisation?

* I believed that the amplitude would naturally tend to small values. Thus, I want to punish small values and encourage large values.
* The loss function is defined so that the smaller the loss, the better.
* Therefore, the larger the amplitude, the smaller the loss should be.
* Subtracting the amplitude from the loss achieves this. (Note that the first element of `parameters_est` was the amplitude).
* By thinking about the learning step, this regularisation would cause the amplitude to increase by a constant amount each step, so there is a constant upward pressure on the amplitude.

Below is the first result of introducing this regularisation.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd4_sin1.mp4" type="video/mp4">
  </video>
</figure>

As you can see, there are moments where the model gets close to the data. This got my hopes up, and made me feel like I was onto something.

I tried various other things to see if I could make it better. I tried changing the weight of the regularisation term. I tried adding other regularisation terms (because in the experiments, it looked like there was now a tendency for the frequency to keep increasing). I can't remember if I tried other things or not. Suffice it to say, I made no progress.

Below is an animation of an experiment which involved changing the weight of the regularisation term. I include it only because I thought it was particularly funky and visually interesting.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd4_sin2.mp4" type="video/mp4">
  </video>
</figure>


## Visualising the loss function
After failing to get regularisation to work, I decided I should try to visualise the loss function, and find out exactly where the local minima were, and hopefully better understand why things were not working.

The process I followed was:
* Create data to be fit. That was just `y = sin(x)`
* Create generic sinusoidal models `y_est = a*sin(b*x + c) + d`
* Vary the parameters `a,b,c,d` and calculate the loss, `mse(y, y_est)`
* Plot graphs to visualise the loss function

To begin, I set `c` and `d` to `0` and varied `a` and `b`. `a` is the amplitude and `b` is the frequency (multiplied by `2*pi`) or the coefficient of `x`. The reason for fixing `c` and `d` is that it was the amplitude and the frequency which were giving the most trouble.


The first animation below shows a sequence of charts. Each individual chart shows how the loss varies with frequency, and from chart to chart the amplitude is changing.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd4_loss_vs_freq.mp4" type="video/mp4">
  </video>
</figure>

As can be seen from this, there are many local minima, so it seems sensible to believe that the model might have trouble getting stuck in one of them. However, it is not clear why there should be a tendency for the frequency to increase, as we saw in the SGD examples in Part III.

The next animation is the opposite. For each individual chart, we see how the loss varies with amplitude, and from chart to chart we are modifying the frequency.

<figure class="video_container">
  <video controls="true" allowfullscreen="true">
    <source src="{{ site.baseurl }}/images/sgd4_loss_vs_amp.mp4" type="video/mp4">
  </video>
</figure>

Fantastic! This I feel like I can understand. For the majority of frequencies, the optimal value for amplitude is zero and the amplitude will just slide its way to that value. Only for a narrow range of frequencies is the optimal value of the amplitude non-zero.

To summarise, based on these two animations, here is what I would predict:
* Regardless of amplitude, there is a narrow band of frequencies which would result in finding the global minimum. Otherwise, you will get stuck in some other local minimum.
* For 'small' and 'large' frequencies, the amplitude will want to decay to zero. For a certain range of frequncies, the amplitude will tend towards the a sensible value.

As I am writing this up and thinking things through, I am starting to wonder about my conclusion in Part III about the sinusoidal model. In Part III, I concluded that the issue all along was having an inappropriate learning rate, but the two animations above suggest there is more to it. Did I just get lucky and stumble upon starting parameters which fit the criteria I described above, and hence that is why I got the sinusoidal model to fit.  There's only one way to find out, which is to do more experimentation!

## Investigating parameter initialisation
The steps for the investigation are as follows:
* Create data to be fit and generic model, as above.
* Initialise the estimated parameters: `a=1, b=?, c=0, d=0`. We will be varying the value of initial value of `b`
* Do SGD and visualise the learning
