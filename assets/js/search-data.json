{
  
    
        "post0": {
            "title": "Squash rankings, Part I, Scraping wikipedia and data analysis",
            "content": "Other posts in series . Introduction . I am going through the CS109 Harvard lectures on data science. I just watched a couple of lectures on web-scraping with BeautifulSoup, so I wanted to practice. I decided to scrape squash ranking data from wikipedia, as I am a avid fan of the sport. On wikipedia, the best information I could find was the top 10 players at the end of each year for the past 25 years or so. . Results . The results of the scraping process are in the following tables, summarising some key stats for the players. The tables are ordered by players’ average rank in the dataset. It is worth emphasising that the data only contains information on Top 10 rankings and only at the end of each year; this will skew the data in various ways. . Summary for female players . | player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year | |:|-:|:|-:|--:|--:|:| | Michelle Martin | 1.400000 | 5 | 1 | 2 | 1994 | 1998 | | Sarah Fitz-Gerald | 2.333333 | 9 | 1 | 5 | 1994 | 2002 | | Nicol David | 2.357143 | 14 | 1 | 6 | 2004 | 2017 | | Raneem El Weleily | 2.444444 | 9 | 1 | 7 | 2011 | 2019 | | Leilani Rorani | 2.750000 | 4 | 1 | 7 | 1998 | 2001 | | Nour El Sherbini | 3.142857 | 7 | 1 | 6 | 2012 | 2019 | | Rachael Grinham | 3.909091 | 11 | 1 | 8 | 2001 | 2011 | | Carol Owens | 4.100000 | 10 | 1 | 8 | 1994 | 2003 | | Laura Massaro | 4.111111 | 9 | 2 | 9 | 2010 | 2018 | | Cassie Jackman | 4.545455 | 11 | 2 | 8 | 1994 | 2004 | | Natalie Grinham | 4.666667 | 9 | 2 | 9 | 2003 | 2013 | | Natalie Grainger | 5.100000 | 10 | 3 | 7 | 1999 | 2009 | | Sue Wright | 5.250000 | 4 | 4 | 8 | 1994 | 1998 | | Jenny Duncalf | 5.375000 | 8 | 2 | 9 | 2005 | 2013 | | Nouran Gohar | 5.400000 | 5 | 3 | 9 | 2015 | 2019 | | Linda Elriani | 5.555556 | 9 | 3 | 9 | 1997 | 2005 | | Suzanne Horner | 5.625000 | 8 | 2 | 9 | 1994 | 2001 | | Tania Bailey | 5.666667 | 6 | 5 | 9 | 1999 | 2007 | | Vanessa Atkinson | 5.875000 | 8 | 1 | 10 | 2002 | 2010 | | Camille Serme | 5.888889 | 9 | 3 | 10 | 2010 | 2019 | | Nour El Tayeb | 6.000000 | 5 | 3 | 8 | 2014 | 2019 | | Kasey Brown | 6.000000 | 2 | 5 | 7 | 2010 | 2011 | | Liz Irving | 6.200000 | 5 | 3 | 10 | 1994 | 1998 | | Alison Waters | 6.400000 | 10 | 3 | 10 | 2008 | 2018 | | Joelle King | 6.666667 | 6 | 4 | 10 | 2012 | 2019 | | Vicky Botwright | 7.000000 | 4 | 5 | 9 | 2003 | 2007 | | Amanda Sobhy | 7.000000 | 2 | 7 | 7 | 2016 | 2019 | | Sarah-Jane Perry | 7.000000 | 3 | 6 | 8 | 2017 | 2019 | | Low Wee Wern | 7.333333 | 3 | 7 | 8 | 2012 | 2014 | | Madeline Perry | 7.428571 | 7 | 4 | 10 | 2006 | 2013 | | Sabine Schoene | 7.500000 | 4 | 6 | 9 | 1995 | 1998 | | Omneya Abdel Kawy | 7.900000 | 10 | 4 | 10 | 2004 | 2016 | | Fiona Geaves | 8.444444 | 9 | 5 | 10 | 1994 | 2004 | | Laura Lengthorn-Massaro | 8.500000 | 2 | 8 | 9 | 2008 | 2009 | | Annie Au | 8.750000 | 4 | 8 | 10 | 2011 | 2015 | | Tesni Evans | 9.000000 | 2 | 9 | 9 | 2018 | 2019 | | Rebecca Macree | 9.250000 | 4 | 8 | 10 | 2001 | 2004 | | Stephanie Brind | 9.250000 | 4 | 7 | 10 | 1999 | 2003 | | Claire Nitch | 9.333333 | 3 | 9 | 10 | 1994 | 1996 | | Jane Martin | 9.500000 | 2 | 9 | 10 | 1994 | 1995 | | Hania El Hammamy | 10.000000 | 1 | 10 | 10 | 2019 | 2019 | | Shelley Kitchen | 10.000000 | 2 | 10 | 10 | 2007 | 2008 | | Dipika Pallikal | 10.000000 | 1 | 10 | 10 | 2012 | 2012 | . Summary for male players . | player | average_rank | years_in_top10 | best_rank | worst_rank | earliest_year | latest_year | |--:|-:|:|-:|--:|--:|:| | Peter Nicol | 2.400000 | 10 | 1 | 8 | 1996 | 2005 | | Ali Farag | 3.250000 | 4 | 1 | 7 | 2016 | 2019 | | Jansher Khan | 3.333333 | 3 | 1 | 8 | 1996 | 1998 | | Jonathon Power | 3.666667 | 9 | 1 | 9 | 1997 | 2005 | | Mohamed El Shorbagy | 3.700000 | 10 | 1 | 10 | 2010 | 2019 | | Ramy Ashour | 3.909091 | 11 | 1 | 7 | 2006 | 2016 | | Grégory Gaultier | 4.000000 | 15 | 1 | 10 | 2003 | 2018 | | Ahmed Barada | 4.250000 | 4 | 2 | 7 | 1997 | 2000 | | Rodney Eyles | 4.333333 | 3 | 2 | 7 | 1996 | 1998 | | Nick Matthew | 4.642857 | 14 | 1 | 10 | 2004 | 2017 | | David Palmer | 4.727273 | 11 | 1 | 9 | 2000 | 2011 | | Amr Shabana | 4.727273 | 11 | 1 | 9 | 2004 | 2014 | | Paul Johnson | 5.000000 | 3 | 4 | 7 | 1998 | 2000 | | Stewart Boswell | 5.000000 | 2 | 4 | 6 | 2001 | 2002 | | Thierry Lincou | 5.200000 | 10 | 1 | 9 | 2001 | 2010 | | James Willstrop | 5.272727 | 11 | 1 | 10 | 2005 | 2017 | | Anthony Ricketts | 5.500000 | 4 | 3 | 7 | 2002 | 2006 | | Karim Darwish | 5.600000 | 10 | 1 | 9 | 2003 | 2013 | | Karim Abdel Gawad | 5.666667 | 6 | 2 | 9 | 2015 | 2019 | | Martin Heath | 5.666667 | 3 | 5 | 6 | 1998 | 2000 | | Del Harris | 6.000000 | 2 | 6 | 6 | 1996 | 1997 | | Dan Jenson | 6.000000 | 1 | 6 | 6 | 1998 | 1998 | | Marwan El Shorbagy | 6.250000 | 4 | 5 | 9 | 2016 | 2019 | | John White | 6.571429 | 7 | 2 | 10 | 1999 | 2007 | | Paul Coll | 6.666667 | 3 | 5 | 8 | 2017 | 2019 | | Omar Mosaad | 6.666667 | 3 | 4 | 8 | 2012 | 2016 | | Tarek Momen | 6.800000 | 5 | 4 | 10 | 2014 | 2019 | | Lee Beachill | 6.800000 | 5 | 1 | 10 | 2002 | 2006 | | Miguel Ángel Rodríguez | 7.000000 | 3 | 5 | 10 | 2015 | 2019 | | Diego Elias | 7.000000 | 1 | 7 | 7 | 2019 | 2019 | | Stefan Casteleyn | 7.000000 | 1 | 7 | 7 | 1999 | 1999 | | David Evans | 7.000000 | 2 | 4 | 10 | 2000 | 2001 | | Simon Parke | 7.000000 | 5 | 3 | 10 | 1996 | 2000 | | Craig Rowland | 7.000000 | 1 | 7 | 7 | 1996 | 1996 | | Chris Walker | 7.000000 | 2 | 4 | 10 | 1996 | 1997 | | Brett Martin | 7.000000 | 2 | 5 | 9 | 1996 | 1997 | | Borja Golán | 7.000000 | 2 | 7 | 7 | 2013 | 2014 | | Peter Barker | 7.142857 | 7 | 5 | 9 | 2008 | 2014 | | Anthony Hill | 7.333333 | 3 | 5 | 9 | 1996 | 1999 | | Simon Rösner | 7.500000 | 6 | 3 | 10 | 2014 | 2019 | | Ong Beng Hee | 7.666667 | 3 | 7 | 8 | 2001 | 2003 | | Mark Chaloner | 8.666667 | 3 | 8 | 10 | 1996 | 2002 | | Laurens Jan Anjema | 9.000000 | 1 | 9 | 9 | 2010 | 2010 | | Alex Gough | 9.000000 | 2 | 9 | 9 | 1999 | 2000 | | Wael El Hindi | 9.000000 | 3 | 8 | 10 | 2007 | 2009 | | Mathieu Castagnet | 9.000000 | 2 | 9 | 9 | 2015 | 2016 | | Paul Price | 9.500000 | 2 | 9 | 10 | 2000 | 2001 | | Derek Ryan | 10.000000 | 1 | 10 | 10 | 1998 | 1998 | | Mohd Azlan Iskandar | 10.000000 | 1 | 10 | 10 | 2011 | 2011 | | Mohamed Abouelghar | 10.000000 | 1 | 10 | 10 | 2018 | 2018 | | Daryl Selby | 10.000000 | 2 | 10 | 10 | 2012 | 2013 | . Patterns and observations . It is satisfying to be able to compare the various big names in squash. . From this, the most outstanding player is Nicol David. They have the largest number of years in the top 10 of any female player by a big margin, and she has the 3rd best average rating. Only two players have a higher average rating: Sarah Fitz-Gerald’s average is for practical purposes the same (2.33 vs 2.35) and Michelle Martin’s is probably skewed by the fact the data only goes as far back as 1994. | A stand-out statistic is that Gaultier has been in the Top 10 for 15 years! Though it is mentioned by commentators frequently, it is only when I compare it to other players’ durations do I fully appreciate how incredible the achievement is. Nick Matthew is not far behind with 14 years. | Another surprise for me is how high Ramy Ashour is, compared to other players. It is well-recognised that he is the best player of his generation by a large margin, but he has also been plagued by injury for most of it, too. I would have predicted that it would have had a noticable dent on his stats. It is scary to think how much better his stats would have been if he did not have injuries! | Ali Farag’s average is very high. Though I do not want to diminish this achievement, I think this is a reflection of how the modern game has fewer elite players, whereas ten years ago, 7 or 8 of the top 10 players had all achieved a World Ranking of 1. | It is interesting to see general patterns. E.g. the players who have spent the most years in the top 10 are also players with higher averages and higher maximum ranks. For the males, this pattern is stark: if they have spent at least 9 years in the Top-10 rankings then they have reached the No. 1 spot. For the females, the pattern is not as clear. This might suggeset that female squash has had a few players that have dominated the top spot, with the remaining players competiting for the other spots in the top 10. | . Like I said earlier, the data is skewed in various ways. . The data only includes rankings at the end of each year. This will hide variations throughout the year. If you use monthly data instead, I expect the patterns will be clearer, and the best players will stand out even more from the good players. | The data only includes rankings that are in the top 10. For the absolute best players, this is not a loss of much data, but for the players in the 5-10 range, significant data is missing about their ranking history. | The data only goes back to the early 90s. This skews data by missing out the achievements of previous great players. The main one is Jahangir Khan, who had a 500+ match winning streak in the 80s! | . Next steps . I will see if I can find more detailed ranking information, so I can get a more fine-grained analysis of the players. A project I have in the back of my mind is to create my own ranking system based on match history, and see if I can create a system which is more predictive than the current system. I think this should be possible, but again, I will need to see if I can obtain the relevant data. . The code . import requests from bs4 import BeautifulSoup from IPython.core.display import HTML import pandas as pd import numpy as np urls = [&#39;https://en.wikipedia.org/wiki/Official_Women%27s_Squash_World_Ranking&#39;, &#39;https://en.wikipedia.org/wiki/Official_Men%27s_Squash_World_Ranking&#39;] def is_not_numeric(s): try: float(s) except ValueError: return True else: return False def table_to_pandas(table): rows = table.find_all(&#39;tr&#39;) headers = [col.text.replace(&#39; n&#39;, &#39;&#39;) for col in rows[0].find_all(&#39;th&#39;)[1:]] n_cols = len(rows[-1].find_all(&#39;td&#39;)) data = [ [col.text.replace(&#39; n&#39;,&#39;&#39;) for col in row.find_all(&#39;td&#39;)[1:] if is_not_numeric(col.text.replace(&#39; n&#39;,&#39;&#39;)) ] for row in rows[1:] ] return pd.DataFrame(data, columns = headers, index = range(1,11)) def url_to_pandas(url): &quot;&quot;&quot; headers contains id needed to cut html into two pieces &quot;&quot;&quot; html = requests.get(url).text start = html.find(&#39;id=&quot;Year_end_world_top_10_players&#39;) end = html.find(&#39;id=&quot;Year-end_number_1&#39;) tables = BeautifulSoup(html[start:end], &#39;html.parser&#39;).find_all(&#39;table&#39;) df = pd.concat([table_to_pandas(t) for t in tables], axis=1) df_stack = df.stack().reset_index() df_stack.columns = [&#39;rank&#39;, &#39;year&#39;, &#39;player&#39;] return df_stack def player_summaries(df): # years_in_top10 = df.player.value_counts() players = ( df.groupby(&#39;player&#39;) .agg({&#39;rank&#39;: [np.mean, &#39;count&#39;, np.min, np.max], &#39;year&#39;: [np.min, np.max] }) ) players.columns = [&#39;average_rank&#39;, &#39;years_in_top10&#39;,&#39;best_rank&#39;, &#39;worst_rank&#39;, &#39;earliest_year&#39;,&#39;latest_year&#39;] players.sort_values(by = [&#39;average_rank&#39;], inplace = True) return players df_f, df_m = [url_to_pandas(urls[i]) for i in range(2)] players_f = player_summaries(df_f) players_m = player_summaries(df_m) .",
            "url": "https://lovkush-a.github.io/blog/python/scraping/2020/09/17/squash1.html",
            "relUrl": "/python/scraping/2020/09/17/squash1.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and **S**GD",
            "content": "Other posts in series . Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD . | Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data . | . Introduction . In the previous posts, I used gradient descent to model linear, quadratic and sinusoidal data. In the first post, the linear and quadratic models could be fit, but the sinusoidal data could not be fit. In the second post, we saw how neural networks kind of fit the data, but not very well. . This time, I will add the stochasticity by introducing mini-batches. My hope is that I will be able to fit the sinusoidal data that I could not fit in the first post. I will discuss this example at the end, because it is the most interesting . Models with neural networks . Here are animations of a neural network trying to fit using stochastic gradient descent. . The end results look similar to the end results without using mini-batches. The big difference is that the network converges much faster. . Fitting sinusoidal data using a sinusoidal model . Here is a representative example of my first attempts using SGD on sinusoidal data. Note that in all of the animations in this section, the same dataset and initial parameters were used, so the comparisons are more rigorous. . To my dismay, we seem to have the same problem as with normal gradient descent. The system gets stuck in some local minimum where the amplitude is small. It still continues to ‘learn’ though. . At this point, I experimented a little (e.g. with some ‘regularisation’), which I plan to describe in a separate post (spoiler alert - they didn’t work). While planning this blogpost, I re-watched the animation above and thought that maybe the learning rate is too big. I presumably tried playing with the learning rate already but for the sake of completeness, I thought it would be good to produce a series of animations to show you that varying the learning rate does not help. . The learning rate in the animation above was 0.1. Below is an animation for a learning rate of 0.01. . ! That was unexpected! It managed to find good parameters, but then jumped to some inferior local minimum. I managed to achieve something similar to this using the regularisation mentioned above (and which I will describe in a later post), but I was not expecting to see this by changing the learning rate. Clearly my memory is off and I had not experimented with the learning rate. I thought I would re-run the calculations to see if the same behaviour would occur again. (Note that the initial parameters are the same in all these animations, but there is still randomness from how the mini-batches are selected.) . So we get similar behaviour. For some time it looks like we are getting close to a good model but then it jumps away to some other set of parameters. . Looks like I should make the learning rate smaller, and see if that prevents jumping away from the correct model. The next animation is for a learning rate of 0.001. . !!! Wow! Given that I had failed after several hours of trying, this was basically magic to me. The model gently and slides its way into position, increases its amplitude, then stays there. Fantastic! . Now a big question arises. Were the learning rates in the first post of this series too high, and that was the reason for the struggles with sinusoidal models? There’s only one way to find out, and that’s by doing the experiment. Below is the resulting animation. . !!! All this time, it was as simple as changing the learning rate. How did I miss this?! What is noteworthy is how slow the learning is in gradient descent as compared to stochastic gradient descent. . Conclusions . There are two big lessons I learnt from this. . The first is to somehow take good notes of what I have tried and to be systematic. In my previous job teaching maths to STEM Foundation Year students, my colleague who taught Laboratory Skills was trying to explain the purpose of a lab-book to students: it should be a record of what you did, why you did it, what you observed, etc. so that somebody else (in particular, future-you) can read it and re-live your experience. It looks like I have only now learnt this lesson my colleague was trying to teach. Better late than never, I suppose. . | The second is that the main benefit of stochastic gradient descent seems to be in efficiency/speed. I have read in places that it can help with preventing local minimums, but I am still unsure of this latter point. . | . The code . The code for this project is in this GitHub repository. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask. .",
            "url": "https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html",
            "relUrl": "/data%20science/neural%20network/python/2020/09/17/sgd3.html",
            "date": " • Sep 17, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD",
            "content": "Other posts in series . Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and SGD . | Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data . | . Introduction . In the previous post, I showed my attempts at using gradient descent to fit linear, quadratic and sinusoidal data using (respectively) linear, quadratic and sinusoidal models. However, the universal approximation theorem says that the set of vanilla neural networks with one hidden layer can approximate any function to arbitrary precision. (An excellent and interactive sketch proof of this, where I first learnt about this theorem, is given in Michael Nielsen’s online book on neural networks.) Therefore, it should be possible for a neural network to model the datasets I created in the first post, and it should be interesting to see the visualisations of the learning taking place. . Linear data . I created some linear data y = a*x + b + noise, and then tried to fit a neural network to it. . This is enchanting. I never thought I’d see something so delicate and graceful from crunching a whole bunch of numbers. I could watch this over and over again. . You will probably notice that the learning suddenly speeds up at about 17 seconds in the video. This is a result of me increasing the learning rate (from 1e-5 to 1e-4) at a certain cut-off point. It is nice to be able to visually see the effect of changing the learning rate. Regarding the learning itself. it seems that the neural network struggles to fit the line well. . I next tried increasing the learning rate. This next video is an example when the learning rate was 1e-3 throughout. . Changing the learning rate has improved the performance of the neural network considerably. The video is not as calm and satisfying to watch as the first one (though there is something comical about the jerky movements at the start), but it illustrates the value in choosing a good learning rate. . I next tried introducing a cutoff point where the learning rate increases from 1e-3 to 3e-3. I have two examples of this. . In both of these videos, there is a moment in the middle where things go a bit wacky for a few iterations and then quickly settle back down. This wacky moment occurs precisely when the increase in the learning rate kicks in. My explanation for this is that we are seeing the algorithm jump away from one local minimum and moving towards another. . One thing I should have said is that in each of these videos, the training dataset remains the same, but the initialisation of the parameters of the neural network are different. So one other thing we are witnessing from these experiments is how different initialisation results in different models using gradient descent - i.e. there are many local minimums in the parameter space! . Quadratic data . I created some quadratic data y = a*x*x + b*x + c + noise and tried to fit a neural network to it. Below are three examples. Note that as above, the dataset is staying the same each time, but the parameters are initialised differently each time and I play with the learning rates a bit, too. . All these examples have similar overall behaviour. It is quick to get close-ish to the quadratic, and then the learning dramatically slows (but there is still learning going on throughout). Again we see how the different initialisations leads to different final models, showing how we are finding different local minimums. . Sinusoidal data . I created some sinusoidal data y = a*sin(b*x+ c) + d and tried to fit a neural network to it. As before, same dataset but with different starting parameters. . The first one shows some oscillating behaviour, similar to the first sinusoidal case in the first post of this series. Looks like the behaviour is not as unlikely as I previously thought. The second two produce mediocre results, at best. . Looking at the graphs, there seem to be ‘too many wiggles’, so I thought I’d try reducing the number of neurons in the hidden layer. The next three videos show what happens with eight neurons. . These perform less well than with twenty neurons. The final video is intersting for its crazy behaviour at the start - the function seems to spin around and do all kinds of weird stuff before settling down. I do not know what to make of that. . There is lots more experimentation that can be done, but I feel this is a good place to stop and move on to other things to investigate. . Conclusions . There are various things I learnt doing this. . There is a lot to be learnt by playing around with the ideas. Though (I think) I understand the theory of gradient descent and of vanilla neural networks, it is evident that the whole is greater than the sum of its parts. I under-estimated what I could have learnt by experimenting. . | It seems neural networks are only useful for interpolation - i.e. making predictions for data similar to the training data. They are hopeless at extrapolating: I think they necessarily have horizontal asymptotes in both x-directions. . | The initialisation of parameters matters a lot for neural networks. My very first settings were useless, and were seemingly only capable of modelling a sigmoid function. For a while, I thought my code was buggy, but after some experimentation, it turned out the initial parameter settings were inappropriate. | Different initialisations resulted in vastly different final models. Presumably this is also the case with more complex tasks, but maybe it does not matter when doing a classification on real data. | . | The range of values in the data impacts the choice of parameters. As I mentioned in the first post, I have read that normalising is important. The fact that the parameter settings are sensitive to the range of values in the dataset could be a big reason why normalising is important. . | Manually creating a neural network (even with only 1 layer) is a bit of faff; I should learn how to create one using the pytorch library. . | You can create some funky visuals with neural networks! Somebody more creative than I am could add some music to each of these to produce some interesting art. These animations may even be a weird alternative to Rorschach inkblot tests: ‘How does this video make you feel? What do you see in this video?’ | . The code . The code for this project is in this GitHub repository. I encourage you to play around and see what you can learn. If there is anything you do not understand in the code, please ask. .",
            "url": "https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html",
            "relUrl": "/data%20science/neural%20network/python/2020/09/11/sgd2.html",
            "date": " • Sep 11, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Stochastic Gradient Descent, Part I, Gradient descent on linear, quadratic and sinusoidal data",
            "content": "Other posts in series . Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and SGD . | Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD . | . Introduction . At the end of Lecture 3 of the 2020 FastAI course and at the end of Lecture 2 of the 2018 FastAI course, there are visualisations of the gradient descent algorithm. I quite liked them, in particular the animation from the 2018 version, and I wanted to re-create them and on more complex examples. . The animations I created are available below. Note that in all animations, the orange dots represent the training data, and the blue line represents the model’s predictions. I will go through them and give my thoughts. At the end I describe some insights I gained by doing this. . Linear data . I created some linear data y = a*x + b + noise, and then tried to use gradient descent to determine the coefficients. . This animation is representative of the various examples I tried for linear data. Gradient descent is quick to get very close to the data, but then the learning dramatically slows down and it takes many iterations to improve further. (Note, you have to pay close attention to notice that there is still learning going on throughout the whole video). Clearly, there is some optimisation that can be done with the learning rate; I did try to create a cutoff point where the learning rate gets bigger, but I am sure there are much better ways of doing things. . Quadratic data . Next I created some quadratic data y=a*x*x + b*x + c + noise. . The pattern here is very similar to the pattern for the linear case: gradient descent quickly reaches a good model, and then the learning dramatically slows down. This is not too surprising, because though the final function is non-linear, this is still a linear-regression problem by treating x*x and x as separate features. . Sinusoidal data . Next I created some sinusoidal data y = a*(sin(b*x + c)) + d. Things were more interesting here. . The first video shows you what happened when I chose a learning rate that was too large (but not so large so as to have everything diverge to infinity): . Crazy, right! The model is oscillating back and forth, with the oscillations slowly getting larger with time. In Lecture 3 of the 2020 course, this behaviour is illustrated with the example of using gradient descent to minimise a quadratic function, but I never thought I would actually encounter this behaviour out in the wild. . This second video shows what happens when I choose a smaller learning rate: . No craziness here, but it does not converge to an appropriate solution. I think the explanation for this is that the algorithm has found a local-minimum, and so the algorithm gets stuck and cannot improve. This is qualitatively different to the linear and quadratic cases: since those were both instances of linear regression, it is known from theory that there is only one minimum so gradient descent will reach it. This sinusoidal case cannot be re-written as a linear regression problem, so there is not automatic guarantee of there being only one minimum point; from this experimentation, it looks like there multiple minimum points! . Conclusion . I learnt various things by doing this experiment. . The learning rate is very important! I had to play around with the learning rates to get things to work. | The range of values in the training data seemed to have big impact on the which learning rates to use. I am not 100% sure about this, but I have read in places that it is important to normalise your data, and perhaps its effect on learning rates is a big reason. | I learnt how to create animations in matplotlib! And also how to include video files in this blog. :D | . There are various things I would like to try. . The next thing I will try is using the same datasets, but seeing if I can fit a neural network to the data. | Stochastic gradient descent. My hope is that it will avoid the local minimum problem in the sinusoidal case. | Creating a web-app out of this, so you can easily experiment for yourselves. | . The code . The code for this project is in this GitHub repository. .",
            "url": "https://lovkush-a.github.io/blog/data%20science/python/2020/09/10/sgd1.html",
            "relUrl": "/data%20science/python/2020/09/10/sgd1.html",
            "date": " • Sep 10, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "FastAI Course, Part III, Frustrations with creating an image classifier",
            "content": "Other posts in series . FastAI Course, Part II, Lesson 1 and sentiment analysis . | FastAI Course, Part I, Lessons 1 and 2 . | . Introduction . After reading through Chapter 2 of the Fast AI course/book, it was time I tried putting the ideas into practice. I decided to create a classifier that distinguishes between squash, tennis and badminton rackets. Then using voila and binder, I created a little web app for it: check it out here! . A comedy of errors . This project was frustrating for me. I somehow managed to make numerous errors at every step which could not be straightforwardly de-bugged. I try to describe them here, but I have undoubtedly forgotten many of them. . The Bing API . The instructions from the notes: “To download images with Bing Image Search, sign up at Microsoft for a free account. You will be given a key, which you can copy and enter in a cell as follows.” Sounds simple. I went to the Microsoft website and logged on. This is what I saw: . . Hmm, there’s nothing obvious here regarding Bing or APIs. I spent a whole bunch of time looking through all the menus, but to no avail. The instructions made it sound simple, and yet I couldn’t manage. I searched in the FastAI forums and a relevant post was easy to find. It explained the (non-trivial) steps needed. A big non-triviality is that you should be using a Microsoft Azure account, not a Microsoft account. I think the book should make this clearer. It would have saved me (and presumably many others) a bunch of time. . Padding . I downloaded the datasets, viewed some of the images, and deleted the files which were erroneous. Next up was to create a DataBlock with the appropriate transforms/data augmentation. To ensure the images have the correct aspect ratio, I decided that padding is the best option. I tried it, using pad_mode = &#39;zeros&#39;. It looked alright, but most of the images have white backgrounds, so I thought maybe it would be better for the images to be padded with white space. I guessed that pad_mode = &#39;ones&#39; would work, but alas it did not. So, I did what has been recommended numerous times, and went to the docs. . This is what I found: . . Maybe I am in the minority, but I do not find this helpful. Here are some things that frustrate me about this: . What are the possibilities for *args and ** kwargs? How would I find out? | What does ‘All possible mode as attributes to get tab-completion and typo-proofing’ mean?! What does it have to do with padding images? | What is TensorBBox? Or TensorPoint? | What’s the relevance of the change in font in the headings? Is it self-evident? Looking at it more closely, you have one font for classes and one font for methods. But do we need a different font for that? Changing heading formats usually indicates a change in the level of headings, but I don’t think that is what is happening here. | Most importantly, what are the different options for pad_mode? If you scroll further down, three examples are provided (zeros, reflection and border), but it is not clear if these are all the options. Maybe they are, maybe they aren’t. I don’t know. | Really, what does ‘All possible mode as attributes to get tab-completion and typo-proofing’ actually mean?? | . Perhaps I could find out more by looking at the source code. After all, as is emphasised several times in the lectures, most of the source code is only a few lines and thus easy to understand. . class CropPad(RandTransform): &quot;Center crop or pad an image to `size`&quot; order = 0 def __init__(self, size, pad_mode=PadMode.Zeros, **kwargs): size = _process_sz(size) store_attr() super().__init__(**kwargs) def encodes(self, x:(Image.Image,TensorBBox,TensorPoint)): orig_sz = _get_sz(x) tl = (orig_sz-self.size)//2 return x.crop_pad(self.size, tl, orig_sz=orig_sz, pad_mode=self.pad_mode) . Yes, there are only several lines of code here. However, they are not helpful, at least not for me. In order to understand any of this, I need to go searching through the rest of the code base, which I am not inclined to do. I should not have to do this to find out if I can create white padding instead of black padding (or maybe I should?). . Anyway, I stopped here with the presumption that it is not possible to get white padding. . RuntimeError: DataLoader worker (pid 19862) is killed by signal: Killed. . I made various other decisions for the DataBlock and created the DataLoaders. Now it was time to do the modelling! I ran the cell, and to my dismay, I got the error message shown in the heading. I didn’t understand it. I checked all my code carefully and did not spot anything wrong. Next I tried to run the examples from the book as is. The same error came up. This was bizarre! . It was time for some Googling. I searched around, saw people with similar error messages, and tried their suggestions. None of them solved the problem. It was time to get to bed at this stage, so I decided I will ask about this in the forum, and have another stab the next day. . Next day, I got ready to tackle the problem and opened up Gradient. But something was a little off: . . The machine type was set to CPU! I did not remember selecting this, so was a bit confused. Anyway, I tried starting up the notebook, and the pop-up comes up to select the options. The default selection is the CPU. Hmm, that is odd. I tried to change it to GPU, and then I discovered the problem: there was no Free GPU available. It looks like the day before, I just assumed the settings would be the same as previous runs, so I did not check the machine type, and I must have been using the CPU option. This is almost certainly the cause of the DataLoader worker error. At least I now know the issue. . Time to move to Colab. . Google Colab . This is just me, but I dislike the Colab interface. It is similar to Jupyter Notebook, but different in little ways that my brain finds off-putting. One example is that in Colab, the shortcut a to insert a new cell automatically enters the cell; this is not what I want, as I often want to create a bunch of cells at once, and then move into them. . These are minor issues, but I thought I may as well include them, in case anybody else out there gets some comfort knowing they are not alone. . Re-training model post cleaning . I continued with the project, and managed to get the classifier to work. I then cleaned the data using the easy-to-use widget created by FastAI. I then tried to do some further fine tuning, but I got an error message saying that a certain image file, 00000149.png, cannot be found. . I had a little look at the code, and realised my error. I deleted some images while cleaning (including image 00000149.png), but did not re-run the cell that defines the dataloaders dls variable, where the information on the dataset is stored. So I re-ran the cell, and tried again. But I still got the error message. . I wanted to investigate the dls variable to find out what is going on; specifically, I wanted to see which filenames are stored in the variable and check whether 00000149.png is listed there. Similar to the issues I had with the padding above, I did not find the documentation helpful. However, using dir (a super handy function, by the way. It lists all the properties and methods that an object has) and playing a bit, I was able to find the list of filenames stored in dls; 00000149.png was not there! . This left only one possibility: that filenames are stored in the learn variable. Based on my experience with other ML tools (e.g. scikitlearn), this is counter-intuitive to me. At this stage, I just wanted to get a working example as quickly as possible, so I did not investigate how I can do additional fine-tuning cycles on new datasets. Instead, I just created a fresh learner object, and trained that. . I am not sure anybody can do anything about this kind of problem, as it stems from using tools as black-boxes, without understanding how they work. Another example of this I have had is in using conda: I am comfortable creating new environments, but I do not know how to rename a directory or move directories around, because that information is stored somewhere for conda to function, and I do not know how to change that information. . Unfortunately, it does not change that the experience is frustrating - there is a simple task that I want to do, my naive approach does not work, and in order to learn how do it properly, I have to get a fairly comprehensive understanding of the tool. If anybody has any advice or suggestions, do please let me know! . Binder . The next bunch of tasks went well, as far as I can remember. Exporting the learner, creating a local environment where I can create the jupyter notebook that will be turned into the web app, installing all the packages, creating the notebook, getting voila to work locally, and pushing it to github. I was literally one step away: get binder to work. The finish line was literally right in front of me. . Oh boy was I wrong. I do not know how I managed to make such a mess of this step. From what I remember, I encountered two main problems, but my ineptitude dragged out the process far longer than it ought to have. . The first mistake was not to change the File setting to URL, when you enter the path to the file in Binder. This setting was in my blindspot; I never even noticed that box. This is made worse because the voila documentation clearly states that you should do this; in fact, this is how I found the error, because I chose to re-read the instructions carefully, after my various other attempts failed. . The second issue I had was with the requirements file that should be provided. I first tried using the file created by using conda env export, but that did not work. Googling the error messages revealed that conda lists many device specific requirements, which will not work with the Linux machines that binder uses. Googling also showed the two-step process to get around this issue: create a modified version of the requirements by adding some tags to the conda env export command, and then manually remove any requirements that still cause an error on Binder. . This was it: I fixed the two problems and was minutes away from completion. I got binder going, and as it does, it was taking a few minutes to things set-up. But things were taking very long. Something was not right. I just kept waiting, but eventually, the process quit with some other error message. . I was so close, yet so far. I tried searching around on the forums, to see if anybody else had any issues on binder. I ended up finding a blog post and example github repo, so I tried to imitate that. The main takeaway was that they manually constructed a requirements file with 4 or 5 requirements, rather than using an automatically generated file from conda with many more requirements. . After a few more attempts and ironing out several little mistakes (e.g. they used fastai2 in their requirements, which is now out-of-date), it finally worked. . Final remarks . This has been a stressful but enlightening experience. Just writing this blog made me re-live the emotions: the frustration when things seemed unnecessarily complicated, and the sense of achievement and relief when I finally reached the end. . It may sound like I dislike FastAI and the course, but the opposite is true. Like I said in my first post on the FastAI course, I am impressed with the teaching and all the thought that has gone into it. I would strongly recommend this course to anybody interested in AI or Machine Learning. However, I think this may also be why I experience more frustration with it. I have such a high opinion of the course, that any negative stands out; I am holding the team to particularly high standards. . To end, I hope this blog entry has been useful for you in some way. I welcome any comments or thoughts or disagreements; please feel free to let me know. .",
            "url": "https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/09/08/fastai3.html",
            "relUrl": "/data%20science/neural%20network/python/2020/09/08/fastai3.html",
            "date": " • Sep 8, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Analysing the movies I've watched, Part V, Data visualisation II",
            "content": "Other posts in series . Analysing the movies I’ve watched, Part IV, Data visualisation . | Analysing the movies I’ve watched, Part III, Joining the tables . | Analysing the movies I’ve watched, Part II, Data cleaning . | Analysing the movies I’ve watched, Part I, Data collection . | . Introduction . In a previous post, I described how the joining missed out half the data. After making various plots, is turns out the joining was sometimes erroneous too. Hence, I bit the bullet and decided I to manually do the joining, and get a complete and clean dataset. Learning the lessons from past attempts at manual processing, it was a fairly smooth process. (Furthermore, things were significantly sped up by using Selenium - which allowed me to automate the process of searching for a movie on imdb). In the end, it took roughly 3 hours to tidy the dataset and have the official imdb id attached to each entry in my database. And now for the charts! . Year of release . Below are charts showing information about the year of release for the movies I have watched. The first one is just a histogram, and the second shows a scatter-chart showing the year I watched the movie against the year the movie released. . . Not sure if there is anything surprising about any of these charts. One neat thing is that one could probably get an idea of my age based on the first chart. The second chart seems to show that the majority of movies I watch are relatively new. . Ratings . Below is a chart showing the average ratings (by users of imdb) for the movies I have watched, along with the distribution of ratings for all the movies in imdb. . . It is clear from this chart that I tend to watch ‘decent’ movies, as judged by the movies, and that I avoid ‘bad’ movies, but I also seem to avoid the ‘best’ movies. . Below is a chart comparing the ratings for all the movies I have watched with the ratings of the movie I would recommend. . . It is reassuring to see that, overall, the movies I recommend are generally better regarded than the movies I do not recommend. . Lastly, here is a chart showing the variation in ratings by the year I watched the movie. . . I cannot really discern any patterns here, that aren’t revealed by other charts. Patterns that I can see are that I watched a lot more movies (and greater variety of quality of movies) in 2008 and 2009, and that the movies I recommend tend to have higher ratings than those I don’t. . To end this section, here is a list of the most and least popular movies I have watched. . Movie Rating on imdb . the shawshank redemption | 9.3 | . the dark knight | 9.0 | . 12 angry men | 8.9 | . the lord of the rings: the return of the king | 8.9 | . pulp fiction | 8.9 | . fight club | 8.8 | . the lord of the rings: the fellowship of the ring | 8.8 | . forrest gump | 8.8 | . inception | 8.8 | . the matrix | 8.7 | . the lord of the rings: the two towers | 8.7 | . goodfellas | 8.7 | . star wars: episode v - the empire strikes back | 8.7 | . se7en | 8.6 | . city of god | 8.6 | . interstellar | 8.6 | . saving private ryan | 8.6 | . the silence of the lambs | 8.6 | . star wars: episode iv - a new hope | 8.6 | . gladiator | 8.5 | . the departed | 8.5 | . casablanca | 8.5 | . the usual suspects | 8.5 | . dear zachary: a letter to a son about his father | 8.5 | . whiplash | 8.5 | . joker | 8.5 | . psycho | 8.5 | . senna | 8.5 | . Movie Rating on imdb . steel | 2.8 | . troll 2 | 2.9 | . the foreigner | 3.4 | . batman &amp; robin | 3.7 | . half a person | 3.8 | . friday the 13th part viii: jason takes manhattan | 4.6 | . barely legal | 4.7 | . elektra | 4.7 | . doe: dead or alive | 4.8 | . friday the 13th: a new beginning | 4.8 | . uncut gem | 4.8 | . blair witch | 5.0 | . a teacher’s crime | 5.0 | . tekken: the motion picture | 5.3 | . in their skin | 5.3 | . daredevil | 5.3 | . scary movie 2 | 5.3 | . the circle | 5.3 | . friday the 13th part vii: the new blood | 5.3 | . little | 5.4 | . The one thing I feel slightly ashamed of is that I watched so many of the Friday the 13th movies. I think my reasoning was that if a series has that many movies, there must be something good about it. I kept going on, even though I thought the movies were bad, in the hope that there is a moment in the series where the movies become good. I did finally stop watching the series - it just took eight movies to make that decision… . Genres . Below is a table showing a breakdown of the genres of movie I watched, ordered by which genre has the highest odds of me recommending it. Note that most movies had multiple genres. . Genre Frequency Frequency Recommended Percent recommended . thriller | 126 | 37 | 0.29 | . war | 14 | 4 | 0.29 | . sport | 18 | 5 | 0.28 | . drama | 319 | 87 | 0.27 | . sci-fi | 86 | 22 | 0.26 | . crime | 146 | 36 | 0.25 | . biography | 47 | 11 | 0.23 | . music | 18 | 4 | 0.22 | . romance | 56 | 11 | 0.20 | . comedy | 210 | 42 | 0.20 | . mystery | 80 | 15 | 0.19 | . animation | 51 | 8 | 0.16 | . family | 25 | 4 | 0.16 | . history | 26 | 4 | 0.15 | . fantasy | 70 | 10 | 0.14 | . documentary | 31 | 4 | 0.13 | . adventure | 188 | 23 | 0.12 | . horror | 66 | 7 | 0.11 | . action | 227 | 23 | 0.10 | . musical | 6 | 0 | 0.00 | . news | 2 | 0 | 0.00 | . western | 1 | 0 | 0.00 | . The entries that stand out to me are those with the largest frequencies. There is strong evidence that I am more likely to recommend a thriller or a drama than a comedy, and that I am unlikely to recommend adventure or action movies. . Directors . Below is a list of the directors, along with the movies, whose movies I have most watched. . Steven Spielberg a.i. artificial intelligence catch me if you can jaws jurassic park minority report raiders of the lost ark saving private ryan the adventures of tintin Peter Jackson king kong the hobbit: an unexpected journey the hobbit: the battle of the five armies the lord of the rings: the fellowship of the ring the lord of the rings: the return of the king the lord of the rings: the two towers Christopher Nolan dunkirk inception interstellar memento the dark knight the dark knight rises Guy Ritchie lock, stock and two smoking barrels rocknrolla sherlock holmes sherlock holmes: a game of shadows snatch the man from u.n.c.l.e. Martin Scorsese cape fear goodfellas silence taxi driver the departed the wolf of wall street Quentin Tarantino inglourious basterds kill bill: vol. 1 once upon a time... in hollywood pulp fiction reservoir dogs the hateful eight Danny Boyle 28 days later steve jobs t2 trainspotting trainspotting yesterday Matthew Vaughn kick-ass kingsman: the secret service layer cake stardust x-men: first class David Fincher fight club gone girl se7en the curious case of benjamin button the social network M. Night Shyamalan glass lady in the water split the sixth sense unbreakable Darren Aronofsky black swan mother! noah requiem for a dream the wrestler . I tend not to pay any attention to who the director is, so for me, the interesting thing here is to see how movies that I would have said had no connection in fact have the same director. E.g. I did not realise I had watched that many movies by Spielberg. The other thing that stands out here is that all the directors are male. . Actors . Below is a list of actors, along with the movies, that I have most watched. . Brad Pitt 12 years a slave allied burn after reading fight club inglourious basterds killing them softly megamind moneyball once upon a time... in hollywood se7en snatch the big short the curious case of benjamin button troy war machine Bruce Willis alpha dog die hard die hard 2 die hard 4.0 die hard with a vengeance fast food nation glass looper moonrise kingdom pulp fiction sin city the sixth sense unbreakable Samuel L. Jackson die hard with a vengeance glass jumper kingsman: the secret service pulp fiction star wars: episode iii - revenge of the sith the hateful eight the hitman&#39;s bodyguard the incredibles the negotiator unbreakable unicorn store Matt Damon adjustment bureau contagion good will hunting jason bourne ponyo saving private ryan the bourne identity the bourne ultimatum the departed the legend of bagger vance the martian the talented mr. ripley Tom Hanks angels &amp; demons catch me if you can forrest gump saving private ryan the circle toy story toy story 2 toy story of terror you&#39;ve got mail Robert De Niro analyze this cape fear dirty grandpa goodfellas hide and seek joker meet the parents righteous kill taxi driver Rachel McAdams about time aloha doctor strange mean girls red eye sherlock holmes sherlock holmes: a game of shadows state of play wedding crashers Natalie Portman annihilation black swan closer star wars: episode i - the phantom menace star wars: episode ii - attack of the clones star wars: episode iii - revenge of the sith the other boleyn girl thor v for vendetta Leonardo DiCaprio blood diamond catch me if you can inception once upon a time... in hollywood the departed the man in the iron mask the revenant the wolf of wall street Ben Affleck argo batman v superman: dawn of justice daredevil gone girl good will hunting paycheck state of play the accountant Jake Gyllenhaal donnie darko jarhead nightcrawler nocturnal animals prisoners proof source code velvet buzzsaw Ian McKellen stardust the hobbit: an unexpected journey the hobbit: the battle of the five armies the lord of the rings: the fellowship of the ring the lord of the rings: the return of the king the lord of the rings: the two towers x-men x2: x-men united Christian Bale 3:10 to yuma american psycho public enemies terminator salvation the big short the dark knight the dark knight rises the machinist Morgan Freeman along came a spider deep impact lucy million dollar baby se7en the bucket list the shawshank redemption wanted Tom Cruise collateral edge of tomorrow minority report mission: impossible rain man the last samurai valkyrie vanilla sky Johnny Depp a nightmare on elm street fantastic beasts: the crimes of grindelwald fear and loathing in las vegas pirate of the caribbean: at world&#39;s end pirates of the caribbean: dead man&#39;s chest public enemies the tourist Ewan McGregor angels &amp; demons black hawk down star wars: episode i - the phantom menace star wars: episode ii - attack of the clones star wars: episode iii - revenge of the sith t2 trainspotting trainspotting Laurence Fishburne john wick: chapter 3 - parabellum passengers predators the matrix the matrix reloaded the matrix revolutions the signal Robert Downey Jr. avengers: infinity war iron man 2 iron man 3 sherlock holmes sherlock holmes: a game of shadows spider-man: homecoming the avengers Jude Law a.i. artificial intelligence closer contagion gattaca sherlock holmes sherlock holmes: a game of shadows the talented mr. ripley Kevin Spacey a bug&#39;s life american beauty margin call moon se7en the negotiator the usual suspects Orlando Bloom good doctor pirate of the caribbean: at world&#39;s end pirates of the caribbean: dead man&#39;s chest the lord of the rings: the fellowship of the ring the lord of the rings: the return of the king the lord of the rings: the two towers troy Scarlett Johansson her jojo rabbit lost in translation lucy marriage story the avengers the other boleyn girl Angelina Jolie alexander girl, interrupted kung fu panda kung fu panda 2 the tourist wanted Paul Giamatti duplicity lady in the water private life sideways the illusionist win win Joaquin Phoenix gladiator her hotel rwanda joker quills you were never really here Jim Carrey ace ventura: pet detective dumb and dumber eternal sunshine of the spotless mind the mask the truman show yes man Anthony Hopkins alexander noah proof red dragon the silence of the lambs thor Tom Hardy dunkirk layer cake locke mad max: fury road the dark knight rises the revenant Keanu Reeves john wick john wick: chapter 2 john wick: chapter 3 - parabellum the matrix the matrix reloaded the matrix revolutions Emma Watson beauty and the beast harry potter and the deathly hallows: part 1 harry potter and the goblet of fire harry potter and the half-blood prince noah the circle Clive Owen children of men closer duplicity inside man sin city the bourne identity Ethan Hawke boyhood gattaca lord of war predestination the magnificent seven the purge John Malkovich being john malkovich burn after reading johnny english ripley&#39;s game the man in the iron mask warm bodies Elijah Wood deep impact happy feet maniac the lord of the rings: the fellowship of the ring the lord of the rings: the return of the king the lord of the rings: the two towers Cameron Diaz being john malkovich shrek forever after shrek the third the mask vanilla sky what happens in vegas . Nice list of some of the more famous actors in the industry. One thing that surprised is how many movies I have watched that starred Brad Pitt - I did not realise it was that many! Once again, it is worth noting that females are under-represented here. . Conclusion . This project has probably been the one that I spent the most time on so far. I have learnt a whole bunch and managed to produce some nice visuals and summaries. Hopefully it was interesting to read! .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/09/02/mymovies5.html",
            "relUrl": "/python/data%20science/2020/09/02/mymovies5.html",
            "date": " • Sep 2, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "FastAI Course, Part II, Lesson 1 and sentiment analysis",
            "content": "Other posts in series . FastAI Course, Part III, Frustrations with creating an image classifier . | FastAI Course, Part I, Lessons 1 and 2 . | . Introduction . Recently, FastAI launched a new version of their course, using their revamped FastAI version 2 packages. As I had only completed the first two lessons of the previous version of the course, I thought I’d restart. It is good I did re-start, because they have made big changes to how the course is organised and also to the syntax (e.g. in the old version, there was significant discussion about setting the learning rate using their ‘learning rate finder’, but so far, there has been no mention of this and it looks like that has been automated). Towards the end of the first lesson, Jeremy showcases the different domains in which one can use Deep Learning. One of those was NLP. I have not yet done any NLP, so I thought this is a good chance to play around a little, without having to delve into any of the technical details. . In this particular example, the NLP consists of sentiment analysis of movie reviews. Looking at the code, they use AWD LSTM architecture. A quick google search reveals that this is a fairly new algorithm, combining various ideas and tools into one. I am not yet at the stage to understand this though, but hopefully it will be discussed later in the course. . Experimentation . The input to the model is a string containing a movie review. The output is a probability/score between 0 and 1 indicating how much the model thinks the review is positive. My initial thought was to to see if the model had learnt about numbers, by seeing how it rates reviews of the form ‘x out of y’. . ‘x out of 5’ . The results of this experimentation: . 0 out of 5: 0.5570 1 out of 5: 0.4760 2 out of 5: 0.3281 3 out of 5: 0.4038 4 out of 5: 0.4073 5 out of 5: 0.8569 . This was surprising to me: . The model thinks that the review ‘0 out of 5’ is likely a positive review | The model is roughly undecided about ‘1 out of 5’ | The model thinks 2,3 or 4 out of 5 is negative | The model thinksthat ‘5 out of 5’ is positive. | . As with most of these ML models, this must be a reflection of the underlying dataset. I imagine very few people actually give a movie 0 or 1 out of 5, so the model doesn’t really know what to do with it, whereas ‘2 out of 5’ probably is more common and basically says the movie is bad, and ‘3 out of 5’ and ‘4 out of 5’ are more average-y ratings. . ‘x out of 10’ . The results of this: . 0 out of 10: 0.5402 1 out of 10: 0.4478 2 out of 10: 0.2972 3 out of 10: 0.3741 4 out of 10: 0.3798 5 out of 10: 0.8475 6 out of 10: 0.9897 7 out of 10: 0.99996 8 out of 10: 0.99991 9 out of 10: 0.9998 10 out of 10:0.99996 . Again the model is confused with the worst reviews, presumably because they just don’t occur in the training data. Then the model seems to deal with 2,3 and 4 out of 10 sensibly. But then it has big jump and thinks the rest are definitely positive, with anything above 7 out of 10 being considered positive with high confidence. Again, the explanation for these unusual evaluations of the reviews must be that they are uncommon in the dataset. . Variations on ‘x out of 5’ . I tried a few little variations on ‘x out of 5’ to see if it would make any impact: adding an exclamation mark at the end, adding the word ‘stars’ at the end, and adding the word ‘Only’ at the start. . x ‘x out of 5’ ‘x out of 5!’ ‘x out of 5 stars’ ‘Only x out of 5’ . 0 | 0.5570 | 0.7436 | 0.7247 | 0.0240 | . 1 | 0.4760 | 0.6756 | 0.6538 | 0.0247 | . 2 | 0.3281 | 0.4901 | 0.4884 | 0.0196 | . 3 | 0.4038 | 0.5670 | 0.5634 | 0.0189 | . 4 | 0.4073 | 0.5634 | 0.5589 | 0.0179 | . 5 | 0.8569 | 0.9051 | 0.8969 | 0.5464 | . Adding an exclamation mark makes the model think the reviews are more positive, which in my opinion is sensible. | Adding the word ‘stars’ seems to have very similar effect to adding an exclamation mark. I do not know if this is just a coincidence - I do not have intuition for this. I guess it must mean that if the word ‘star’ appeared in the training data, then it mostly meant it was a positive review. | Adding the word ‘only’ dramatically reduced the positivity score, which makes sense. | . Miscellaneous statements . Lastly, I tried a variety of miscellaneous statements: . I liked some bits and I disliked other bits 0.9345 I disliked some bits and I liked other bits 0.8857 I liked most of it, but not all of it 0.9906 I disliked most of it, but not all of it 0.9782 I hated most of it, but not all of it 0.9486 I disliked the movie 0.7621 I hated the movie 0.6873 I cannot believe how bad the movie is 0.5535 . The model is comically bad at judging whether the statement is positive. I started by trying a neutral statement, but the model thought it was great. I then tried the favourable statement ‘I liked most of it…’ and the model thought it was positive (sensible), but still thought the statement was positive when I switched liked and disliked. Using the word ‘hated’ instead reduced it a bit. . Maybe the model hasn’t even learnt the word ‘disliked’ and ‘hated’. So I just tried the simple statements ‘I dislike/hated the movie’, and the model does reduce its score, but still thinks these are overall positive. I end with a particularly strong negative review, which the model perceives as slightly positive. . Conclusions . It is nice to play around with a model without any expectation to understand the inner-intricacies or working. My main takeaways are: . The model has learnt some sensible patterns, e.g. adding ‘only’ indicates a negative review, adding an exclamation mark indicates a positive review, etc. | The model fails spectacularly badly on several examples, which do not feel contrived to me. This makes me question the reliability of sentiment analysis. Maybe the examples I created are not representative of real-world examples. | If I were a decision-maker in some business, I would definitely be using manual analysis until I see better evidence. | .",
            "url": "https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/30/fastai2.html",
            "relUrl": "/data%20science/neural%20network/python/2020/08/30/fastai2.html",
            "date": " • Aug 30, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Increasing the resolution of an image using an SRGAN",
            "content": "Introduction . I wanted to print out an image for a friend, and I wanted to first crop the image to only include a relatively small part of the photo (namely, the individuals in the photo. The original photo was 90% scenary). However, after the cropping, the image became too small and would have been pixelated when printed. I thought that a neural network ought to be able to solve this problem, because it could be trained on a manually created dataset (just take a bunch of images and use these as the outputs, then reduce the quality of these images to create the inputs). I decided to Google around to see what already had been done, and I found out about SRGANs. At my current level of knowledge, I cannot understand the details of the architecture, so for now, my aim was just to see if I could use a pre-trained network to achieve my goal. After a bit more searching, I found this Github repo. . Steps taken . Create a new directory and environment. | Download the github repo. | Install the requirements. | Create a directory inputs for input images, and folder outputs for where the output images will go. | Add images to the inputs directory. | Run the command `python infer.py –image_dir ‘inputs’ –output_dir ‘outputs’ | . Results . Below are samples of the images, comparing before and after applying the neural network. . Example 1 . . Example 2 . . Example 3 . . This is amazing! This feels like magic to me. . Limits . I tried applying this to blurry images, expecting it to make the images less blurry. This did not happen: . . This reveals how limited my understanding is! After thinking and googling a bit, I think the issue is that resolution and blurriness/sharpness are two separate issues. Previously, I just lumped them together as ‘reasons an image can look bad’. After a bit more searching, it seems I might learn about anti-blurring / sharpening in the FastAI course that I am doing, so that is something to look forward to. . Finally, I hope I will soon be at at stage where I can understand these intricate architectures, and be able to train my own cutting edge neural networks. .",
            "url": "https://lovkush-a.github.io/blog/python/neural%20network/2020/08/28/srgan1.html",
            "relUrl": "/python/neural%20network/2020/08/28/srgan1.html",
            "date": " • Aug 28, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Analysing the movies I've watched, Part IV, Data visualisation",
            "content": "Other posts in series . Analysing the movies I’ve watched, Part V, Data visualisation II . | Analysing the movies I’ve watched, Part III, Joining the tables . | Analysing the movies I’ve watched, Part II, Data cleaning . | Analysing the movies I’ve watched, Part I, Data collection . | . Movies per year . . The most striking thing is how many movies I watched in 2008 and 2009! More than an average of 2 per week. The uptick in 2016 and 2017 can be explained by my mum buying me an Odeon Unlimited card in September 2016. The slight rise in 2018 and 2019 compared to mid 2010s I think is explained by moving into a house where watching movies was a common social activity for the housemates. And 2020 is significantly above average (given it is not complete), and this is explained by the covid-19 pandemic. . Movies by source . . The majority of my movie-watching experience is done on Netflix. To help see any other patterns, I tried grouping them together: . . Not sure if there is anything noteworthy to say about this. Note that the category ‘internet’ may or may not refer to streaming from dodgy websites. . Movies by source and year . . Stackplots are awesome! Visually striking, and provides an overall sense of how my movie watching habits changed. Some patterns which are clear from this diagram: . There is a clear decline in the use of ‘the internet’ to watch movies in 2017. | There is a clear rise in the use of Netflix, starting from around 2014. | There is a big bulge in cinema viewings in 2016 and 2017, corresponding to when I had an Odeon Unlimited card. | There is a big pink bulge, for a surge in ‘other online’ activity. A quick search in the dataframe shows this corresponds to me making a full use of free trials of NowTV. | . Movies by month . . Again, there are some patterns visible in this plot: . There are generally peaks in December, the holiday period! Looks like I enjoy watching movies when I am back home during the Christmas break. | There are smaller peaks in several summer times, also likely due to me watching movies while at home. | You can see some peaks at the end of 2016 and start of 2017, corresponding to the odeon unlimited card. | You can see the increase in movie watching from March 2020, corresponding to covid-19. | . I recently contributed to an open source project Darts, which does Time Series predictions. I was curious to see what patterns it would find. The following is obtained via an exponential smoothing: . . The clearest pattern in the model’s prediction is that it predicts peaks in December. Given how small and error-filled the dataset is, I do not think there is much to read in the smaller peaks at other times of the year. . Conclusions . Even with a dataset as noisy as this one, it is still possible to obtain some nice visuals and uncover some overall patterns. My favourite chart is the stackplot showing how the source used to watch movies has changed over the years. . Another nice thing about this project has been that it included various firsts for me: first stackplot, first time series model (admittedly basic), first pivotting in pandas (to create stack chart), first time grappling with Time formats to create precisely the plot I want (for the plot by month). . Next time, I will see what patterns there in the subset of data for which I could join it with imdb data. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/08/24/mymovies4.html",
            "relUrl": "/python/data%20science/2020/08/24/mymovies4.html",
            "date": " • Aug 24, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Analysing the movies I've watched, Part III, Joining the tables",
            "content": "Other posts in series . Analysing the movies I’ve watched, Part V, Data visualisation II . | Analysing the movies I’ve watched, Part IV, Data visualisation . | Analysing the movies I’ve watched, Part II, Data cleaning . | Analysing the movies I’ve watched, Part I, Data collection . | . Introduction . In the last post, I described the cleaning that I did, In particular, I had to manually correct over 140 titles in my data that did not occur in the imdb dataset. So I should be ready to join, right? No, of course not, why would things be so simple. . The main problem . Many movie titles occur multiple times in the imdb database, and as far as I know, there is not an automated way to deal with this. To measure the extend of the problem, I added a column to count how many matches there are in the imdb database: . def num_rows_in_imdb(movie_title, print_rows = False): &quot;&quot;&quot; returns the number of rows in the imdb dataset that have primaryTitle equal to movie_title. if print_rows is True, then also print the rows &quot;&quot;&quot; temp = imdb[imdb.primaryTitle == movie_title] num_rows = temp.shape[0] if print_rows: print(temp) return num_rows df[&quot;rows_in_imdb&quot;] = np.nan df[&#39;rows_in_imdb&#39;] = df.movie_title.map(num_rows_in_imdb) . Doing this revealed that most of the entries had multiple occuences in the imdb database! This was not good. I looked into a couple of examples by searching on the imdb website. It looked like many of the repetitions were from matches with TV episodes. So I repeated the above process but with only the movies from the imdb dataset: . imdb = pd.read_csv(&#39;title.basics.tsv&#39;, sep=&quot; t&quot;, na_values = &#39; N&#39;, usecols = [&#39;tconst&#39;, &#39;primaryTitle&#39;, &#39;startYear&#39;, &#39;titleType&#39;]) imdb = imdb[imdb.titleType.isin([&#39;short&#39;, &#39;movie&#39;, &#39;tvMovie&#39;])] . By removing all the TV episodes, the number of matches decreased, but we still have a large problem. Only 351 of my entries have precisely one match. That is a low number indeed. . Conclusion . I have decided to leave the dataset in the mess that it is and move onto the analysis stage. I will look for patterns within my dataframe first, and then look at those 351 entries which should be able to join with the imdb dataset for any patterns there. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/08/19/mymovies3.html",
            "relUrl": "/python/data%20science/2020/08/19/mymovies3.html",
            "date": " • Aug 19, 2020"
        }
        
    
  
    
        ,"post10": {
            "title": "DynamoDB Part I, the CAP Theorem's never ending rabbit hole",
            "content": "Other posts in series . Introduction . I recently started this EdEx course on Amazon’s DynamoDB service. Within one of the first few videos, the CAP Theorem was discussed. As someone with a pure maths background, theorems are irresistable bait, so I wanted to find out more about it. . My journey . I cannot remember all the details, but here are some of the things I have found and thought in my trip down the rabbit hole. . The wikipedia page was the starting point, of course. I couldn’t say it really helped me, as most of the terms described were not helpful. | However, when looking at the references, something stood out. One of my friend’s brothers, Martin Kleppmann, has written a bunch of stuff on this! | This blogpost on why the CAP framework is practically unhelpful, was helpful. It gave an intuitive proof of the theorem. However, the proof is so simple that I wonder what the big deal is. There is clearly something I am missing. | The blogpost contained various links. At this stage, I lose track of things and which links came from where. | A couple of other helpful blogposts were this and this. I gained a better sense of why this is an important issue, and what underlying problems are that people are thinking about. | One thing that is frustrating is how many terms people take for granted. E.g. the concepts of nodes and network are taken as self-evident, yet they are fundamental to the statement of the theorem. They cannot just be the same thing as nodes and networks from Graph Theory, because stuff is actually happening within the nodes, but none of the blog posts I found actually explain these concepts. | There is whole bunch of things I found in this journey that look interesting and I intend to read at some point: This paper on how certain multiprocessors do not provide sequentially consistent memory, and their proposed solution. | These lecture notes on queueing theory and its application to computer systems. | This article by Microsoft explaining data consistency in databases using baseball. | This book on ‘The big ideas behind reliable, scalable, maintainable data-intensive systems’. Seems to be well-regarded by many people. | This blogpost containing a list of highly recommended essays on programming. | This book ‘Explain the Cloud Like I’m 10’ looks like an easy to read book where I will learn a lot about what really goes on behind the scenes. | . | . Conclusions . There is too much to learn! One thing I still need to improve in myself is my ability to prioritise all these difference things. .",
            "url": "https://lovkush-a.github.io/blog/data%20science/data%20engineering/2020/08/14/dynamodb1.html",
            "relUrl": "/data%20science/data%20engineering/2020/08/14/dynamodb1.html",
            "date": " • Aug 14, 2020"
        }
        
    
  
    
        ,"post11": {
            "title": "FastAI Course, Part I, Lessons 1 and 2",
            "content": "Other posts in series . FastAI Course, Part III, Frustrations with creating an image classifier . | FastAI Course, Part II, Lesson 1 and sentiment analysis . | . First impressions . I recently started this FastAI Deep Learning course and done a couple of little projects. Below I summarise some of what I did and my thoughts. . I am impressed by how the course is taught. It is clear a lot of thought has been put into how to teach the course, which examples to use, what messages/advice to give to students, what documentation to provide, etc. | I quite like how they have a system for the live audience to submit questions, which the lecturer Jeremy Howard then answers during the lecture. | I like how Jeremy showcases various examples of what students have done. It is surprising to see how many state-of-the-art things can be done with just the knowledge given in the first couple of lectures. The FastAI team should be chuffed with themselves for creating such a course, tool and community. | I tried a couple of image classification tasks. The first was to distinguish between the Dragonball Z characters Goku and Vegeta. The second was to distinguish between squash and tennis rackets. I got ok results. At this stage, the main way to improve the algorithm is to improve the image set (I just downloaded the first bunch of images from Google Image Search). | This was slowed down a bit, because the first online GPU provider I used was paperspace, but there was some bug which meant I could not re-open a notebook after using it once. I then tried Google Colab, which took a bit more time to learn, and got things working there. In the meantime, the bug in paperspace was fixed, so I will use that again. | .",
            "url": "https://lovkush-a.github.io/blog/data%20science/neural%20network/python/2020/08/09/fastai1.html",
            "relUrl": "/data%20science/neural%20network/python/2020/08/09/fastai1.html",
            "date": " • Aug 9, 2020"
        }
        
    
  
    
        ,"post12": {
            "title": "Web Scraping for STEP past papers and solutions, Part II, a bug",
            "content": "Other posts in series . Web Scraping for STEP past papers and solutions | . A bug . Earlier this week, I tried to open one of the files containing a solution to a past STEP paper. To my surprise, it did not open. I tried several other files, and none of them opened. A big lesson here is to actually check the files have properly downloaded! I just assumed that the files successfully downloaded because I saw the correct filenames appear in the directory. (This is somewhat ironic because one of my takeaways at the end of the previous post was to not make assumptions…) . So I had to go back to my previous attempt and try to work out what went wrong. I compared my code to various other examples online, and I could not see the error. I then tried several other variations that I found while searching, and none of them worked. . Next I tried to look at the object obtained after running r = requests.get(url). I first ran print(r.content) to actually see what I was writing to the files. The output was html for a webpage - that made no sense, since the url directly goes to a jpg file. I was feeling a bit clueless here. I used dir(r) to see if there were any functions that might help me, but none stood out. One of the methods was url and in desperation I decided to do run print(r.url), expecting just to the url that was inputted into the .get method. However, the output was: . https://2017.integralmaths.org/login/index.php . Bingo! The problem was now clear. The requests function is distinct from the selenium objects, so you have to separately log-in to the website for requests. After some Googling, I found out how you can enter log-in credentials using requests, and it worked! What a relief. The new bits of code are given at the end. . Lessons learnt . As mentioned above, I really need to absorb the lesson that one should not make assumptions. | If you’re completely stuck, it might be worthwhile to go through all the methods of the object, to see what can be uncovered. | There is a limit to my understanding of the code and the modules. Stitching together code from random blogs and stackoverflow works, but I have no real understanding. I did try looking at the documentation for some of the modules I was using, but they are incomprehensibly dense. I am not sure what best practice is here. | Try to have as much of your work saved, so if there is a bug, you do not need to repeat everything. In this case, I should have saved a list of the urls during my first attempt, so that for any potential future attempts, all I would have to do is loop through these urls and download the images, rather than having to navigate via a browser to find all the urls again. | . Code . Below is the new bits of code I had to add into the original code. It may be incomprehensible without the context, but I think the syntax is mostly self-explanatory. . login_url = &quot;https://2017.integralmaths.org/login/index.php&quot; payload = { &#39;username&#39;: &#39;mei-step&#39;, &#39;password&#39;: &#39;Stepaea1&#39; } with requests.Session() as s: s.post(login_url, data=payload) r = s.get(url, allow_redirects=True, stream=True) open(filename, &#39;wb&#39;).write(r.content) .",
            "url": "https://lovkush-a.github.io/blog/python/scraping/2020/08/06/downloadstep2.html",
            "relUrl": "/python/scraping/2020/08/06/downloadstep2.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post13": {
            "title": "Analysing the movies I've watched, Part II, Data cleaning",
            "content": "Other posts in series . Analysing the movies I’ve watched, Part V, Data visualisation II . | Analysing the movies I’ve watched, Part IV, Data visualisation . | Analysing the movies I’ve watched, Part III, Joining the tables . | Analysing the movies I’ve watched, Part I, Data collection . | . Fixing bad indexing . One of the columns is an index to show how many movies I have watched since I started the record. This was mostly accurate, except for reasons unknown to me, the latest records went through the indices 670 to 679 a few times; instead of an index changing from 679 to 680, I would go back to 670. So, I wrote some code to fix this. . Creating datetime column . One of the columns has year, and another column has date formatted as a string “DD Mon” (e.g. 1 Jan). I wanted to combine these to create a column with datetime type, and “yyyy-mm-dd” format. I first tidied the month column (some months had multiple entries, e.g. Jun and June), and sometimes other data was in the month column (e.g. #recc), so I fixed those. Then I tidied the date column - I padded out the digits so they were always two strings long. Once it was all tidy, I created the new column: . df[&#39;datetime&#39;] = pd.to_datetime(df.year.astype(str)+&#39; &#39;+df.month+&#39; &#39;+df.date, errors = &#39;coerce&#39;, format = &#39;%Y %b %d&#39;) . Tidying the source column . There is a column called “source”, which contains information on where I watched the movie, e.g. Netflix, cinema, TV, etc. Tidying this required a lot of manual work, because there were various different sources that I wanted to deal with in different ways. E.g. sometimes the source was the name of a friend (e.g. I watched a movie at their house), and so I wanted to replace source with friend, and move the friend’s name to the Other column. In retrospect, I could have made the process a bit more automated. But what is done is done, I suppose. . Tidying the other column . There is a column called “other” to contain miscellaneous information. This required no cleaning, which is nice! . Tidying the title column . I anticipate joining this table with an imdb dataset, so I can get information like ratings, director, movie length, that might be fun to include in my analysis. I downloaded the title.basics.tsv dataset, then read it into a dataframe, keeping only the columns tconst, primaryTitle and startYear. I converted all the primaryTitles to lowercase. . Next I created a column in my dataframe to say whether the title in my records occurs in the imdb dataset. . imdb_titles = set(imdb.primaryTitle.values) df[&#39;title_in_imdb&#39;] = df.movie_title.map(lambda x: x.lower() in imdb_titles) . Doing a little check reveals that 549 of the entries are fine, but 147 are not. So, I bite the bullet and go into another batch of manual corrections. I anticipate their being various reasons why the title would not work, so try to make the process as flexible as possible. I create the following function to make it easy to update a specific row, then loop through the values which were not in the imdb database. . def update_row(index): new_title = input(&quot;title: &quot;) df.loc[index, &#39;movie_title&#39;] = new_title new_other = input(&quot;other: &quot;) if new_other != &quot;&quot;: df.loc[index, &#39;other&#39;] = new_other return for index, row in df.iterrows(): if row[-1]: continue else: print(f&#39;{index}. {row[1]}; {row[-3]}&#39;) update_row(index) df.to_csv(&#39;third_tidy.csv&#39;, sep=&quot; t&quot;) print(&quot;&quot;) print(&quot;&quot;) . Conclusion . This was a considerable effort, with a lot of manual grinding. I am curious to know what would happen in a commerical context with millions of entries, with a similar hodge-podge of bad data. I presume there must be more efficient ways of tidying, but maybe you really do just have to get your hands dirty if the data is messy enough. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/08/03/mymovies2.html",
            "relUrl": "/python/data%20science/2020/08/03/mymovies2.html",
            "date": " • Aug 3, 2020"
        }
        
    
  
    
        ,"post14": {
            "title": "Analysing the movies I've watched, Part I, Data collection",
            "content": "Other posts in series . Analysing the movies I’ve watched, Part V, Data visualisation II . | Analysing the movies I’ve watched, Part IV, Data visualisation . | Analysing the movies I’ve watched, Part III, Joining the tables . | Analysing the movies I’ve watched, Part II, Data cleaning . | . Introduction . Since 2008, I have been maintaining a record of all the movies I have watched. It started because I borrowed a CD stack (see here if you are too young to not know what a CD stack is) of movies from a friend, and the easiest way of keeping track of which movies I had watched was to create a list. Once I finished the movies on the CD stack, I thought I may as well continue adding to the list, and it has been going ever since. Now I figured this could be a fun data set to analyse. . The dataset . The list started out as a notepad .txt file, then I moved it into a Google spreadsheet, and it now resides in Dynalist. The format of the entries in the list has changed over time. . The earliest entries in the list have the format movie_title, i, where i is an index to keep track of how many films are in the list. | In Easter 2009, I started including the date, so the standard format is movie_title, date, i. The date is the day of the month and the month. | In September 2009, I started including the source / location of the movie, e.g. a cinema, or a DVD, or a friend, etc. The standard format is movie_title, date, source, i. This has been the format ever since. | I keep track of movies I would recommend. This is recorded by including #recc between movie_title and whatever the second item of the entry is. | I keep track of year by having an entry like this: 2020 above, 2019 below | . The first task was to get this dataset into a pandas dataframe. It should not be too difficult - I have a few formats that can be easily identified, and I can create a new row as appropriate. . Collecting the data . What I was not conscious of was how many entries did not fit into the formats described above, roughly 150 or so. To my frustration, there is no simple pattern to them either so I had to manually sort these out. To illustrate, here are just a small sample of the exceptions: . Having ?? for the date, or just leaving out the date altogether | Including extra details, e.g. whether I had watched the whole movie or not, or a description of the movie to help me remember what the movie is, or, inclusion of which city I was in. These extra details were not in consistent locations within the entry. | Sometimes the movie title contained a comma in them, which messed things up because I was using splitting each entry by a comma. | . The result is that the process took much longer than I anticipated. I can better understand now how data processing can be the biggest part of a data scientist’s role! One thing I am curious about is whether there are better alternatives to doing manual work. If there were millions of entries, what could I do? . The code . Here is the code. Many details will not make sense without knowing exactly how the data is stored in dynalist. Hopefully the flow of the process is clear. . # read text file from dynalist with open(&quot;Regular.txt&quot;, &#39;r&#39;) as f: lines = f.readlines() # extract relevant rows from lines, by finding the start and end indices # store these rows into variable movies, separated by commas i = 0 while &#39; tmovies&#39; not in lines[i]: i += 1 i += 2 while &#39; t t t&#39; in lines[i]: i += 1 start = i while &#39; t t&#39; in lines[i]: i += 1 end = i movies = [re.split(&#39; s*, s*&#39;,line[2:-1]) for line in lines[start:end]] # add the year to end of each entry in movies year = 2020 for i in range(len(movies)): if &#39;&#39; not in movies[i][0]: movies[i].append(year) elif &#39;&#39; in movies[i][0]: year -= 1 # delete year separator entries, those with for i in range(len(movies)-1,-1,-1): if &#39;&#39; in movies[i][0]: del(movies[i]) # define date regex pattern date_pattern = r&quot; d{1,2} w+&quot; # define function to determine the format of the row def check_format(row): dates = [ True if re.match(date_pattern, item) else False for item in row[:-1]] recc = [item == &#39;#recc&#39; for item in row] digits = [item.isdigit() for item in row[:-1]] length = len(row) # first format is movie_title, date, source, my_index, year if length == 5: if all([dates[1], digits[3]]): return 1 # second format is movie_title, date, my_index, year if length == 4: if all([dates[1], digits[2]]): return 2 # third format is movie_title, recc, date, source, my_index, year if length == 6: if all([recc[1], dates[2], digits[4]]): return 3 # fourth format is movie_title, my_index, year if length == 3: if digits[1]: return 4 # fifth format is movie_title, source, my_index, year if length == 4: if digits[2]: return 5 # sixth format is movie_title, recc, source, my_index, year if length == 5: if all([recc[1], digits[3]]): return 6 return None # define function to obtain manual inputs from me, for those entries that do not match # the standard format def get_input(col, row): raw = input(col+&quot;: &quot;) if raw == &quot;&quot;: return &quot;&quot; elif raw.isdigit(): return row[int(raw)] else: return raw # create list of column names # create variable df_rows that will contain data that will be turned into frame columns = [&#39;my_index&#39;,&#39;movie_title&#39;,&#39;year&#39;,&#39;date&#39;,&#39;source&#39;,&#39;recommended&#39;,&#39;other&#39;] df_rows = [] # loop through entries in movies, and create appropriate row data for i, row in enumerate(movies): df_row = {col: np.nan for col in columns } df_row.update(year = row[-1]) row_format = check_format(row) # first format is movie_title, date, source, my_index, year if row_format == 1: df_row.update(movie_title = row[0]) df_row.update(date = row[1]) df_row.update(source = row[2].lower()) df_row.update(my_index = int(row[3])) df_row.update(recommended = False) # second format is movie_title, date, my_index, year elif row_format == 2: df_row.update(movie_title = row[0]) df_row.update(date = row[1]) df_row.update(my_index = int(row[2])) df_row.update(recommended = False) # third format is movie_title, recc, date, source, my_index, year elif row_format == 3: df_row.update(movie_title = row[0]) df_row.update(recommended = True) df_row.update(date = row[2]) df_row.update(source = row[3].lower()) df_row.update(my_index = int(row[4])) # fourth format is movie_title, my_index, year elif row_format == 4: df_row.update(movie_title = row[0]) df_row.update(my_index = int(row[1])) df_row.update(recommended = False) # fifth format is movie_title, source, my_index, year elif row_format == 5: df_row.update(movie_title = row[0]) df_row.update(source = row[1].lower()) df_row.update(my_index = int(row[2])) df_row.update(recommended = False) # sixth format is movie_title, recc, source, my_index, year elif row_format == 6: df_row.update(movie_title = row[0]) df_row.update(recommended = True) df_row.update(source = row[2].lower()) df_row.update(my_index = int(row[3])) elif (row_format is None): for i,item in enumerate(row[:-1]): print(f&#39;{i}: {item}&#39;) print(&quot;&quot;) df_row.update(movie_title = get_input(&#39;movie_title&#39;, row)) df_row.update(date = get_input(&#39;date&#39;, row)) df_row.update(source = get_input(&#39;source&#39;, row).lower()) df_row.update(my_index = int(get_input(&#39;my_index&#39;, row))) df_row.update(recommended = bool(get_input(&#39;recommended&#39;, row))) df_row.update(other = get_input(&#39;other&#39;, row)) print(&quot;&quot;) df_rows.append(df_row) # create dataframe from data, and save as csv file df = pd.DataFrame(df_rows) df.to_csv(&#39;mymovies1.csv&#39;) .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/08/02/mymovies1.html",
            "relUrl": "/python/data%20science/2020/08/02/mymovies1.html",
            "date": " • Aug 2, 2020"
        }
        
    
  
    
        ,"post15": {
            "title": "First ever open source contribution",
            "content": "Introduction . A few days ago, I successfully completed my first ever open source contribution. :) In this article, I will briefly describe the experience, and some little things I learnt. . Highlights . I contributed to the relatively new package darts by Unit8. It is a package that makes TimeSeries analysis easy. It does not create any new models, but just creates a TimeSeries class which uses the fit and predict syntax from scikitlearn to carry out analyses imported from other packages. . I went to the issues tab and saw an issue that seemed accessible - implementing a map method for the Time Series object. Next I read the contribution guidelines, to make sure I follow the right steps and conventions. Next I forked the repository and then pulled it to my local machine. After this, I spent some time getting familiar with the package, and working out how the TimeSeries class works. Next I wrote the code for the map function. I would refer to other bits of the code to make sure I was using consistent conventions and wording (e.g. wording for the doc text). Then I tested the code by creating a Jupyter Notebook, and seeing that the function does what was intended. Once it all looked fine, I then pushed the changes to my forked copy of the repository, and then made a pull request. . It was a bit nerve-racking, because there are so many little things to get correct to successfully contribute to a project. Shortly after, I got some feedback, and fortunately it was positive! Phew. They liked how the function was implemented, but had various minor comments. I made the changes, pushed them to my forked copy and was about to make another pull request. But then I noticed that the original pull request automatically absorbs any new commits - that is a neat feature! . After a couple of other minor comments and changes, the owners of the project were happy and merged the changes into the main code. Woohoo! My first ever contribution is done. . The code . In case you care, here is the code I wrote for the map function: . def map(self, fn: Callable[[np.number], np.number], cols: Optional[Union[List[int], int]] = None) -&gt; &#39;TimeSeries&#39;: &quot;&quot;&quot; Applies the function `fn` elementwise to all values in this TimeSeries, or, to only those values in the columns specified by the optional argument `cols`. Returns a new TimeSeries instance. Parameters - fn A numerical function cols Optionally, an integer or list of integers specifying the column(s) onto which fn should be applied Returns - TimeSeries A new TimeSeries instance &quot;&quot;&quot; if cols is None: new_dataframe = self._df.applymap(fn) else: if isinstance(cols, int): cols = [cols] raise_if_not(all([0 &lt;= index and index &lt; self.width for index in cols]), &#39;The indices in `cols` must be between 0 and the number of components of the current &#39; &#39;TimeSeries instance - 1, {}&#39;.format(self.width - 1), logger) new_dataframe = self.pd_dataframe() new_dataframe[cols] = new_dataframe[cols].applymap(fn) return TimeSeries(new_dataframe, self.freq_str()) .",
            "url": "https://lovkush-a.github.io/blog/github/python/2020/07/30/firstopen.html",
            "relUrl": "/github/python/2020/07/30/firstopen.html",
            "date": " • Jul 30, 2020"
        }
        
    
  
    
        ,"post16": {
            "title": "Web Scraping for STEP past papers and solutions",
            "content": "Introduction . Though I am transitioning from teaching to data science, I still continue to teach via private tuition. I am currently helping somebody prepare for the STEP, so I wanted to obtain as many past papers and solutions as I could. To do this manually would have required hours of clicking on links and ‘Saving as…’, so I decided to automate it with python. . Downloading the step papers . This was straightforward. The website stepdatabase.com has past papers on them, and the urls and naming system they use is systematic and 100% consistent. I was able to download the papers using a simple loop with a wget command. See the code below. . Downloading the answers . This was less straightforward. This website provides answers to the STEP papers but the urls did not have a consistent format, so I could not just use wget again. Therefore, I used some web-scraping tools to help me. . There was a considerable learning curve, as I had not done any web scraping before. There were actually a couple of moments where I was going to give up. However, I persisted in the knowledge that if I want to be successful in a tech role, I will encounter such difficulties, and the only way to improve is to perservere. . In the end, I succeeded and the final code is given below. I will not go through the full process of trying things out, failing, tweaking, de-bugging, etc., but I will provide some of the key learning points for me. . BeautifulSoup is only suitable for statically generated pages. For dynamic ones, you can use Selenium. | The dir function in python is extremely handy. I have to thank this Kaggle tutorial for introducing me to this function. On two or three occasions, I wanted to do something, and by looking at the list of methods, I was able to find one which worked. The example I remember is find_elements_by_partial_link_text. | StackOverFlow is extremely handy. I am not sure what I would have done without it. | Don’t make assumptions. I assumed that the answers would all have different file names, but that was not always the case. This meant that previously downloaded were sometimes over-written by later answers. Fortunately, the was apparent in the first few minutes of running the program, so I could stop the program, and quickly modify it to add my own naming convention. | . Code . The code to download the past papers is: . import subprocess for i in range(87,119): for j in range(1,4): i = i%100 url = f&quot;https://stepdatabase.maths.org/database/db/{i:02}/{i:02}-S{j}.pdf&quot; subprocess.run([&quot;wget&quot;, url]) for i in range(87,119): for j in range(1,4): i = i%100 url = f&quot;https://stepdatabase.maths.org/database/db/{i:02}/{i:02}-S{j}.tex&quot; subprocess.run([&quot;wget&quot;, url]) . The (ugly) code I created to download the solutions is: . from bs4 import BeautifulSoup from selenium import webdriver import requests import re # open browser and go to the url browser = webdriver.Chrome() URL = &#39;https://mei.org.uk/step-aea-solutions&#39; browser.get(URL) # click button, which opens new tab, so move to new tab browser.find_element_by_xpath(&#39;//button[text()=&quot;STEP Solutions&quot;]&#39;).click() browser.switch_to_window(browser.window_handles[1]) # click on link to go to next page browser.find_element_by_link_text(&#39;STEP past paper worked solutions&#39;).click() # obtain list of links in this page # each of these refers to a group of step papers, e.g. STEP I, 2016-2019 elements = browser.find_elements_by_partial_link_text(&#39;STEP solutions&#39;) groups = [] for element in elements: groups.append(element.get_attribute(&quot;href&quot;)) # define regex pattern to help identify correct links pattern = r&quot;STEP (1|I)+: d d d d&quot; # loop through links in groups for url_group in groups: # open the link browser.get(url_group) # obtain list of links and names of papers in this group. # this requires the regex pattern from above papers = [] for paper in browser.find_elements_by_partial_link_text(&#39;STEP &#39;): if re.match(pattern,paper.get_property(&#39;textContent&#39;)): papers.append([paper.get_attribute(&quot;href&quot;), paper.get_property(&#39;textContent&#39;)]) # loop through the list of papers for url_paper, paper_name in papers: # open link for an individual paper browser.get(url_paper) # obtain list of links for answers to individual questions questions = browser.find_elements_by_partial_link_text(&#39;Question &#39;) # loop through the questions for question in questions: # open link to answer for individual question question.click() browser.switch_to_window(browser.window_handles[2]) # download image # note that the url you end on is different to the url you use to get to this page url = browser.current_url filename = paper_name+&#39;-&#39;+url.split(&#39;/&#39;)[-1] r = requests.get(url, allow_redirects=True) open(filename, &#39;wb&#39;).write(r.content) # close browser and switch back to page with list of questions browser.close() browser.switch_to_window(browser.window_handles[1]) .",
            "url": "https://lovkush-a.github.io/blog/python/scraping/2020/07/27/downloadstep.html",
            "relUrl": "/python/scraping/2020/07/27/downloadstep.html",
            "date": " • Jul 27, 2020"
        }
        
    
  
    
        ,"post17": {
            "title": "EuroPython Conference 2020, Summary",
            "content": "Other posts in series . EuroPython Conference 2020, Day 2 . | EuroPython Conference 2020, Day 1 . | . Introduction . I attended the online conference EuroPython 2020 and recorded notes while watching the talks. Today, I reviewed the notes with the aim of consolidating the main lessons, grouping together similar talks, and recording key lessons. . Workflows . This seems to be a big theme, and sounds like the next big challenge for the data scientist profession. . GitLab Tools, William Arias . I found this talk hard to follow, so my notes are not great. | . Parallel stream processing, Alejandro Saucedo . I found this talk hard to follow, but notes are bit better than the example above | Faust - stream processor | Kafka - ? | Seldon - deployment | Example of developing workflow to use ML to help moderate reddit comments. | . Example workflow for Translation, Shreya Khurana . I found this talk hard to follow, but still managed to take some notes. | Tools used include: seq2seq, Fairseq, Flask, uwsgi, nginx, supervisord, Docker | . Example from teikametrics, Chase Stevens . Significant time spent emphasising need for good systems and workflows. Make the effort to automate! | My notes are not great when it comes to actually describing the tools or workflow they used. Watch the video to find out. | Interesting example at end: they automated task of choosing which AWS instance to use. | . DVC, Hongjoo Lee . Software development has following standard workflow and tools (I don’t know what most of these things mean…): Coding management: git | Build: sbt maven | Test: jUnit | Release: Jenkins | Deploy: Docker, AWS | Operate: Kubernetes | Monitor: ELK stack | . | Hongjoo goes through live example of using DVC on simple model | Some alternatives to DVC: git-LFS, MLflow, Apache Airflow | . Data pipelines, Robson Junior . I found the speaker particularly hard to follow. Despite this, it looks like I got some decent notes. | List of tools for different tasks | ELT: Apache Spark, dash, luigi, mrjob, ray | Streaming: faust and kafka, streamparse and Apache Storm | Analysis: Pandas, Blaze, Open Mining, Orange, Optimus | Management and scheduling: Apache Airflow | Testing: pytest, mimesis (to create fake data), fake2db, spark-test-base | Validation: Cerberus, schema, voluptuous | . Example from DeepAR talk, Nicola Kuhaupt . Workflow was not focus of talk, but talk contained information on workflows anyway | Integrated into Sagemaker, which has many in built tools Ground Truth. Use mechanical turk to build data sets | Studio. An IDA | Autopilot. For training models | Neo. For deployment | . | boto3, to access aws | s3fs, file storage | Others were also mentioned. Re-watch talk to find them. | . NLPeasy, Philipp Thomann . NLPeasy is intended to make NLP easy | Has lots of built in tools, e.g. spaCy, Vader, BeautifulSoup | The talk presented example of analysing EuroPython abtracts | Had nice dashboard to visualise lots of outputs of models | . Kedro, Tam-Sanh Nguyen . Open source complete workflow package by QuantumBlack | Talk rushed through features of Kedro, but from the brief glimpse, it looks easy to use and has a nice UI. | See examples on GitHub or Tam-Sanh’s YouTube series DataEngineerOne | . Recommended packages / tools . spaCy, Alexander Hendorf . Open source library for NLP | Has many state-of-the-art algorithms built-in | Highly recommended by the speaker | . diffprivlib, Naoise Holohan . Open source library for differential privacy | (For background theory on differential privacy, I recommended this talk from FacultyAI.) | If you want to work with sensitive data, this is a good open source library to consider | . Google’s ML APIs and AutoML, Laurent Picard . Google has a lot of ML tools available | Looks nicely packaged and looks very user-friendly. | Is there even any point in me learning data science?! | . SimPy, Eran Friedman . SimPy can be used to do discrete event simulation | E.g. for robotics training | . Data Visualisation Landscape, Bence Arato . See notes or the talk for brief descriptions of the different tools. | Tools discussed are in image from this slide: | . Binder, Sarah Gibson . For repeatable research (and for teaching/workshops), use Binder | Just have to give Binder link to your GitHub repo which contains a jupyter notebook and a standard requirements configuration file, and then Binder will create link. | You give link to somebody, they go to it, and they can run the jupyter notebook from their browser. Super easy | Binder is open source, so can be configured for your own needs. E.g. can make it so only certain individuals can access the link (or something like that). | Talk contained details of how Binder works, and the tools and infrastructure they use | . IPython, Miki Tebeka . Did live example of using IPython to do some initial experimentation of data and pre-processing style stuff | Magic commands with %, comand line with !, pprint for pretty printing, ? for help, ?? for source code, %timeit for time analysis, can do sql with extension, %cow for ascii art. | See talk for more examples not listed here | . Analytical Functions in SQL, Brendan Tierney . Talk was cancelled, but their slides are available | Looks like SQL can do a lot more than what most of us know | Something worth researching | . Tricks and tools for efficiency/speed gains . Several talks were about this, so I thought it was worth grouping them together . concurrent.futures, Chin Hwee Ong . Built in package in Python for parallel / asynchronous computing | For big data, can use tools like Spark. | For small big data, overhead cost is too large | Using concurrent.futures module can speed things up | . daal4py and SDC by Intel, Fedotova and Schlimbach . These are open source tools that can drastically speed things up. | daal4py gives optimised versions of scikitlearn functions. But still in production. Some functions do not give identical output to their scikitlearn counterparts. | SDC. A just-in-time compiler. Extension of Numba. Easy to use; just add decorate @numba.jit | Only works for statically compilable code | . | Examples of speed ups provided in the talk | . Tips and tricks for efficiency gains in Pandas, Ian Ozsvald . RAM considerations. Use category instead of strings, for low cardinality data | Use float32 or float16 instead of float64 | Has tool dtype_diet to automate optimisation of a dataframe | . | Dropping to NumPy E.g. df.sum() versus df.values.sum() | . | Some tools bottleneck. See talk for example | dtype_diet | ipython_memory_usage | numba, njit wrapper. | Parallelise with Dask. Use profiling to check if benefit outweighs overhead | Vaex and modin. | . | Our own habits Write tests! | . | Lots of other examples in their blog and book | . 30 Rules for Deep Learning Performance, Siddha Ganju . 30 tips and tricks for deep learning in TensforFlow | No point repeating them all here. See notes from talk, or watch talk (or buy their book Practical Deep Learning) | . Miscellaneous . Tips for Docker, Tania Allard. . Expect things to be tricky and frustrating. There are many bad tutorials online | Do not re-invent the wheel: use cookie-cutter, repo2docker | Many other useful tips in the talk | . Data Cleaning Checklist, Hui Zhang Chua . Hui provided a checklist of tasks you should do when cleaning data. | Refer to the notes from the talk for the list | . History of Jupyter Notebooks, William Horton . Does what it says on the tin. I stopped taking notes because I do not anticipate learning the history. Watch the talk if you want to know more. | . Neural Style Transfer and GANs, Anmol Krishan Sachdeva . Anmol described the algorithm for GANs and for neural style transfer | Showed example code in the talk | See notes for the details. It’s pretty clever stuff! | . Probabilistic forecasting with DeepAR, Nicolas Kuhaupt . DeepAR is an algorithm to produce probabilistic time series predictions, by Amazon | One interesting feature is it deals with multiple time series simultaneously | I did not understand details. Would have to read paper to understand | Example uses cases Sales at amazon. Each product has its own time series | Sales of magazines in different stores. Each store has its own time series. | Loads on servers in data centers | Car traffic. Separate time series for each lane | Energy consumption by household. | . | Goes through an example in the talk | .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/conference/2020/07/26/europython3.html",
            "relUrl": "/python/data%20science/conference/2020/07/26/europython3.html",
            "date": " • Jul 26, 2020"
        }
        
    
  
    
        ,"post18": {
            "title": "EuroPython Conference 2020, Day 2",
            "content": "Other posts in series . EuroPython Conference 2020, Summary . | EuroPython Conference 2020, Day 1 . | . Introduction . I am attending the online conference EuroPython 2020, and I thought it would be good to record what my thoughts and the things I learn from the talks. . 7:00, Automating machine learning workflow with DVC, Hongjoo Lee . Notes from talk . Works for SK hynix, a memory chip maker in South Korea. | Waterfall vs agile production. Waterfall = design, then build, then release, then done. Agile, more iterative approach. | Software dev/dev ops: code git, build sbt maven, test jUnit, release Jenkins, deploy Docker aws, operate Kubernetes, monitor ELK stack | ML dev lifecycle: get data, preprocess, build model, optimise model, deployment. Often getting data requires domain expertise | Often software engineers are needed for deployment | Still improvement and modification needed for ML dev workflow. In early stages. | . | Data versioning. Can be terrible, with names like raw_data, cleaned_data, cleaned_data_final. | Need to have system where any change in data will trigger pipeline | . | ML is metric driven. Software engineering is feature driven. ML Models version should be tracked. | Metrics should be versioned/tracked | . | DVC helps. Some laternatives: git-LFS, MLflow, Apache Airflow Easy to use | Language independent | Useful for individuals and for large teams | . | Example code: on github | Hongjoo works through a cats vs dogs example in talk Download template directory | Use git to keep track of changes | Use dvc command-line commands to define each of the steps in ml pipeline | dvc dag command - shows ascii diagram of pipeline | dvc repro - checks if there are changes, and if so, re-runs pipeline. E.g. change model by adding a layer, then dvc repro does everything. you have to manually to git commit command and naming of versions | dvc metric show. shows metrics of all versions done. | . | . My thoughts . Looks simple enough! I could follow the talk. :D It is clear that finding a good ML workflow is a big theme. At the end of the conference, I will have to go through these notes and collate the various tools and workflows people use, so I have a reference for when I need to use it. . 7:30, Tips for Data Cleaning, Hui Ziang Chua . Notes from talk . Background. Singaporean. Works at essence. Blog data double confirm. | Will try to give business centred context - very different from academic/research/learning contetx | Tasks Get column names | Get size of dataset. Make sure all data has been loaded. | Check datatypes. Sometimes goes wrong. Sign for data-cleaning. | Get unique values. Some cases, need to combine different values into one. E.g. ‘Male’ and ‘M’ | Get range of values | Get count of values. Group-by various columns as appropriate. | Rename column names. E.g. for merging | Remove symbols in values. E.g. currency signs | Convert strings to numeric or to dates | Replace values with more sensible values. E.g. ‘Male’ vs ‘M’ | Identify variables/columns similar or different across datasets | Concatenate data. Data from different quarters added to single table. | Deduplication. Remove duplicate data. | Merge | Recoding. Feature engineering | Data profiling (optional) | Input missing values (optional) | . | Common issues inconsistent naming of variables | bad data formats | invalid,missing values | . | Resources: tinyurl.com/y5b3y7to | . My thoughts . It is useful to have a checklist of tasks one should do when they have to clean data. Interesting that they considered the input of missing values as a bonus task - the impression I got from the Kaggle tutorials is that one ought to do some imputation. . 9:00, Neural Style Transfer and GANs, Anmol Krishan Sachdeva . Notes from talk . Recap of GANs Discriminative model. Supervised classification model. Fed in data. | Generative model. Mostly unsupervised. Generates new data by underlying underlying data distribution. Generates near-real looking data. (Conditional generators have some element of supervised learning included). Learning of distribution called implicit density estimation. | In end, have GAN which takes random input and creates an output that has similar distribution it was trained on. | Training. Dicriminator gets true entry x and fake input from generator network x*. Generator tries to classify. Compute error and backpropogate. Generator then trained. Similar. | . | Book: GANs in Action by Langr and Bok | GANs have been successful in creating near-real images. | Style Transfer: Content image + style image gives new image with content from first in style of second image. | Aim of Style Transfer Networks Not to learn underlying distrbution. | Somehow learn style from style image and embed it into content cimage | Interpolation is bad | Used in gaming industry, mobile applications, fashion/design | . | Popular networks: Pix2Pix, CycleGAN, Neural Style Transfer | Neural Style Transfer. No training set! Just take as input the two images. | Example from Neural Algorithm of Artistic Style. Photo turned into painting of certain artist | Content loss - measure of content between new image and content image | Style loss - measure of style between new image and style image. | total variation loss. Check for blurriness, distortions, pixelations | How is learning done? No trianing set, no back prop? | . | Shows example notebook, using TensorFlow and Keras. Imports pre-trained model. | Content loss. Pixel by pixel comparison not done. | Compare higher level features, obtained from pre-trained model. E.g. VGG19 is 19 layered CNN, which classifies images into 1000 categories. Trained on million images. | VGG architecture. See Generative Deep Learning by David Foster. | Keras repo. Neural style transfer. | Take 3rd layer of block 5 from VGG19 model as measure of higher level features. | Content loss = mean squared error between this vector encoding of high level features from VGG19 for content image and generated image. | . | Style loss Take dot product of ‘flattened feature maps’ | Lower level layers represent style. | GRAM matrix. Dot product of ‘flattened features’ of image with itself. | Didn’t understand this. | Loss = MSE (gram matrix(style image), gram matrix(generated image)) | Then take weighted sum over different sets of layers | Shows example code. | features = K.batch_flatten(K.permute...) | Take first layer in each block | . | Total variation loss sum of squared difference between image and image shifted on pixel down, and of shifted one pixel right | . | Training Add up all the loss | Use L-BFGS optimisation. Essentially just gradient descent on individual pixel values. | Example code shown in talk. | . | Pix2Pix Various cool things you can do. E.g. aerial image to a map. Sketch to full image. | . | CycleGAN E.g. convert image of apples to oranges. Or horses to zebras! | . | . My thoughts . Excellent talk! I had seen some of the neural style transfer images before, and now I have some understanding of how they are created! . 10:00, Data Visualisation Landscape, Bence Arato . Notes from talk . There are lots of libraries out there. | Imperative vs declarative. Imperative: specify how something should be done. Declarative: specify what should be done. | Matplotlib. Biggest example. Has gallery + code examples Background in MATLAB | Challenge: imperative | . | Seaborn. Aim to provide higher level package on top of matplotlib. Good defaults for charts that look good. Example scatterplot much easier in seaborn than in matplotlib | . | plotnine. Aim to higher-level. Based on ggplot2 in R. Syntax is basically same as ggplot syntax! | aes, geom_point etc. | . | bokeh. 2nd most widely known tool. Big thing is interactivity, like sliders, checkboxes, etc. | Example scatterplot. Quite long and low-level | Based on web/javascript background | . | HoloViews Higher level language for bokeh, matplotlib, plotly | Just have to change one line code to switch between bokeh or matplotlib output | Scatterplot example. Short code. | . | hvPlot. built on top of holoviews | pandas bokeh. add plot_bokeh() method in pandas. | Chartify. From spotify. Built on top of bokeh. | Plotly. Might be only one to do 3d charts well | Low level charts | . | Plotly express Higher level version of plotly. | . | Vega, vega-lite Visualisation ‘grammar’. | JSON based way of describing charts | . | Altair High level version of vega | . | Dashboards. Plotly Dash | Panel. Anaconda related. Built on top of four big charting libaries above | Voila. Looks cool - can visualise ML things. Need to check out! | Streamlit. Datascience specific tools. Example of GAN Image generator with sliders. Change slider will re-run model and create new image. | . | PyViz. Thorough list of all visualisation libraries. | Python Data Visualisation, at AnacondaCON 2020 | . My thoughts . Excellent talk. Well structured, good examples, good summary of key things I should know about. . 10:30, Binder, Sarah Gibson . Notes from talk . Table of levels of reproducibility | Same or different data or analysis. Rep = same, same | Replicable = different data | Robust = different analysis | Generalisable = diff, diff. | Repeatable is subset of reproducible. Literally same programs running same data and analysis to get same result. | Reproducible means getting same result using same data and method - but maybe implemented in different language or program or … | . | CI/CD = continous integration / development | Two big tools needed for repeatable research: dockers and version control. But requires learning. Not for everyone. | Shows example from ligo about gravitational waves. | Steps Use jupyter notebook | Upload on public repositiy, e.g. GitHub. | Describe software needed to run notebook. Binder automatically identifies common configuration files | Done! | . | Brief history of Binder. Now 140,000 sessions per week! | Binder is open source, can adapt to your own needs. E.g. share only with specific people in your institution. | Technologies Github, clone reposity | repo2docker. Build docker image based on standard configuration files. Don’t need docker file! | Docker. Execute docker image | Jupyter Hub. Allocate resources, make image accessible at url | Binder, redirect user to the url | . | Scaling up Created federation | Highly stable. Uses different kubernetes implementations | . | User surveys Around 80% of respondants would recommend service | Most common use case is teaching related. Examples, workings, uni teaching, demos, etc. | Biggest complaint: needs to be faster to load | Hard to speed up. But fully explained on jupyter hub blog. Why it is slow but also tips to speed things up. | . | . My thoughts . I had already heard of Binder - because I am friends with the spaker Sarah Gibson! However, the talk was still good, and I learnt more than I already knew. In particular, I liked the classification of different levels of reproducibility. . 11:00, Data pipelines with Python, Robson Junior . Notes from talk . Agenda: Not about code. Anatomy of data product, different architectures, qualities of data pipelines, how python matters. | Anatomy of data product. Ingress. Logs, databases, etc. Volume and variety are important. | Processes. Processing both input and output data. Veracity and velocity are important here. E.g bank processing payment may have to run fraud detection - has to be very quick! | Egress. Apis or databases. Veracity is important. | . | Lambda and Kappa Architeture Above anatomy of data product is same as computer program: input is files and memory, processes in ram, output to screen. | Lambda. Input data. Processing split into two layers. Speed layer for stream data, real time view. Batch layer, all data, batch views, usually processed periodically. Then output via query. See talk for diagram. Some pros and cons given in talk | Kappa. Only have a speed layer - no batch layer. Pros and cons given in talk. | . | Qualities of a pipeline Should be secure. Who has access to which data levels, use common format for data storage, be conscious of who has access to which parts of data and why. | Should be automated. Use good tools. Versioning, ci/cd, code review. | Monitoring. ?? | Testable and tracable. Test all parts of the pipeline. Try to containerise tools. | . | Python tools ELT. Apache Spark, dash, luigi, mrjob, ray | Streaming. faust based on Kafka streams, streamparse via Apache Storm | Analysis. Pandas, Blaze, Open Mining, Orange, Optimus | Mangaement and scheduling. Apache Airflow. Programmatically create workflow | Testing. pytest, mimesis (create fake data), fake2db, spark-test-base | Validation. Cerberus, schema, voluptuous | . | . My thoughts . Unfortunately, I found the talk/speaker hard to follow. But I still got a list of tools that I can use as a reference. . 12:15, Probabilistic Forecasting with DeepAR, Nicolas Kuhaupt . Notes from talk . Freelance data scientist. German. | DeepAR published by Amazon Research | It is probabilistic, like ARIMA and regression models, but not like Plain LSTMs | Automatic Feature Engineering. Key point of neural networks! Like Plain LSTMs but unlike ARIMA and regression models. | One algorithm for multiple timeseries. Seems unlike other algorithms. Like meta learning? Transfer learning?? | Disadvantages: time and resource intensive to train, difficult to set hyperparameters (like most neural networks). | How it works: inputs time series x. At time t, outputs z_t, which are parameters for a probability distribution to predict. Then z_t and x_t+1 are inputted into next stage. | Example datasets. Sales at amazons - one time series for each product, sales of magazines in different stores, forecast loads of servers in datacenters, car traffic - each lane has its own time series, energy consumption in households - each household has its own time series. | Integrated into Sagemaker. Provides notebook | Lots of different tools for different stages of data science workflow. e.g. Ground Truth to use mechanical turk to build data sets. Studio is IDE. Autopilot for training models, Neo for deployment, etc. | . | boto3 - access services in aws. s3fs - file storage stuff. various other details in talk | data inform of json lines, (not pandas) start - start time, target - the time series, cat - some categories that timeseries belongs to, dynamic_feat - extra time series (of same length as target). note that different json lines can have time series of different lengths | . | hyperparameters. standard stuff, with few extras for deepar. | code to set up model and train it | . My thoughts . Looks like a powerful algorithm. Probabilistic algorithms are definitely the way to go - how else can you manage and predict risk? . 13:15, Fast and Scalable ML in Python, Victoriya Fedotova and Frank Schlimbach . Notes from talk . Python is useful but slow. So often companies hire engineers to translate python to faster languages like C++. | Intel made a python distribution. No code changes required. * Just-in-time computation. JIT gives big speed boost. | ML workflow: input and preprocessing via pandas, spark, SDC. model creation and prediction: scikit learn, spark, dl frameworks, daal4py. | In this talk, talk about SDC, sckit learn, daal4py | Intel Daal. Data analytics acceleration library. Linear Algebra already sped up (e.g. with MKL from intel), but this new library helps in situations whcih do not use linear algebra - e.g. tree based algorithms. | Talk gives details on how to install packages and use it. | Many algorithms have equivalent output to scikit learn algorithms. But not all - e.g. randomforest does not have 100% same output. | Example of k-means in scikit-learn versus daal4py. Similar structure. Slightly different syntax. e.g. n_clusters versus nClusters. | Scalable dataframe compiler SDC. a just in time compiler. Extension of Numba - made by anaconda. | Easy to use. Just add decorator @numba.jit to function that you want to be compiled. | Explanation of compiler pipeline. | Talks through basic example of reading file, storing in frame, calculating mean, ordering a column. E.g. reading of file is parallelised, whereas pandas just reads data in single line. | SDC requires code to be statically compilable - i.e. type stable. Examples where this wouldn’t work. | Charts showing speed-ups of different operations, as you increase the number of threads. Some operations get good speeds up, and some get mega speeds ups. Most was apply(lambda x:x) with 400x speed up. Something to do with lambda function being compiled too, not just apply function. | . My thoughts . Looks easy to use and can give big speed boosts. Is there a catch? . 14:15 , Small Big Data in Pandas, Dask and Vaex, Ian Ozsvald . Notes from talk . Discuss how to speed things up in pandas. Why when there are tools out there? Ought to increase our knowledge and push our speed in current tools. | Example of company registration data in uk | Ram considerations Strings are slow. Takes up lots of ram. | In example of company category taking up 300MB. Convert to category type and it takes 4.5 MB. Numeric code stored instead of strings. | Get speed up on value_counts. 0.485s vs 0.028s. | Example of settign this column to index and then creating mask based on index. 281ms vs 569 microseconds. | Try using category for low cardinality data. | . | float64 is default and expansive. Example of age of company, up to 190 years. | Use float32 or float16 instead. | Less RAM. Small time saving. (float16 might actually be slower!) | Might have loss in precision in data. Depends on data and usage | . | Has a tool dtype_diet to automate process of optimising dataframe. Produces table showing how different things can improve RAM usage. | . | Saving RAM is good. Can process more data. Speeds things up. | Dropping to NumPy. df.sum() versus df.values.sum(). | In example, 19.1ms to 2ms. | Somethings to watch out for, e.g. NaN. | James Powell produced diagram showing all files and functions called when doing sum in pandas versus in numpy. (Doing this using ser.sum()). | . | Is pandas just super slow. Can get big boost just by using bottleneck. see code in example. | just instal bottleneck, numexpr | Investigate dtype_diet | ipython_memory_usage | . | Pure python is slow Use numba, njit wrapper. See Intel talk above for newer extensions to numba. | Parallelise with Dask. Overhead may overwhelm benefit. USe profiling and timing tools to check. | . | Big time savings come from our own habits Reduce mistakes. Try nullable Int64, boolean | Write tests, unit and end-to-end | Lots of other examples from blog | . | Vaex and Modin. Two other tools. | . My thoughts . Excellent talk! Ian clearly knows his stuff. Lots of insights. These are things I should start to implement to get some easy time savings. . 14:45, IPython, Miki Tebeka . Notes from talk . Programmer for 30 years. | Wrote book, Python Brain Teasers | Likes ipython. It is a REPL: Real, Eval, Prompt Loop | Magic commands, via %. E.g. %pwd | Can refer to outputs like variables. logs_dir = Out[4] | Command line. !ls $logs_dir | Auto-complete features | He uses Vim! Woo! | pprint for pretty printing. | ? for help. ?? for source code | %timeit function to give time analysis | %%timeit` for multiline stuff | Can do sql stuff. have to install extension. see video for example. | %cow IPython Rocks! Produces ascii art of cow! | . My thoughts . Always good to see a live demo to see exactly how somebody does things. I learnt some neat little features. Also, cool to see someone using Vim! . 15:15, NLPeasy, Philipp Thomann . Notes from talk . Co-creator of liquidSVM, Nabu, NLPeasy, PlotVR | Works at D One. ML consultancy | NLP. Big progress recently. Word2Vec, Deep learning and many good pre-trained models. Lots of data, many use cases | Challenges for data scientists. NLP is generally harder - high dimensions, specialised pre-processing required, nlp experts/researchers focus only on text but there is usually other data in business. | Methods have repuation of being hard to use | standard tools not good for text. e.g. seaborn, tableau | . | NLPeasy to the rescue! | Pandas based pipeline, built in Regex based tagging, spaCy based NLP methods, Vader, scraping with beautiful soup, … | ElasticSearch. ?? | See Github repo for code, ntoebook example, etc. | Talk through example, of looking at abstracts from some conference. | Live demo! Scraping EP 2020 data. | Doing NLPeasy stuff | Go to Elastic dashboard | Lots of things shown, possible. E.g. entity extraction, tSNE | . | REstaurant similarity using clustering algorithms. based only on reviews! can detect similarities | Kibana (I think) can produce geoview / heatmap | Can create networks using entity recognition | Can try examples in different setups, e.g. Binder, or do it all yourself. | . My thoughts . Not much to say. Another tool that I now know about. . 17:45, 30 Golden Rules for Deep Learning Performance, Siddha Ganju . Notes from talk . Forbes 30 under 30! | Recommended book | 95% of all AI Training is Transfer Learning Playing melodica much easier if you already know how to play the piano | In Neural Network, earlier layers contains generic knowledge, and later layers contain task specific knowledge - at least in CNNs | So remove last ‘classifier layers’, and classify on new task, keeping first layers as is. | See github PracticalDL for examples and runnable scripts* Optimising hardware use. | In standard process, CPU and GPU switch between being idle or active. | Use a profiler. E.g. TensorFlow Profiler + TensorBoard | Example code shown to use profiler. | Simpler: nvidia-smi. | . | We have thing we want to optimise and metric. So how can we optimise. Here comes the 30 rules. . | DATA PROCESSING | Use TFRecords Anti-pattern: thousands of tiny files/gigantic file. | Better to have handful for large files. | Sweet spot: 100MB TFRecord files | . | Reduce size of input data Bad: read image, rezie, train. Then iterate | Good: read all images, resizes, save as TFRecord. Then read and train - iterating as appropriate. | . | Use TensorFlow Datasets import tensorflow_datasets as tfds | If you have new datasets, publish your data on TensorFlow datasets, so people can build on your research. | . | Use tf.data pipeline Example code given | . | Prefetch data Somehow breaks circular dependency, where CPU has to wait for GPU and vice versa. | Does asynchronous stuff | Code given in talk | . | Parallelize CPU processing. same as number of cpu cores | . | Paralleize input and output. interleaving. | Non-deterministic ordering. If randomising, forget ordering. Somehow, not reading files ‘in order’ avoids potential bottlenecks. | . | Cache data Avoid repeated reading from disk after first epoch | Avoid repeatedly resizing | . | Turn on experimental optimisations | Autotune parameter values. code given in talk. Seems to refer to hardware based parameters | Slide showing it all combined. . | DATA AUGMENTATION | Use GPU for augmentation, with tf.image Still work in progress. Limited functionality. Only rotate by 90 degrees as of now. | . | Use GPU with NVIDIA DALI . | TRAINING | Use automatic mixed precision. Using 8-bit or 16-bit encodings rather than 32 or 64-bit. | Caveat - fp.16 can cause drop in accuracy, and even loss of convergence. E.g. if gradient is tiny, fp.16 will treat it as zero. | auto mixed precision somehow deals with this issue | . | Use larger batch size. Larger batch size leads to smaller time per epoch (but less steps per epoch too, no?) | Larger batch size leads to greater GPU utilization | . | Use batch szies that are multiples of eight. Big jump in performance from 4095 to 4096! Video describes other restrictions/options | . | Finding optimal learning rate Use keras_lr_finder | ‘point of greatest decrease in loss’ corresponds to best learning rate, somehow. Leslie N Smith paper | . | Use tf.function @tf.function. | . | Overtrain, then generalise. STart from from dataset and increase. | Install optimised stack | Optimise number of parallel threads | Use better hardware. | Distribute training. MirroredStrat vs Multiworker… | Look at industiral benchmarks. . | INFERENCE Use an efficient model. Fewer weights. | Quantize. 16 to 8bit. | Prune model, remove weights close to zero | Used fused operations | Enable GPU persistence | . | . My thoughts . Very handy list of tips and tricks. Sevreal of them go beyond my understanding, but does not mean I can not benefit from using them! . 19:00, Analytical Functions in SQL, Brendan Tierney . Notes from talk . TALK CANCELLED | . My thoughts . Not applicable . 19:30, Collaborative data pipelines with Kedro, Tam-Sanh Nguyen . Notes from talk . Apparently 40% of Vietnamese have surname Nguyen. | Data engineering is relatively new discipline, so there aren’t established practices. | QuantumBlack addressed this issue with Kedro. | Pipelines. Kedro viz used to visualise messy data pipeline. | But how does it get there? | Starts off simple. Iris data -&gt; cleaning function, cleaned data, analyze function, analyzed data. | But then splitting data to test/train, gets messy. | Then have multiple data sources, each which needs to be split, and then all jumbled up | Without any tools, hard to grow larger and more complex than this. | But Kedro can allow you to deal with more complex pipelines. | . | One source of tension is difference between data engineer and data science Data science usually have strong engineering skills. More data modelling skills | Data science has more research bent / experimental bent / want to be close to data and have many iterations | Data engineers more like engineers. Focus is on making things tidy, rather than experimentation. | . | Two other challenges Being ready for production quickly, for business use | Is it easy to pass the pipeline to future users - which may even be the future you! | . | QuantumBlack startup from London, famous for doing work on F1. Got bought by McKinsey | Kedro made by QuantumBlack. Big focus on standardisation, and making it as easy as possible for long-term use. . | How does Kedro work | Analogy - audio world has standardised: Input and output tools. Microphones, speakers, etc. | Functional transformers. Filters, etc. | Redirecting components. Make it easy for output from one tool easy to input into others. | Standard organisational conventions. Mic, audio mixer, computer. | . | Standard is to use jupyter notebook. Some conventions (e.g. inputs and outputs via pd.read_csv and pd.to_csv), but mostly hard to follow. E.g. many many parameters is hard-coded, e.g. names of files being saved, parameters throughout processing, etc. | Live example: install kedro | kedro new. follow steps, e.g. naming things, etc. | created default template | kedro viz to visualise pipeline | . | Run out of time, so rushes through lots of kedro features | Has YouTube series DataEngineerOne | . My thoughts . Looks like an intuitive system. Looks simpler than other pipelines presented in the conference. But is it because it actually is simpler, or is it because I am just getting used to pipelines. (Before this conference, I hadn’t studied pipelines at all). | .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/conference/2020/07/24/europython2.html",
            "relUrl": "/python/data%20science/conference/2020/07/24/europython2.html",
            "date": " • Jul 24, 2020"
        }
        
    
  
    
        ,"post19": {
            "title": "EuroPython Conference 2020, Day 1",
            "content": "Other posts in series . EuroPython Conference 2020, Summary . | EuroPython Conference 2020, Day 2 . | . Introduction . I am attending the online conference EuroPython 2020, and I thought it would be good to record what my thoughts and the things I learn the talks. . 08:00, Waking up . I struggled to wake up in time, for two main reasons: . For the past several months, I have had no need to wake up early, and so my standard wake up time has been 9am. | I stayed up until 2am watching Round 2 of the Legends of Chess tournament… | . But I managed! I then got onto the Discord server to get to the first keynote talk, 30 Golden Rules of Deep Learning Performance, which sounds like it would be particularly insightful. But, unfortunately, the speaker could not give the talk so it was cancelled. At least it gives me time to get breakfast! . 09:00, Docker and Python, Tania Allard . Notes of the talk . Why use Docker? Without Docker, hard to share your models because hard to make sure everyone is using the same modules/appropriate versions of packages. | What is Docker? Helps you solve this issue with ‘containers’. Bundles together the application and all packages/requirements. | Difference to virtual machines: Each application has its own container in Docker, which is small and efficient, whereas virutal machine is highly bloated as each applications is grouped with whole OS and unnecessary extra baggage. | Image - an archive with all the data need to run the app. Running an image creates a container. | Common challanges in DS: Complex setups/dependencies, reliance on data, highly iterative/fast workflow, docker can be hard to learn. | . | Describes differences to web apps | Building docker images. Tania had many struggles/frustrations to learn Docker. Many bad examples online/in tutorials. | If building from scratch, use official Python images, and the slim versions. | If not building from scratch (highly recommended), use Jupyter Docker stacks. | . | Best practices: Be explicit about packages. Avoid ‘latest’ or ‘python 3’. | Add security context. LABEL securitytxt=’…’. E.g. snake. | Split complex run statements | Prefer Copy to Add | Leverage cache Clean, e.g. conda clean | Only use necessary packages | Use Docker ignore (similar to .gitignore) | . | Minimise privilege. Run as non-root user. Dockers runs as root by default | Minimise capabilities user | . | Avoid leak sensitive information Information in middle ‘layers’ may appear hidden, but there are tools to find them. | Use multi-stage builds | . | . | This can be overwhelming, and that is normal. Try to automate and avoid re-inventing the wheel. Use standard project template, e.g., cookie cutter data science. | Use tools like repo2docker. conda instal jupyter repo2docker, jupyter-repo2docker &quot;.&quot; | . | Re-run docker image regularly. One benefit is making sure you have latest security patches. Don’t do this manually, use GitHub Actions or Travis, for example. | Top top tips: Rebuild images frequently. Get security updates. | Do not work as root/minimise privileges | Don’t use Alpine Linux. You are paying price for small size. Use buster, stretch, or Jupyter stack. | Be explicit about what packages you are require, version EVERYTHING. | Leverage build cache. Separate tasks, so you do not need to rebuild whole image for small change. | Use one dockerfile per project | Use multi-stage builds. f * Make images identifiable | Use repo2docker | Automate. Do not build/push manually | Use a linter. E.g. VSCode has docker extension | . | . My thoughts . A lot of this went over my head. The main lesson I learnt is that I should expect things to be tricky when I eventually do start using Docker. I will refer back to this video when I do start using Docker. . 10:00 spaCy, Alexander Hendorf . Notes of the talk . NLP: avalanche of unstructured data | Estimate 20:80 split between structured and unstructured ata | Examples of NLP: chatbots, translation, sentiment analysis, speech-to-texxt or vice versa, spelling/grammar, text completion (GPT-3! :D) | Example of Alexander’s work: certain group had large number of documents and search engine was not helping them. Used NLP to create clusters of documnets, create keywords, create summaries. | spaCy. Open source library for NLP, comes with pretrained language models, fast and efficient, designed for production usage, lots of out-of-the-box support. | Building blocks of spaCy Tokenization | Part of speech tagging. E.g. which words are nouns or verbs, etc. | Lemmatization. cats-&gt;cat | Sentence boundary detection | Named Entity Recognition. (Apple -&gt; company). Depends on context! | Serialization. saving | Dependency parsing. How different tokens depend on each other | Entity linking | Training. Updating models | Text classification | Rule-based matching | . | Built-in rules Rules for specific languages, e.g., adding ‘s’ to end of noun makes it plural in English. | Usually does not cover many exceptions | Most languages won’t be supported. Most research done on English. | . | Built-in models Language models. E.g. word vectors | Can train your own models with nlp.update() | Need a lot of data to train these models. Few documents is not enough | . | spaCy is pythonic should understand objects, iterations, comprehensions, classes, methods | Might be overwhelming, but not as overwhelming as Pandas yet! | . | Pipelines Has nice image in slides | default pipline: tokenize, tag, parse, then your own stuff | . | Visualisation used to be separate package displacy | visualisation of sentence grammar/dependencies | visualise entities. e.g. given sentence, highlight grouping of nounes. e.g. is a word a person, or a date, or an animal, or… | . | Serialization uses pickle | . | Danger zones Privacy, bias, law, language is not fixed in stone | . | Can’t do all languages | Extensions spaCy universe | E.g. NeuralCoref. Matching up ‘Angela Merkel’ and ‘chancellor’ recognised as same person. | . | Bugs spaCy will maintained, and quick response to bug reports | Extensions more variable | . | Status Other options: NLTK, Gensin, TextBlob, Pattern | spaCy: usually close to the state of the art, especially for language models, flexible, extendable via spacy universe, fast (powered by cython) | For special cases, probably use other models. E.g. RASA for contextual textbots | . | . My thoughts . I have not yet done any NLP, but when I do, I will be sure to look into spaCy after this talk! . 10:30 Differential Privacy, Naoise Holohan . Notes of the talk . Many examples where ‘anonymised’ but actually could still identify individuals by matching data with other data publically available. Netflix data. Matched with imdb databse | AOL data. NYT managed to identify individual and make available their full search | Limosine service. Person matched it with images of celebrities, and managed to find out about individuals use of taxis/limos | Many other examples out there. | . | Differential privacy. Main idea: blur noise. | Implemented in diffprivlib, for scikit learn. | Modules Mechanisms. Algorithms to add noise | Models. Has scikit learn equivalents as its parent class | Tools. Analogue for NumPy. | Accountant. Track privacy budget. Help optimise balance between accuracy and privacy. | . | Examples Gives warning for privacy leakage with certain bound is not specified. If bound is not specified, then original dataset is used to estimate the parameter! | Pipelines. With and without privacy. Without, got 80.3% accuracy | With, got 80.7% accuracy! Adding noise can actually reduce over-fitting. !! | Graph of epsilon vs accuracy. Low epsilon highly variable performance. Higher epsilon tends to 80% baseline | . | Some exploratory data analysis | . | . My thoughts . I have actually heard of differential privacy before, via this talk from FacultyAI. To anybody interested in this topic, I recommend watching the FacultyAI talk for more background on differential privacy itself. . Main lesson here is that if I want to analyse sensitive data, diffprivlib is a good open source option. . The big surprise factor was that the accuracy can sometimes be better after adding privacy! Adding noise can reduce over-fitting. . 12:15, Parallel and Asynchronous Programming in DS, Chin Hwee Ong . Notes of the talk . Background. Engineer at ST Engineering. Background in aerospace engineer and modelling. Contributor to pandas. Mentor at BigDataX. | Typical flow: extract raw data, process data, train model, evaluate and deploy model. | Bottlenecks in real world Lack of data. Poor quality data | Data processing. 80/20 dilemma. More like 90/10! | . | Data processing in python For loops, list = [], for i in range(100), list.append(i*i) | This is slow! | Comprehensions. list = [i*i for i in range(100)] | Slightly better. No need to call append on each iteration | Pandas, optimised for in-memory analytics. But get performance issues when dealing with large datasets, e.g. 1&gt;GB. Particularly in 100GB plus range. | Why not just use spark? Overhead cost of communication. Need very big data for this to be worthwhile. What to do in ‘small big data’? | Small Big Data Manifesto by Itamar Turner-Trauring | . | Parallel processing Analogy: preparing toast. | Traditional breakfast in Singapore is tea, toast and egg | Sequential processing: one single-slice toaster | Parallel processing: four single-slice toaster. Each toaster is independent | . | Synchronous vs asynchronous Analogy: also want coffee. Assume it takes 5 mins for each coffee, 2 mins for single toaster. | Synchronous execution: first make coffee, and then make toast. | Asynchronous: make coffee and toast at the same time | . | Practical considerations Parellelism sounds great. Get mega time savings | Is code already optimised? Using loops instead of array operations | Problem architecture. If many tasks depends on previous tasks being completed, parallelism isn’t great. Data dependency vs task dependency. | Overhead costs. Limit to parallelisation. Amdahl’s. | Multi processing vs multi threading | . | In python concurrent.futures module | ProcessPoolExecutor vs ThreadPoolExecutor. | . | Example Obtaining data from API. JSON data. 20x speed up versus list comprehension. | Rescaling x-ray images. map gives 40 seconds. list comprehension 24 seconds. ProcessPoolExecutor about 7s with 8 cores. | . | Takeaways Not all processes should be parallelized. Amdahl’s law, system overhead, cost of re-writing code. | Don’t use for loops! | . | . My thoughts . Something for me to investigate. I have not needed to use this yet. Nice bonus - I learnt about Singaporean breakfasts! . 12:45, Automate NLP model deployment, William Arias . Notes of the talk . Background. Colombian, lives in Prague. Works at GitLab | Often, data scientists are a one-person band. Have to do learn many different tool. | Define a symphony. Produce flowchart of data workflow. Make explicit where different people can/should contribute to process. Favour for yourself | Makes easier for everyone to understand process | . | Symphony components. | I found it hard to follow details here. I’d have to re-watch the video. | This might be standard knowlege for people who already work in software engineering. But somebody with maths background, say, this kind of automation and workflow is not obvious. | This will make your life easier! | Has video showing example of making small change to chat bot, and how much is automated. | See examples on github. | . My thoughts . Probably too advanced for me at this stage. But, something I should be aware of when I work on bigger projects. . 13:15, Building models with no expertise with AutoML, Laurent Picard . Notes of the talk . Background, French, ebook pioneer, cofounder of bookeen | Their definition of ML: given data, extract data. | Correct definition: AI contains machine learning contains deep learning. | Graph showing increase of ‘Brain Model’ at Google. Number of directories using it. At 7000 around 2017. | AutoML - somewhere between ML APIs (developer skills) and ML (machine learning skills). | Ready-to-use models. | Vision API. Laurent in 90s tried to detect edges, and it was very hard. Label detection. What is in picture? | Locate picture by matching with google’s database | Bounding boxes for objects in the picture, e.g. box for trousers, box for person | Face detection. Emotion prediction. | Text detection. Identify blocks of text. Works even if image is slanted. | Hand-writing detection. Not as good as text detection (obviously), but still good. | Web entity detection/image matching. Identify source of image, identify topic of image. E.g. picture of Tolkien identified as Tolkien and its source found. | from google.cloud import vision | . | Video intelligence Apply image analysis to each frame | from google.cloud import videointelligence | codelabs.developers.google.com/codelabs/cloud-video-intelligence-python3 | . | NLP Syntax analysis. Language detection, syntax analysis (dependency, grammar, etc.) | Entity detection. Understands context. Given match, gives unique id and wikipedia link! | Content classification. | Sentiment analysis. E.g helps company judge how people are talking about their service or product. | Tutorials available on codelabs | Translation API. | Speech-to-text API | Speech timestamps. Given script and audio, attach | Text-to-speech. WaveNet by DeepMind | . | Cloud AutomL You provide data. AutoML does training, deployment and serving | Can create your own API for cloud model | For offline, can get TF Lite model for mobile, TF model for browser, or container for anywhere. | Example of identifying between different types of clouds. Upload around thousand images. Can specify computer hours and visualise results. | . | . My thoughts . Not sure what my takeaway message is here. Looks like a useful and easy to use set of tools. But not sure when I will use it given that I am aiming to become a data scientist. . 14:15, Simulating hours of robots’ work in minutes, Eran Friedman . Notes of the talk . Works at Fabric. Helps develop ground robot. | SimPy library. Discreate event simulation | Three objects: environment, … | And I stopped taking notes. | . My thoughts . I do not anticipate needing to know about simulations any time soon, and I am feeling exhausted from first several hours of talks, so I decided to take a break. SimPy looks cool, but not for me. . 14:45, Parallel Stream Processing at Massive Scale, Alejandro Saucedo . Notes of the talk . Chief Scientist at Institue for Ethical AI. Director at Seldon. | Realtime ML in today’s talk, conceptual intro to stream processing, tradeoffs in different tools, example case | Real-time ML model for reddit comments. To help automate comment moderation. | ETL, Extract Transform Load framework for data transformation. Batch processing. Variations: ETL, ELT, EL, LT. Many specialised tools. Image of about 40 different packages that deal with this. EL, Nifi and Flume | ETL, Oozie, Airflow | ELT, Elasticsearch, Data Warehouse | Jupyter notebook? | . | Batch vs streaming Data processed in batches. E.g. periodically | Stream processing. Process data as it comes in, each data entry at a time. Real time | In reality, have combination of the two. Rarely all or nothing. | . | Stream concepts Windows. Can have moving window or tumbling window | Checkpoints. Keep track of stream progress. Leads to other ideas, e.g. processing at most/at least once. | Water mark. Somehow allows you to deal with data that arrives later than is expected | . | Some tools. Flink, Kafka, Spark, Faust, Apache Beam, Seldon | Traditional ML workflow. Train a model on cleaning training data. Obtain ‘persisted’ model. Then get unseen data, and use model to make predictions. Reddit example: clean text, spaCy tokenizer, TFIDF vectoriser, logistic regression. | You are a DUMMY!!!! -&gt; You are dummy -&gt; [PRON, IS, DUMB] -&gt; [0010, 1000, 1100] -&gt; 1 (which equals moderated). | . | Stream-based workflow. Note that ‘core’ is part of Seldon. | Gives example code in video for each step in workflow. | Seldon still developing service. Have open examples on github. They are open to feedback. | . My thoughts . Good to learn about this newer workflow and the tools available for stream processing. Again, at this stage of my learning, I am unlikely to use the ideas directly any time soon, but it is good to have an awareness. . 18:30, A Brief History of Jupyter Notebooks, William Horton . Notes of the talk . Tension between traditional python IDE and jupyter notebooks “I don’t like notebooks” - Joel Grus | First Notebook War, Martin Skarynski. Talk at PyData 2018 | . | Instead of arguing, lets understand the history. Better understanding as as result No significant piece of software doesn’t come out of nowhere | . | Long-term trends: scientific computing, literate programming, proprietary vs open source, python | Mathematica, 1988. By Stephen Wolfram. Theodore Gray created Notebook interface. Well received at the time. Had two parts to notebooks. Kernel and front-end. Notebooks are objects in themselves that could be manipulated by mathematica. | . | The Art of Computer Programming, by Knuth. Literate programming. Implemented ‘WEB’ system. | TANGLE - generates compilable source code | WEAVE - generate formatted documentation | Used this idea to implement Tex! | . | Maple. In 1992, had ‘worksheet’ interface. | Maple, Mathematica. Mathematical entry vs programming style entry, enter vs shift+enter, etc. | But both expensive and propritary | . | Open source. SciPy, IPython and Matplotlib | Big name: Fernando Perez. Created IPython in 2001 as grad student. | … | . My thoughts . I stopped taking notes, because I do not think I will need to refer back to this. Time to just enjoy the talk! . 19:00, Quickly prototype translation from scratch and serve it in production, Shreya Khurana . Notes of the talk . Background. Data Scientist at GoDaddy, deep learning NLP | Workflow in academia is different to business use. Deployment is whole extra part of the workflow. | Lots of new tools that you wouldn’t be familiar with from just training models. | Example: seq2seq model i.e. translation. E.g. german sentence to english sentence. Small dataset from TedTalk transcripts. | . | Fairseq. Used for preprocessing, training, model serving. In some unknown language - shell script? | Flask. Tool to create API. Development server. Example code given in talk. | uwsgi. Helps make Flask app secure / ready for production. There is uwsgi.ini file to configure stuff. | nginx. (pronounced ‘engine-x’). Idea of QPS - queries per second. Make sure all requests are appropriately routed to server. Unknown language for nginx. | supervisord. coordinates nginx and uwsgi. | Docker. The above system has many dependencies. Docker creates containers where you can isolate all the requirements and programs, which can loaded up and run remotely. | General good practice: check logs frequently, caching, unit tests. | . My thoughts . A lot of useful information here. I will be returning to this when I have to put a model into production. . 19:30, Painless Machine Learning in Production, Chase Stevens . Notes of the talk . Background. Works for teikametrics | Focus on production, not on machine learning. Model building is relatively mature, but still need lots of work on improving production. | Goal of teikametrics - helps online e-commerce businesses. | Motivation: Ops is intrinsic to ML, ‘MLOps’ is unsustainable (where data scientists pass their models to software engineers). Conclusion: data scientists need to productionise their own models. But data scientists want to do data science. Hence, need tooling and services to make it easier as possible. | Looked for services to do full cycle (preprocess, train, evaluate, deploy, repeat), but couldn’t find any. | Interesting graph showing how AUC drops for models over time. Models need to be re-trained! Another example of how covid makes models from 2019 almost useless. | Different clients will have different markets which require different models. | Previous two points show importance of having efficient workflow and cycle. | MLOps is unsustainbly Brief history of programming. Punch cards programmers separate from people who run programs, changes with terminal, late 90s, programmers separate from quality assurance and release team, rise of devops. | MLOps is making similar mistakes of past. Lots of slow back and forth between data scientists and production team. | . | Modern experience at tiekametrics. Use cookiecutter on their sagemaker-framework. Asked a bunch of questions, which after answering, get repo made with good structure built in. | Define preprocessing function (SQL, Pandas), define train and validation training sets and model, define model loading function. | Various details skimmed over. E.g. need to update config file. | Whole list of tasks that care standardised and made easy. See talk for list. | . | Their stack: | Various details given in talk | Big lesson. Do this! Big savings. Even little things: example of automating process of choosing which AWS instance to use. | . My thoughts . Another useful set of resources and examples I can use as a reference if/when I need to make production based decisions. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/conference/2020/07/23/europython1.html",
            "relUrl": "/python/data%20science/conference/2020/07/23/europython1.html",
            "date": " • Jul 23, 2020"
        }
        
    
  
    
        ,"post20": {
            "title": "Santander Dataset, Part III, Learning from others",
            "content": "Other posts in series . Santander Dataset, Part II, Feature Selection . | Santander Dataset, Part I . | . Introduction . I did some hyper-parameter optimisations, but there was nothing particularly noteworthy. I got some incremental improvements using GridSearch and that’s about it. After that, I had a look at what other people on Kaggle did and see what I can learn from them. . Read the instructions! . For some reason, I thought that the metric for this contest was accuracy? I suspect it is because I had read this line from the instructions: “For each Id in the test set, you must make a binary prediction of the target variable.” However, other people were submitting probabilities, not just binary predictions. So, I tried submitting the raw probabilities from my model, and the score jumped up from around 0.77 to 0.86! Something is a amiss. Have I misunderstood accuracy. I re-read the instructions and find that I somehow missed this line: “Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.” That makes more sense. But if they are using AUROC, then why do they ask for a binary prediction?! . EDA . This is something that I should have done, having noticed other people doing it for the credit card dataset, but forgot to do. I will make sure to do this for next time! (I did very minimal exploration, but there is clearly more I could do). . Two other algorithms . There are two more algorithms to add to my toolbox: Naive Bayes and LightGBM. Naive Bayes is intuitive and I am surprised I have not encountered it already. LightGBM seems to be a faster alternative to XGBoost, and this seems to be the most popular algorithm used in this challenge. . Creating a model for each feature separately . This idea seems to be first described in this example and here. The intuition for why this works is that the features are seemingly independent, so you can combine the predictions made from considering each feature, one at a time. I can’t remember which example it is, but somebody else calculated all the pairwise correlations between features and found they were all very small. . Using frequency as new feature . This idea was referred to as the magic feature (e.g. here): for each feature ‘F’, add a new feature ‘F_freq’ which is the frequency of the first feature ‘F’. It is not intuitive to me why this should improve the performance of the model. . Visualising a tree-based algorithm . In this example, there are excellent visuals demonstrating how LightGBM is making its predictions. In particular, there are excellent visuals that demonstrate how including the magic frequency feature improves the models. . A remarkable discovery . YaG320 made an incredible discovery: by looking at the frequencies that different values occur, they concluded that there must be synethic data and then separated out the real data from the synthetic. The reasoning was clever: by noticing that there were fewer unique values in the test data than in the training data, they suspected that the test data started out as some real data and then augmented with some synthetic data. Furthermore, the synthetic data will only use values that occured in the real data. To sniff out the synthetic data, you have to ask yourself: are any of its feature values unique? If yes, then it cannot be synthetic (as synthetic only uses values that already occured in the real data), and if not, then it is very likely synthetic (not certain but very likely). YaG320 wrote code to implement this idea, and found that exactly half the code was real and half was synthetic. Impressive detective work! By removing the fake synthetic data from the construction of their models, people were able to improve their models. . It is unlikely this exact idea will be useful for me in any future projects I will do. However, it highlights the power and thrill of data science. By looking at a bunch of numbers (and with a healthy dose of creativity) one can make deductions that would otherwise be completely hidden. . Conclusion . There is a lot for me still to learn! Trying to analyse these Kaggle datasets and then comparing my approach to others seems to be an excellent way to learn and I will be sure to continue it. However, I need to practice some data cleaning/data scraping, so I will start some projects in that vain soon. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/07/18/santander3.html",
            "relUrl": "/python/data%20science/2020/07/18/santander3.html",
            "date": " • Jul 18, 2020"
        }
        
    
  
    
        ,"post21": {
            "title": "Neural Networks, Part II, First MNIST model",
            "content": "Other posts in series . Neural Networks, Part I, Basic network from scratch | . First MNIST model . Using the neural network class from Part I, I train a neural network on the MNIST dataset. . data_train, data_valid, data_test = mnist_loader.load_data() def train(): net = neural1.Network([784, 30, 10]) print(f&quot;Accuracy on testing data: {net.accuracy(data_test)}&quot;) net.sgd(data_train, 10, 10, 3.0) print(f&quot;Accuracy on testing data: {net.accuracy(data_test)}&quot;) with open(&#39;network1.config&#39;, &#39;wb&#39;) as f: pickle.dump(net, f) . The results of running this were: . Accuracy on testing data: 0.08475 Epoch 0 starting. Epoch 0 done. Accuracy is 0.902 Epoch 1 starting. Epoch 1 done. Accuracy is 0.912 Epoch 2 starting. Epoch 2 done. Accuracy is 0.931 Epoch 3 starting. Epoch 3 done. Accuracy is 0.939 Epoch 4 starting. Epoch 4 done. Accuracy is 0.930 Epoch 5 starting. Epoch 5 done. Accuracy is 0.943 Epoch 6 starting. Epoch 6 done. Accuracy is 0.943 Epoch 7 starting. Epoch 7 done. Accuracy is 0.948 Epoch 8 starting. Epoch 8 done. Accuracy is 0.948 Epoch 9 starting. Epoch 9 done. Accuracy is 0.950 Accuracy on testing data: 0.9496333333333333 . Next steps . I will continue to work through Nielsen’s online book, learning more about neural networks. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/neural%20network/2020/07/14/neural2.html",
            "relUrl": "/python/data%20science/neural%20network/2020/07/14/neural2.html",
            "date": " • Jul 14, 2020"
        }
        
    
  
    
        ,"post22": {
            "title": "Santander Dataset, Part II, Feature Selection",
            "content": "Other posts in series . Santander Dataset, Part III, Learning from others . | Santander Dataset, Part I . | . Introduction . After some Googling and reading of various blog posts and articles, I decide to carry out a few different feature selection techniques, record them all in a pandas frame, and pick out the important features as appropriate. The feature selection techniques I use are: . Calculate ANOVA F-value between each feature and prediction target | Obtain feature importances from XGBoost model | Calculate correlations between each feature and prediction target | Obtain coefficients from logistic regression with L1-regularisation | Obtain coefficients from logistic regression with L2-regularisation | . Visualising the feature scores . Below are plots showing how the different methods of measuring feature importance compare with one another. . The main takeaways for me are: . The different measures are all strong correlated with one another. This is a good thing of course, because it means there really is a consistent notion of feature importance. | The ANOVA F-values and correlations seem to provide exactly the same information. This is presumably not a coincidence, and there will probably be simple mathematical relationship between correlation and the F-values. | The L1- and L2-regularisations have a perfect correlation. Visually scanning the coefficients also showed they were almost exactly the same. This makes me suspicious and wonder if I did something wrong. As far as I could tell I did not. This is something for me to investigate in future, because I was expecting L1 and L2 regularisations to produce some noticable difference. | The logistic regressions and correlations have a very strong correlation. From my understanding this is not a coincidence - I believe there is a direct relationship between the coefficients and correlations (at least when there is only one feature variable). | The XGBoost feature importances are least correlated with the others. I suppose this makes, because I think the other four quantities have direct mathematical relationships between them, whereas tree-models are qualitatively different. | . To remove the non-linearity in some of the charts above, I decided to also plot feature ranks that these different measures produce. . There is nothing new shown in these graphs - it just makes the patterns listed above a bit clearer. . Models with only the most important features . Next I produced several logistic models keeping differing amounts of features removed. I used logistic models because they were the quickest to create. . The patterns here are clear. My takeaways are: . As you increase the number of features kept, the model improves. | The 100 least important features provide very little information to the models. | However, the 100 least important features do provide some information. The models did not improve by removing them. | . Conclusion . It looks like removing the least important features has not improved our models. The one thing it did improve was the time taken to create the models. Also, in a real-life situation (where we knew what the variables corresponded to), we would have gained insight into which variables are important, which presumably would help in decision-making. . Next steps . The next thing I will do is some hyper-parameter optimisations. After that, I will have used up all the tricks I have available, and then look at other people’s models and see what I can learn. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/07/13/santander2.html",
            "relUrl": "/python/data%20science/2020/07/13/santander2.html",
            "date": " • Jul 13, 2020"
        }
        
    
  
    
        ,"post23": {
            "title": "Neural Networks, Part I, Basic network from scratch",
            "content": "Other posts in series . Neural Networks, Part II, First MNIST model | . Introduction . I finally take the plunge and create my first neural network. I have been holding back because I wanted to create my first neural networks from scratch before using the ready-made packages like TensorFlow or PyTorch. This is so that I would develop a deeper understanding (and I should probably do the same thing for the other big algorithms I have already used, like Random Forests). I take the plunge now because I came across this tutorial by Michael Nielsen which explains everything from scratch. (Funnily, I found this tutorial indirectly, via Michael’s excellent article on Quantum Computation). . The neural network I create is almost completely vanilla: no fancy architectures, the loss function is RSS, I use the sigmoid function for the activation function. The one non-vanilla idea was to use mini-batches to estimate the gradient of the cost function, instead of using the whole training dataset. After reading through Chapter 1 of Nielsen’s tutorial and skimming through the example code, I tried to create the program from scratch. I did check back with the example code on several occasions to check I was not going astray, so my code and his example are very similar. . Testing the code . To test the code works, I created some made up easy data to be classified (see code below) and achieved the following output: . Epoch 0 starting. Epoch 0 done. Accuracy is 0.518 Epoch 1 starting. Epoch 1 done. Accuracy is 0.582 Epoch 2 starting. Epoch 2 done. Accuracy is 0.893 Epoch 3 starting. Epoch 3 done. Accuracy is 0.919 Epoch 4 starting. Epoch 4 done. Accuracy is 0.956 Epoch 5 starting. Epoch 5 done. Accuracy is 0.973 Epoch 6 starting. Epoch 6 done. Accuracy is 0.966 Epoch 7 starting. Epoch 7 done. Accuracy is 0.966 Epoch 8 starting. Epoch 8 done. Accuracy is 0.967 Epoch 9 starting. Epoch 9 done. Accuracy is 0.971 Accuracy on testing data: 0.968 . It was satisfying to see that the code appears to work! . A proud moment . A part of doing things from scratch included deriving the back-propogation formulae. I found this trickier than I was expecting - afterall, I just have to use the Chain Rule over and over again. How hard can that be?? After straining my mind for some time, I think I have got it but am not sure. Before trying to code it up, I have a look at Nielsen’s code to check, and I got it correct. I was chuffed with myself! :D . Learning points . The main mistake I made when coding up the algorithm was not paying attention how a vector should be represented in NumPy. In particular, NumPy does not treat a rank-1 array of size (n) the same as a rank-2 array of size (1,n), for example, with transposing. This took some time to debug, because my first suspicion was that I mis-typed the formulae, or that I got the indices mixed up, or some other little error. In the end, I had to change how I coded the vectors to rank-2 arrays of size (1,n). | Nielsen often had a tidier way of coding the same steps or calculations, often by using zip. This is a useful little function which I will be sure to use in the future! | . Next steps . The immediate next step is to use this code to read hand-writing using the MNIST dataset, and then work through the rest of Nielsen’s tutorial where we optimise the network in various ways. After that, the world is my oyster! At some point, I need to learn some RL, so I can continue on my AI for Games project. . The code . import numpy as np import math import random class Network(): def __init__(self, sizes): self.n_layers = len(sizes) self.sizes = sizes self.biases = [np.random.normal(size = (1,size)) for size in sizes[1:]] self.weights = [np.random.normal(size = (size1, size2)) for size1, size2 in zip(sizes[:-1], sizes[1:])] def feed_forward(self, a): b = self.biases w = self.weights for i in range(self.n_layers - 1): a = vsigmoid(np.dot(a,w[i]) + b[i]) return a def sgd(self, data_train, epochs, mini_batch_size, learning_rate): n_dt = len(data_train) mbs = mini_batch_size for epoch in range(epochs): print(f&quot;Epoch {epoch} starting. &quot;, end = &quot;&quot;) random.shuffle(data_train) mini_batches = [data_train[k:k+mbs] for k in range(0, n_dt, mbs)] for mini_batch in mini_batches: self.update_via_minibatch(mini_batch, learning_rate) acc = self.accuracy(data_train) print(f&quot;Epoch {epoch} done. Accuracy is {acc:.3f}&quot;) return None def update_via_minibatch(self, mini_batch, learning_rate): mbs = len(mini_batch) delta_b = [np.zeros((1,size)) for size in self.sizes[1:]] delta_w = [np.zeros((size1, size2)) for size1, size2 in zip(self.sizes[:-1], self.sizes[1:])] for x,y in mini_batch: db, dw = self.backprop(x,y) delta_b = [b1 + b2 for b1,b2 in zip(delta_b, db)] delta_w = [w1 + w2 for w1,w2 in zip(delta_w, dw)] self.biases = [b - (learning_rate/mbs)*db for b, db in zip(self.biases, delta_b)] self.weights = [w - (learning_rate/mbs)*dw for w, dw in zip(self.weights, delta_w)] return None def backprop(self, x, y): # introduce shorthand notation for weights and biases w = self.weights b = self.biases # feedforward. store values of a and z a_temp = x z_temp = x a = [x] z = [x] for i in range(self.n_layers - 1): z_temp = np.dot(a_temp, w[i]) + b[i] a_temp = vsigmoid(z_temp) z.append(z_temp) a.append(a_temp) # define variables to store gradients grad_a = [None for _ in a] grad_z = [None for _ in z] grad_b = [None for _ in b] grad_w = [None for _ in w] n = self.n_layers # initialise gradients for a and z in final layer grad_a[n-1] = 2*(a[n-1]-y) temp = vsigmoid_prime(z[n-1])*grad_a[n-1] grad_z[n-1] = temp # back propogate for i in range(n-2,-1,-1): grad_b[i] = grad_z[i+1] grad_w[i] = np.dot(np.transpose(a[i]), grad_z[i+1]) grad_a[i] = np.dot(grad_z[i+1], np.transpose(w[i])) grad_z[i] = vsigmoid_prime(z[i])*grad_a[i] return grad_b, grad_w def accuracy(self, data_test): acc = 0 for x,y in data_test: y_hat = self.feed_forward(x) match = (np.argmax(y_hat) == np.argmax(y)) acc += int(match) return acc / len(data_test) def sigmoid(z): return 1/(1+math.exp(-z)) def sigmoid_prime(z): return (math.exp(-z))/((1+math.exp(-z))**2) vsigmoid = np.vectorize(sigmoid) vsigmoid_prime = np.vectorize(sigmoid_prime) def test(): net = Network([2,3,4,5]) data_train = [] for _ in range(1000): if random.randint(0,1) == 0: x = np.random.normal(loc = 0.6, scale = 0.15, size = (1,2)) y = np.array([1,0,0,0,0]) else: x = np.random.normal(loc = 0.2, scale = 0.15, size = (1,2)) y = np.array([0,0,0,0,1]) data_train.append((x,y)) net.sgd(data_train, 10, 50, 3.0) data_test = [] for _ in range(1000): if random.randint(0,1) == 0: x = np.random.normal(loc = 0.6, scale = 0.15, size = (1,2)) y = np.array([1,0,0,0,0]) else: x = np.random.normal(loc = 0.2, scale = 0.15, size = (1,2)) y = np.array([0,0,0,0,1]) data_test.append((x,y)) print(f&quot;Accuracy on testing data: {net.accuracy(data_test)}&quot;) test() .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/neural%20network/2020/07/09/neural1.html",
            "relUrl": "/python/data%20science/neural%20network/2020/07/09/neural1.html",
            "date": " • Jul 9, 2020"
        }
        
    
  
    
        ,"post24": {
            "title": "Santander Dataset, Part I",
            "content": "Other posts in series . Santander Dataset, Part III, Learning from others . | Santander Dataset, Part II, Feature Selection . | . Introduction . I have started a new project. I had another look at the Kaggle datasets and chose this Santander dataset. The data is (superficially?) similar to that from the Credit Card Fraud dataset I have previously analysed: the data is clean, all numerical, and the task is to create a binary classifier. A big difference is that this Santander dataset has 200 features, whereas the Credit Card Fraud one had only 30 features. I presume this will make a difference (maybe I have to do some feature selection?), but I guess I will find out soon! Another difference is that we have two datasets: a training dataset on which we should create our models, and a testing dataset on which we use our models to make predictions which are then submitted to Kaggle. . Like with the credit card fraud project, I will start this one by creating some default models, and hopefully gain some ideas on how I ought to progress. . Default models . Some minimal data exploration shows that 90% of the training data has a target feature of 0 and 10% has target feature of 1. Due to this skew, I decide to us AUPRC to evaluate the models. Note that I split this training set further into a sub-training set and sub-testing set, fit the models on the sub-training set and evaluate the models using AUPRC on the sub-testing test. (Is there better terminology for this kind of thing?!). . Also, I did minor pre-processing, namely, I re-scaled the features to have a mean of 0 and a standard deviation of 1. . Logistic Regression . This does not look good. Lets see how other models do. . Decision Tree . This does even worse! I guess this should be expected of decision trees. It is also worth noting that this took a couple of minutes to create, so I decided not to create a random forest, because I presume it would take a very long time. . kNN . This also does poorly. However, I recently started reading Elements of Statistical Learning and it describes the ‘curse of dimensionality’, so I am not surprised by this low performance. Roughly, if you have many features, the nearest neighbours of a point are unlikely to be close to the point, and so not representative of that point. . XGboost . This has basically the same PRC as logistic regression, and much worse than what was achieved in the default credit card dataset. . SVM . And once again, a similar PRC to logistic regression and xgboost. Note that this one took a few hours to complete, so I will not be using these again for this project. . Handmade model . I used the same handmade model that I created in the credit card fraud project over here. As can be seen, this performance is in between the worst so far (decision tree and knn) and the best so far (xgboost, regression, svm). To me, this suggests that main issue is not with the number of features, but that maybe the dataset itself is difficult to work with and that it is hard to distinguish between the two classes. . Random model . Based on all these graphs above, it was clear I had misunderstood something basic about the PRC graph. I believed that the worst case scenario for this curve was a straight line joining the two corners, and initially thought that these models were doing either worse or just as good as a random model. After thinking for a bit, I realised my mis-understanding. To confirm my feelings, I created a purely random model and the PRC is above. This curve makes sense: we get a straight line with a precision of 0.1 because 10% of the data has a target value of 1. If you’re just making random guesses, then you should expect that 10% of the predicted positive cases are truly positive, i.e., you should expect to get a precision of 0.1. . I think this misunderstanding arose because I got mixed up with ROC curves, in which a random model does produce a straight line. . Next steps . I will try to improve the models by doing some feature selection. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/07/01/santander1.html",
            "relUrl": "/python/data%20science/2020/07/01/santander1.html",
            "date": " • Jul 1, 2020"
        }
        
    
  
    
        ,"post25": {
            "title": "Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part V, Final Models . | Investigating Credit Card Fraud, Part IV, n_estimators . | Investigating Credit Card Fraud, Part III, Handmade Model . | Investigating Credit Card Fraud, Part II, Removing data . | Investigating Credit Card Fraud, Part I, First Models . | . Summaries . Part I . In Part I, I described the framework and created the first set of models using default settings. I tried logistic regression, decision tree, random forest and xgboost models, and they respectively achieved an AUPRC of 0.616, 0.746, 0.842 and 0.856. Since then, I have learnt about more models and if I were to do this project again, I would also have included a support vector machine model and a k-nearest-neighbour model. . . Part II . In Part I, it look some time to fit the models, the forest model in particular, and I wanted to do some hyper-parameter optimisations. I wanted to find out if I could reduce the time taken to fit by removing non-fraudulent claims. The results of the experimentation showed that the time to fit was proportional to the size of the training data set, but the AUPRC did not take a massive hit. This is good because it means I can do more hyper-parameter optimisations than before. . . Part III . Though the models were able to identify fraudulent transactions, I had gained no understanding. I tried creating a simple model: for each feature, determine whether the value is closer to the fraudulent mean or the non-fraudulent mean. This achieved an AUPRC of 0.682 and was able to identify about 70% of the frauduluent claims. This was satisfying, and better lets me appreciate what is gained by using more sophisticatd models. . . Part IV . I started doing some hyper-parameter optimisations on the forest model, and noticed the AUPRC varied a lot between the different folds. I decided to investigate how the AUPRC can vary, to better appreciate what is gained by choosing one hyper-parameter over another. After doing this, I could confidently say that choosing 50 estimators is better than the default of 100 estimators. . . Part V . Here I actually carry out the hyper-parameter optimisations, and train the final models. The random forest’s AUPRC increased from 0.842 to 0.852, and the xgboost’s AUPRC increased from 0.856 to 0.872. Modest gains, and from the few articles I have read, this is to be expected. . . Lessons learnt from Kaggle . I had a skim through the several most up-voted kernels on Kaggle. Below are the the things I found out by doing so. There is a lot for me to learn! . AUROC versus AUPRC . Many of the examples (including the most upvoted example!) use AUROC instead of AUPRC. The main reason this surprised me is that the description of the dataset recommended using AUPRC; I suppose there was an advantage to not knowing much before hand! The second reason this surprised me is that AUPRC is a more informative measure than AUROC for unbalanced data. I try to explain why. . The PRC and ROC are quite similar. They are both plots that visualise false positives against false negatives. . False negatives are measured in the same way in both plots, namely, using recall/true positive rate. Recall tells you what percentage of truly fraudulent transactions the model successfully labels as fraudulent. (And so 1 - Recall measures how many false negatives we have, as a percentage of truly fraudulent claims.) | False positive are recorded differently in the two plots. In PRC, precision is used. This is the percentage of transactions labelled as fraudulent that actually are fraudulent. Equivalently, 1-PRC is the number of false positives expressed as a percentage of claims labelled as fraudulent. | In ROC, the false-positive rate is used. This is the number of false positives expressed as a percentage of truly non-fraudulent transactions. | . | . To make this more concrete, lets put some numbers to this: . Imagine there are 100100 transactions altogether, 100 which are fraudulent and 100000 which are not. | Suppose a model predicts there are 200 fraudulent claims, and further suppose 50 of these were correct and 150 of these were incorrect. | For both PRC and ROC, the true positive measurement would be 50%: 50% of the fraudulent claims were found. | For PRC, the false positive measurement is 75%: 75% of the claims labelled as fraudulent were incorrectly labelled. | For ROC, the false positive measurement is 0.15%: only 0.15% of the non-fraudulent claims were incorrectly labelled as fraudulent. | . In short, ROC is much more forgiving of false positives than PRC, when we have highly unbalanced data. . I have also decided to plot PRC and ROC for a couple of the models in this series of posts, so you can visually see the difference. (Note that I have rotated the ROC curve to match up the variables with the PRC curve, so the comparison is easier.) . PRC and ROC for the final XGBoost model . ROC makes the model look much better than PRC does. And it is deceiving: one might look at that second chart and say we can identify 90% of fraudulent claims without many false positives. . PRC and ROC for the handmade model . Here, the effect is far more dramatic and very clearly shows how unfit AUROC is for unbalanced ata. . Under- and over-sampling . It turns out my idea from Part III, to remove non-fraudulent data, has a name: under-sampling. However, it sounds like there is an expectation that under-sampling could actually improve the performance of the models. This is surprising to me; unless you are systematially removing unrepresenative data, how can the model improve with less information?! A quick skim of the wikipedia article suggests I have not completely missed the point: ‘the reasons to use undersampling are mainly practical and related to resource costs’. . Over-sampling looks like an interesting idea, in which you create new artificial data to pad out the under-represented class. Some people on Kaggle used SMOTE, where you take two nearby points, and introduce new points directly in between these two points. Something to keep in mind for future! . Removing anomalous data . A simple idea: try to find entries in the training data that are not representative and remove them to avoid skewing the models / to avoid over-fitting. Based on my limited understanding, I think tree-based models are not sensitive to extreme data (in the same way the median is not sensitive to extreme data), so this particular idea is unlikely to have helped me improve the models for this example. However, this is another tool I will keep in mind for future projects. . Dimensionality reduction and clustering . An interesting idea: try to find a mapping of the data into a smaller dimension that preserves the clusters. The algorithm somebody used was t-SNE which is explained in this YouTube video. A couple of other algorithms used were PCA and truncated SVD. I do not yet understand how I could use this to improve the models (in the example, this was done to give a visual indication of whether frauduluent and non-frauduluent data could be distinguished). . Normalising data . Useful idea I should always keep in mind! Again, I don’t think this matters for tree-based models, but something I should keep in mind. . Outlier detection algorithms . One person used a bunch of (unsupervised?) learning algorithms: isolation forests, local outlier factor algorithm, SVM-based algorithms. More things for me to learn about! . Auto-encoders and latent representation . This person used ‘semi-supervised learning’ via auto-encoders. This was particularly interesting, especially because they had a visual showing how their auto-encoder was better at separating fraudulent and non-fraudulent data than t-SNE. This is definitely something for me to delve deeper into some time, especially because of how visually striking it is. . Visualising the features . Here and here are examples of a nice way of visualising the range of values of each feature for frauduluent and non-frauduluent data. The key thing is that they normalised the histograms, but I am not sure how they did that. Something for me to learn! . GBM vs xgboost vs lightGBM . This kernel compared three algorithms. I quite liked this because it felt historical, and helps me appreciate how the community learns. The person compared the accuracy and time taken for each of the algorithms, and also describes some new settings and options they recently discovered. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/06/25/creditcard6.html",
            "relUrl": "/python/data%20science/2020/06/25/creditcard6.html",
            "date": " • Jun 25, 2020"
        }
        
    
  
    
        ,"post26": {
            "title": "Stop and Search, Part III, Data Analysis",
            "content": "Total number of stops and searches . Grouped by ethnicity . I start by plotting the total number of stops and searches (since May 2017 because that is the earliest data of the dataset), grouped by ethnicity. . . From this chart, a simplistic conlusion would be that white people are searched significantly more than other ethnicities, so there is no racism in the system. This is clearly bad reasoning, as we need to account for the underlying population. . Including population . Population data is taken from here. I use this data to produce the following chart. Note that I grouped the various numbers together in the same way I grouped ethnicities together in producing the ethnicities column. . . Now things look bad. There is clearly a discrepancy between the population and the number of stop and searches. . To visualise this discrepancy more clearly, I decided to create a Sankey diagram using Plotly. . . The diagram makes the discrepancy quite plain to see. Black people are stopped disproportionately more than other ethnic groups. There is evidently a big problem here. . However, and unfortunately, this diagram does not tell us where exactly the problem is. Is the problem with the police or is there a deeper problem? Are the police racist for stopping black people more often, or, is this a reflection of crime rates and the underlying social issues? . Some people would look at the above diagram and wonder how this is not conclusive evidence of police racism. To illustrate the idea, consider the following two charts: . . . The majority of people would not look at these charts and conclude that the police are sexist or ageist, so one should not use the chart above for ethnicity to automatically conclude the police are racist. . To try to shed some light on the question of racism, I will take into account the outcome of the stop-and-search. . Including outcomes . The following stacked barchart shows the breakdown of outcomes for each ethnicity. . . This is not at all what I was expecting. I was expecting to find that black people would have more false stop and searches than white people. It is shocking how consistent the ratio is across ethnicities - almost suspiciously so. There is some discrepancy if you look closely, but dramatically less than what the Sankey diagram above suggested. . Conclusions . My main goal for this was to gain some better understanding of crime data, and the process of cleaning and summarising data. To my surprise, it seems from this simple analysis that police stop-and-search is not inherently racist, but there is a high chance I have not accounted for something or that my process is over-simplistic. Of course, you should refer to more authoritative sources for conclusions on these complex issues, and not base your opinions on an amateur blog. . Some key lessons I learnt: . I had to make some key decisions about how to group the data, namely, how to deal with discrepancy between officer and self defined ethnicity. In particular, it is not clear how one ought to group people of mixed race. Given how even the ratios were in the final chart, I don’t think this decision made a major difference, but it is something that I now know to consider when reading research in this area. | Dramatically different stories can be told depending on how the data is presented. This is something I already knew, but this is the first time I have experienced creating the charts for myself. With great power, comes great responsibility. | The quality of this analysis totally depends on the quality of the underlying data. I did not mention this before, but there are gaps in the data: there are some police forces who do not provide the data for every month. This does not affect my simplistic analysis, but it would matter for more nuanced analyses. | The population data is from 2011, so there will be significant errors introduced by this mis-match between the datasets. | I have to, and do, trust that the data provided is accurate. It is scary to think how easily a government could skew the data, or simply withhold it. Going through this experience lets me better understand the dystopia in 1984. | . | . Code . Here I provide sample of the code used to produce the charts. . Below is the code to produce the first bar chart. . colours_255 = [(66, 133, 244,255), (234, 67, 53,255), (251, 188, 5,255), (52, 168, 83, 255)] colours = [ tuple(n / 255 for n in colour) for colour in colours_255] plt.figure sns.barplot(x = sas_ethnicity.index, y = sas_ethnicity, order = [&#39;White&#39;, &#39;Black&#39;, &#39;Asian&#39;, &#39;Other&#39;], palette = colours) plt.grid(True, axis = &#39;y&#39;) plt.title(&#39;Stop and Searches since May 2017, by Ethnicity&#39;) plt.xlabel(&#39;Ethnicity&#39;) plt.ylabel(&#39;Number of Stop and Searches&#39;) plt.tight_layout() plt.savefig(&#39;sas3_sas_eth.png&#39;) . Here is the code to produce Sankey diagrams. . # create function that plots Sankey diagram given appropriate dataframe def create_sankey(df, title): len = df.shape[0] fig = go.Figure(data=[go.Sankey( node = dict( pad = 15, thickness = 20, line = dict(color = &quot;black&quot;, width = 0.5), label = [&#39;Proportion of Population&#39;] + list(df.index) + [&#39;Proportion of Stop and Searches&#39;], color = &quot;blue&quot; ), link = dict( source = [0]*len + list(range(1,len+1)), target = list(range(1,len+1)) + [len+1]*len, value = df.iloc[:,0].append(df.iloc[:,1]) ))]) fig.update_layout(title_text=title, font_size=15) fig.show() # create dataframe containing population and stop and search data by ethnicity sas_eth_pop = pd.DataFrame({&#39;population&#39;: population, &#39;sas&#39;: sas_ethnicity, }, index = sas_ethnicity.index) sas_eth_pop = sas_eth_pop.loc[[&#39;White&#39;, &#39;Black&#39;, &#39;Asian&#39;, &#39;Other&#39;]] sas_eth_pop.sas = sas_eth_pop.sas/sas_eth_pop.sas.sum()*100 # create sankey diagram create_sankey(sas_eth_pop, &#39;Stop and Searches by Ethnicity&#39;) . Here is the code to produce the stacked barcharts at the end: . # group data by ethnicity and outcome. sas_eth_out = pd.DataFrame(sas.groupby([&#39;ethnicity&#39;, &#39;outcome&#39;]).outcome.count()) sas_eth_out.rename(columns = {&#39;outcome&#39;: &#39;frequency&#39;}, inplace = True) sas_eth_out.reset_index(inplace = True) # convert frequencies into percentages sas_eth_total = sas_eth_out.groupby([&#39;ethnicity&#39;]).frequency.sum() sas_eth_out[&#39;total&#39;] = sas_eth_out.ethnicity.map(lambda eth: sas_eth_total[eth]) sas_eth_out[&#39;percentage&#39;] = sas_eth_out.frequency / sas_eth_out.total * 100 # pivot table, and re-order the rows sas_new = pd.pivot_table(sas_eth_out, values = &#39;percentage&#39;, columns = &#39;outcome&#39;, index = &#39;ethnicity&#39;) sas_new = sas_new.loc[[&#39;White&#39;, &#39;Black&#39;, &#39;Asian&#39;, &#39;Other&#39;]] # plot the graph sas_new.plot.bar(stacked = True) plt.xlabel(&#39;Ethnicity&#39;) plt.ylabel(&#39;Percent of Stop and Searches&#39;) plt.title(&#39;Breakdown of Outcomes of Stop and Searches&#39;) plt.legend(labels = [&#39;False / no further action&#39;, &#39;Minor further action&#39;, &#39;Major further action&#39;], loc=&#39;center left&#39;, bbox_to_anchor=(1, 0.5)) plt.tight_layout() plt.savefig(&#39;sas3_outcome.png&#39;) .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/06/22/sas3.html",
            "relUrl": "/python/data%20science/2020/06/22/sas3.html",
            "date": " • Jun 22, 2020"
        }
        
    
  
    
        ,"post27": {
            "title": "Stop and Search, Part II, Data Cleaning",
            "content": "Other posts in series . Stop and Search, Part III, Data Analysis . | Stop and Search, Part I, Data Collection . | . The cleaning . I cleaned each column, one by one. Note I call the original frame sas and created a copy sas_clean in which I would do the cleaning. . To see a list of all the columns, I ran the code sas.columns. | To investigate the distribution of values in a column (before and after cleaning), I would use the code sas_clean.column.value_counts(dropna = False) | Some columns required no cleaning: age_range | gender | location.latitude and location.longitude (except I renamed thse columns) | force | month | . | The other columns did require some cleaning: self_defined_ethnicity and officer_defined_ethnicity | type | outcome | . | . Ethnicity . officer_defined_ethnicity was mostly clean. The distribution of values were: . White 608092 Black 259504 Asian 139531 NaN 91601 Other 31845 Mixed 2563 . The only change I made was to combine mixed with other. . self_defined_ethnicity was less clean, and the distribution of values were: . White - English/Welsh/Scottish/Northern Irish/British 475167 Other ethnic group - Not stated 154802 White - Any other White background 85322 Black/African/Caribbean/Black British - Any other Black/African/Caribbean background 78340 Black/African/Caribbean/Black British - African 66072 Black/African/Caribbean/Black British - Caribbean 49736 Asian/Asian British - Any other Asian background 44517 NaN 41149 Asian/Asian British - Pakistani 33907 Asian/Asian British - Bangladeshi 24128 Other ethnic group - Any other ethnic group 16449 Mixed/Multiple ethnic groups - Any other Mixed/Multiple ethnic background 15560 Asian/Asian British - Indian 14929 Mixed/Multiple ethnic groups - White and Black Caribbean 14063 White - Irish 7843 Mixed/Multiple ethnic groups - White and Black African 4246 Mixed/Multiple ethnic groups - White and Asian 3598 White - Gypsy or Irish Traveller 1689 Asian/Asian British - Chinese 1476 Other ethnic group - Arab 143 . I decided to group these up according to the same categories used in officer_defined_ethnicity. This was done using .replace: . def simplify_eth(ethnicity): if isinstance(ethnicity, float) or &#39;Not stated&#39; in ethnicity: return np.nan elif &#39;Other&#39; in ethnicity or &#39;Mixed&#39; in ethnicity: return &#39;Other&#39; elif &#39;Asian&#39; in ethnicity: return &#39;Asian&#39; elif &#39;Black&#39; in ethnicity: return &#39;Black&#39; elif &#39;White&#39; in ethnicity: return &#39;White&#39; ethnicities = {eth: simplify_eth(eth) for eth in sas.self_defined_ethnicity.unique()} sas_clean = sas_clean.replace(to_replace = ethnicities) . Finally, I wanted to create a column ethnicity that combines these two columns. I started by renaming the other two columns, creating the new column, and filling it in with values where there is no disagreement between the officer defined and self defined ethnicity. . sas_clean.rename(columns = {&#39;self_defined_ethnicity&#39;: &#39;self&#39;, &#39;officer_defined_ethnicity&#39;: &#39;officer&#39;}, inplace = True) sas_clean[&#39;ethnicity&#39;] = np.nan # if officer and self agree, set ethnicity to either. indices = (sas_clean.officer == sas_clean.self) sas_clean.loc[indices, &#39;ethnicity&#39;] = sas_clean.officer[indices] # if officer is null, set ethnicity to self, and vice versa indices = (sas_clean.officer.isnull()) sas_clean.loc[indices, &#39;ethnicity&#39;] = sas_clean.self[indices] indices = (sas_clean.self.isnull()) sas_clean.loc[indices, &#39;ethnicity&#39;] = sas_clean.officer[indices] . I created a column conflicted to list all the cases where the stated ethnicity differs: . sas_clean[&#39;conflicted&#39;] = np.nan indices = (sas_clean.officer != sas_clean.self) &amp; (sas_clean.officer.notna()) &amp; (sas_clean.self.notna()) sas_clean.loc[indices, &#39;conflicted&#39;] = sas_clean.officer[indices] + &#39;_&#39; + sas_clean.self[indices] sas_clean.conflicted.value_counts() . The output was: . Black_Other 18774 White_Other 12423 Asian_Other 6240 Other_Asian 5319 Other_White 4243 Black_White 2924 Asian_White 2394 White_Asian 2027 Black_Asian 1990 White_Black 1935 Other_Black 1764 Asian_Black 1577 . To decide how to deal with this, I went back into the original self_defined_ethnicity to determine what the appropriate label ought to be. . for i in sas_clean.conflicted.unique(): print(i) indices = (sas_clean.conflicted == i) print(sas.loc[indices, &#39;self_defined_ethnicity&#39;].value_counts()) print() . A sample of the output is: . Black_Other Mixed/Multiple ethnic groups - White and Black Caribbean 9158 Mixed/Multiple ethnic groups - Any other Mixed/Multiple ethnic background 4891 Mixed/Multiple ethnic groups - White and Black African 2689 Other ethnic group - Any other ethnic group 1835 Mixed/Multiple ethnic groups - White and Asian 194 White_Black Black/African/Caribbean/Black British - Any other Black/African/Caribbean background 815 Black/African/Caribbean/Black British - African 633 Black/African/Caribbean/Black British - Caribbean 487 Other_Black Black/African/Caribbean/Black British - African 819 Black/African/Caribbean/Black British - Any other Black/African/Caribbean background 750 Black/African/Caribbean/Black British - Caribbean 195 . Deciding how to deal with these cases was the trickiest part of the cleaning. First, this is a sensitive issue and it feels wrong for me to decide how people should be labelled. Second, there is clearly no ‘right’ answer here, and I have to use my judgement. In the end, for most cases, I chose the self_defined_ethnicity. However, the two big exceptions were when the officer identified the person as Black or White but the person identified themselves as mixed. There were 30000 such cases. If I added them to the ‘Other’ category, this would grossly skew the numbers and misrepresent the situation, so I decided to assign these Black and White (respectively). Different people will make different judgements on this, and I suppose this is one way our own biases can creep into the data analysis. . In the end, the distribution of ethnicities in this new column is: . White 645261 Black 269286 Asian 143468 NaN 40383 Other 34738 . Type . There are 3 types of stop-and-search: . Person search 861870 Person and Vehicle search 246976 Vehicle search 24290 . Theoretically, a vehicle search does not involve any people, and thus should not have any ethnicity attached to it. However, a quick query shows this is not the case: . sas_clean.loc[(sas_clean.type == &#39;Vehicle search&#39;), &#39;ethnicity&#39;].value_counts(dropna = False) NaN 20762 White 1796 Black 843 Asian 619 Other 270 . This gives some indication of how much inherent noise there is in the data. Given the numbers are relatively small, I did not worry about ignoring these entries and so just removed all Vehicle search entries. . sas_clean = sas_clean[sas_clean.type != &#39;Vehicle search&#39;] . Outcome . Last, I cleaned the outcome column. The distribution of values were: . A no further action disposal 675585 Arrest 126538 False 115566 Community resolution 50906 Suspect arrested 30616 Khat or Cannabis warning 26428 NaN 22090 Summons / charged by post 16366 Penalty Notice for Disorder 13617 Offender given drugs possession warning 12717 Local resolution 4709 Caution (simple or conditional) 4520 Suspect summonsed to court 2941 Offender given penalty notice 2802 Article found - Detailed outcome unavailable 2651 Offender cautioned 778 Suspected psychoactive substances seized - No further action 16 . I decided to replace these with a numerical value, where 0 represents that the stop-and-search discovered nothing inappropriate, 1 represents a minor infringement with minimal action and 2 represents a major infringement with significant action. . replacements = {&#39;A no further action disposal&#39;: 0, &#39;Arrest&#39;: 2, &#39;False&#39;: 0, &#39;Community resolution&#39;: 1, &#39;Suspect arrested&#39;: 2, &#39;Khat or Cannabis warning&#39;: 1, &#39;Summons / charged by post&#39;: 2, &#39;Penalty Notice for Disorder&#39;: 2, &#39;Offender given drugs possession warning&#39;: 1, &#39;Local resolution&#39;: 1, &#39;Caution (simple or conditional)&#39;: 1, &#39;Suspect summonsed to court&#39;: 2, &#39;Offender given penalty notice&#39;: 2, &#39;Article found - Detailed outcome unavailable&#39;: 1, &#39;Offender cautioned&#39;: 1, &#39;Suspected psychoactive substances seized - No further action&#39;: 1 } sas_clean[&#39;outcome&#39;] = sas_clean.outcome.replace(to_replace = replacements) sas_clean.outcome.value_counts(dropna = False) . The final distribution of values for outcomes is as follows: . 0.0 791151 2.0 192880 1.0 102725 NaN 22090 . It is surprising the the majority of stop-and-searches amount to nothing. It makes me wonder what the reasons for this are, and if there is a more efficient means of detecting the actual crimes with fewer false positives. . Conclusion and thoughts . That is end of the cleaning and tomorrow I will try to illustrate the patterns in the data with appropriate charts. . The two main lessons: . Data is inherently noisy, and one should not treat data as objective truth. (Though, it is the closest we have got!) | A data scientist has significant power to adjust the story, by grouping and cleaning the data differently. It seems that good practice is to be open about how you processed the data and to check how different choices affect the final results. | .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/06/17/sas2.html",
            "relUrl": "/python/data%20science/2020/06/17/sas2.html",
            "date": " • Jun 17, 2020"
        }
        
    
  
    
        ,"post28": {
            "title": "Do students do their homework last minute?",
            "content": "Introduction . In the STEM Foundation Year at the University of Leicester, we used the e-assessement system Numbas. This system recorded enough information for me to be able to investigate when students did their homework. . To help understand the charts that will come below, it will help to know how the assessment was structured. . There were two semesters. In Semester 1, we taught Physics 1, Physics 2 and Maths 1. | In Semester 2, we taught Physics 3, Physics 4 and Maths 2. | . | The physics modules were structured as follows: Each module lasted half a semester. | Each module had 4 weekly e-assessments, made available on Monday and had a deadline of 10am on the Monday after. | Various other assessments whose details do not matter. | . | The maths modules were structured as follows: Each module lasted a whole semester. | Each module had 8 weekly e-assessments, made available each Monday. | For Maths 1, there was a single deadline which was the weekend before exams. | For Maths 2, there were weekly deadlines like in Physics. | . | . The reason for the inconsistent structure is because we were trying to find out what works. . Collecting and cleaning the data . As mentioned above, we used the Numbas e-assessment system which recorded lots of data, including when students attempted their work. A colleague was in charge of maintaining the system, so I asked them to extract the data for this little project. They did this using a SQL query, and passed the data to me as a csv file. . I then cleaned the data. This involved: . Removing entries that did not correspond to my students. | Determining which assessment each entry corresponded to. This was trickier than expected, because the staff on our team used different naming conventions and because the system produced three different ids for each assessment. | Deciding how to deal with fact that we allowed students to attempt the assessment multiple times. In the end, I decided to pick the first attempt out of all these; it had negligible impact on the final charts. | Deciding how to deal with fact that a student could start an assessment on one day, but finish it later. I decided to pick the time a student first opened an assessment, which I called ‘Start Time’. | . On the technical side, I used R. . I learnt some R by working through R for Data Science, which is an excellent online book that I highly recommended. | For this project, the key tools I used were tibbles, piping and ggplot2. | The other noteworthy technical aspect of this project was getting the x-axis, representing time, to appear just as I wanted. I remember this took significant effort, banging my head over the table to understand POSIXct and POSIClt. | . The charts . Below are the charts for the Physics modules. The x-axis shows the day a student opened an e-assessment and the y-axis shows the number of student who started on each day. The different colours correspond to the different assessments. . . Physics 1, 2 and 4 all have the same patterns. . A good chunk of students open the e-assessment before the weekend. | The modal day to open the e-assessment is Sunday, the day before the deadline. | Several students open the e-assessment on Monday (so after midnight on Sunday). | The bars are shorter in the Physics 2 and Physics 4 charts because fewer students do those modules. | . Physics 3 has a different pattern. The first assessment has the same shape as in the other three modules. The other three assessments are flat for a few weeks and then all bunch up in the week beginning Monday 11th Feb. The reason is that at the end of the first week of Physics 3, we extended the deadline for all the assessments to 10am on Monday 18th Feb. (We did this to account for unforeseen circumstances). . Below are a sample of charts showing the breakdown of timings during Sunday and Monday. . . I do not think there is anything particularly noteworthy in these charts. The main pattern is that most people who started the work on Sunday did so after 6pm. The thing which struck me was that for each assessment, there were several students who started the work between 3am and 9am. . As a result of this data, the director of the Foundation Year decided to change the deadlines from 10am on Monday to 10pm on Sunday. . Below are the charts for the two maths modules. . . Recall that in Maths 1, there was a single deadline for all the assessments, which was the weekend before exam week. . In the first half of the semester, there is a decent chunk of students starting the e-assessments. | In the second half, engagement drops significantly. My explanation for this is that the e-assessments for Physics 2 were considerably longer/harder than those of Physics 1, but there are likely various factors. | A lot of work was done over the Christmas break. To my surprise, a few students left all the work to be done on the final weekend! | . Recall that Maths 2 had weekly deadlines. Recall also that Maths 2 runs concurrently with Physics 3 and Physics 4. . When we extended the deadline in Physics 3, we also had to do it for Maths 2. | Like in Physics 4, the deadlines for second half of Maths 2 were weekly. | Hence, the first half of Maths 2 resembles Physics 3, and the second half of Maths 2 resembles Physics 4. | . Conclusions . Many people who see this will say ‘This is obvious, what is the point?’. There are two main points. . First, it is good to have quantitative data. It provides clearer understanding and also allows us to measure changes from one year to the next. | Second, the higher education industry puts too little weight on (appropriate) data and observations. Either a lecturer simply does not care about teaching (in which case they put no weight on anything) or a lecture does care but bases their decisions on an imagined conception of what students are. | . What conclusions did I draw from this? . The pattern for weekly deadlines is consistent across the year: there is some activity throughout the week, with a clear peak the day before the deadline. One consequence is that we cannot assume comfort with material taught on Monday during a session later in the week, e.g., on Thursday. | . | Students respond to incentives, just like the rest of us. Our choices have a big impact on student habits. | Noteworthy to point out that most students do know the deadlines! This means we are communicating our deadlines well. | Thinking about incentives is important more generally. E.g. it explains the difference between attendance in lectures and attendance in assessed sessions. | . | These findings are particularly important for ‘linear’ subjects, where knowledge/understanding of Week 1 material is required to learn Week 2 material. | Shouldn’t judge students or label them as ‘bad students’. Better to label the habit, not the individual. | This is more to do with human nature, than students in particular. | This is mostly about incentives. Designing a course well includes creating incentives which result in good learning behaviours. (Compare with the famous example of opting-in or opting-out of a country’s organ donation registry.) | . | . Limitations of the data . There are several sources of noise and error in this data. I will say ‘data is positively biased’ to mean that data shows students working earlier than they actually are, and ‘negatively biased’ to say that data shows students are working later than they actually are. . Sources of positive bias. . Looking at Start Time. Students may open the assessment during the week, but actually only finish it on the weekend. | Students have multiple attempts on the coursework and I only looked at the start time of their earliest attempt. | I excluded students who did not attempt the coursework or attempted it late. | . Sources of negative bias. . There was a ‘Practice Version’ of each e-assessment available. Students were encouraged to use these to practice before attempting the actual assessed version. Some students did this, but a brief look at the data shows that most people did not attempt these. | Did not take into account mitigating circumstances, e.g. illness. | Does not account for other forms of independent study. E.g. a student might review lectures/workshop questions before attempting the e-assessment. | . Sources of unknown bias. . Most of our students have done A-Level Maths and/or Physics, so find the year easy. This probably means that students do not need to attempt coursework in a timely manner in order to keep up with the material. | This data only relates to specific style of coursework. There is no data on semester long projects, essays, etc. My prediction is that similar patterns will emerge, but spread out according to the size of the task. | Several students suspended or withdrew or were terminated during year. Their data will be included in early modules but not in later modules. | .",
            "url": "https://lovkush-a.github.io/blog/r/data%20science/2020/06/16/homework.html",
            "relUrl": "/r/data%20science/2020/06/16/homework.html",
            "date": " • Jun 16, 2020"
        }
        
    
  
    
        ,"post29": {
            "title": "Stop and Search, Part I, Data Collection",
            "content": "Other posts in series . Stop and Search, Part III, Data Analysis . | Stop and Search, Part II, Data Cleaning . | . Introduction . In both traditional and social media, the issue of racial discrimination within the police is a hot topic. I decided to investigate this issue and better understand the statistics that go around. . I googled ‘crime data’ and one of the top results was data.police.uk, which seems like a reliable source of data for crime in the UK. With regard to race, the only data available on this website is about ‘stop-and-search’ (as opposed to prison data, for example). . Stop and search . In the UK, a police officer has the legal authority to stop and search you if they have ‘reasonable grounds’ to suspect you’re involved in a crime, e.g. carrying an illegal item. This UK Government website provides a short and clear summary of the rules, this Scottish Government website also provides clear summary of the rules but with more detail on what counts as reasonable and how a search should be conducted, and finally here is the actual legislation, which is predictably written in unclear legalese. . Downloading the data . I will only describe the final and clean code used to obtain the information I wanted, after several attempts necessary to get everything correct. . First, I downloaded a JSON file listing the name and ‘id’ of each police force, stored it in a pandas dataframe, and saved it as a csv file. The id is just a shortened version of their name and is used in all the other data sources. The code to do this is: . forces_response = requests.get(&#39;https://data.police.uk/api/forces&#39;) forces_json = forces_response.json() force_df = pd.DataFrame({&#39;id&#39;:[], &#39;name&#39;: []}) for entry in forces_json: force_df.loc[force_df.shape[0]] = [entry[&#39;id&#39;], entry[&#39;name&#39;]] force_df.to_csv(&#39;force.csv&#39;) . Next I downloaded a JSON file describing for which months and for which forces stop-and-search data was available: . availability_response = requests.get(&#39;https://data.police.uk/api/crimes-street-dates&#39;) availability_json = availability_response.json() availability_df = pd.DataFrame({&#39;month&#39;:[], &#39;id&#39;: []}) for entry in availability_json: date = pd.to_datetime(entry[&#39;date&#39;], format=&#39;%Y-%m&#39;).to_period(&#39;M&#39;) for id in entry[&#39;stop-and-search&#39;]: availability_df.loc[availability_df.shape[0]] = [date, id] . I then loop through this information and download the stop-and-search data, saving the data onto my laptop. . for i in range(availability_df.shape[0]): force = availability_df.iloc[i].id month = availability_df.iloc[i].month.strftime(&#39;%Y-%m&#39;) response = requests.get(f&quot;https://data.police.uk/api/stops-force?force={force}&amp;date={month}&quot;) if response.status_code == 200: data = response.json() with open(f&#39;{month}_{force}.json&#39;, &#39;w&#39;) as f: json.dump(data, f) . I add a column to the availability dataframe to track which pieces of data were actually successfully downloaded or not. I do this by trying to open each file, and recording a fail if an error occurs while trying to open it. (While writing this paragraph, I realise I could have done this at the same time as the previous step.) . availability_df[&#39;downloaded&#39;] = True for i in range(availability_df.shape[0]): force = availability_df.iloc[i].id month = availability_df.iloc[i].month.strftime(&#39;%Y-%m&#39;) try: file = open(f&#39;{month}_{force}.json&#39;, &#39;r&#39;) file.close() except: availability_df.iloc[i,2] = False print(f&#39;{month}_{force}&#39;) availability_df.to_csv(&#39;availability.csv&#39;) . Lastly, I combine all of the data into one mega pandas dataframe, keeping only those columns that I think will be relevant to my investigations. . cols = [&#39;age_range&#39;, &#39;outcome&#39;, &#39;self_defined_ethnicity&#39;, &#39;gender&#39;, &#39;officer_defined_ethnicity&#39;, &#39;type&#39;, &#39;location.latitude&#39;, &#39;location.longitude&#39;, &#39;force&#39;, &#39;month&#39;] sas_df = pd.DataFrame({col:[] for col in cols}) for i in range(availability_df.shape[0]): if availability_df.iloc[i,2]: force = availability_df.iloc[i].id month = availability_df.iloc[i].month month_str = month.strftime(&#39;%Y-%m&#39;) file = open(f&#39;{month_str}_{force}.json&#39;, &#39;r&#39;) data = json.load(file) new = pd.json_normalize(data) new[&#39;force&#39;] = force new[&#39;month&#39;] = month sas_df = sas_df.append(new, ignore_index=True)[cols] sas_df.to_csv(&#39;sas.csv&#39;) . A chart . It would be sad for this post to have no charts whatsoever, so I quickly created one which just counts the number of stops-and-searches, grouped by ethnicity. . . One might say, ‘Look, white people are stopped more than black people, so the police are not racist.’ This is obviously simplistic. The aim of the project is to dig deeper into the data and see what patterns I can find. .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/06/15/sas1.html",
            "relUrl": "/python/data%20science/2020/06/15/sas1.html",
            "date": " • Jun 15, 2020"
        }
        
    
  
    
        ,"post30": {
            "title": "AIs for Games, Part III, Pruning Min-Max for Pentago",
            "content": "Other posts in series . AIs for Games, Part II, Min-max for Pentago . | AIs for Games, Part I, Brute Force TicTacToe . | . Performance before any optimisations . I created a few boards and timed how long it took the algorithm to run a depth-2 search on these boards, starting with both Player 0 and Player 1. An image showing the boards used is at the end of the post. The results are: . Board: 0, Player: 0, Time taken: 1.59 | Board: 0, Player: 1, Time taken: 1.58 | Board: 1, Player: 0, Time taken: 0.00117 | Board: 1, Player: 1, Time taken: 0.00103 | Board: 2, Player: 0, Time taken: 5.04 | Board: 2, Player: 1, Time taken: 4.94 | Board: 3, Player: 0, Time taken: 29.2 | Board: 3, Player: 1, Time taken: 32.0 | Board: 4, Player: 0, Time taken: 9.55 | Board: 4, Player: 1, Time taken: 9.25 | . I also tried running a depth-3 search on Board 4 for Player 1 (because Player 1 can find a winning move with this depth), but it did not finish running even after a couple of hours of running. . The optimisations . I made several optimisations. I describe them here chronologically, i.e., in the order I tried implementing them. . Alpha-beta pruning . There are situations where a board position does not need to be analysed, because based on the boards we have already analysed, this board position will definitely not be chosen in optimal player. This resulted in the following times: . Board: 0, Player: 0, Time taken: 1.34 | Board: 0, Player: 1, Time taken: 1.39 | Board: 1, Player: 0, Time taken: 0.00120 | Board: 1, Player: 1, Time taken: 0.00104 | Board: 2, Player: 0, Time taken: 5.28 | Board: 2, Player: 1, Time taken: 4.61 | Board: 3, Player: 0, Time taken: 26.1 | Board: 3, Player: 1, Time taken: 29.4 | Board: 4, Player: 0, Time taken: 8.62 | Board: 4, Player: 1, Time taken: 8.28 | . This was not a significant improvement in the times. I was surprised by this. I decided to run cProfile to try to determine why there was not a significant time-save. It seemed to be that the process of creating new boards was taking up a lot of time - and I was only pruning a board after the board was created. I needed to prune before the board was created. . To achieve this, I had to significantly re-structure the whole program, removing the frontier and instead writing the main function find_move recursively. The resulting times were a significant improvement: . Board: 0, Player: 0, Time taken: 1.07 | Board: 0, Player: 1, Time taken: 1.06 | Board: 1, Player: 0, Time taken: 0.00118 | Board: 1, Player: 1, Time taken: 0.00102 | Board: 2, Player: 0, Time taken: 1.90 | Board: 2, Player: 1, Time taken: 1.86 | Board: 3, Player: 0, Time taken: 5.33 | Board: 3, Player: 1, Time taken: 5.41 | Board: 4, Player: 0, Time taken: 2.98 | Board: 4, Player: 1, Time taken: 2.90 | . Phew! It was satisfying to see the times drop so much, and this motivated me to keep going. . Stop if winning move found . If a winning move was found in a current board position, there is no need to continue analysing this position, so can stop this early. Pruning does not detect this (at least, not how I implemented it. This could be a sign I did it wrong…), so I had to manually add this. This resulted in further improvements: . Board: 0, Player: 0, Time taken: 1.09 | Board: 0, Player: 1, Time taken: 1.11 | Board: 1, Player: 0, Time taken: 0.00104 | Board: 1, Player: 1, Time taken: 0.00119 | Board: 2, Player: 0, Time taken: 0.324 | Board: 2, Player: 1, Time taken: 0.344 | Board: 3, Player: 0, Time taken: 0.141 | Board: 3, Player: 1, Time taken: 0.897 | Board: 4, Player: 0, Time taken: 3.12 | Board: 4, Player: 1, Time taken: 2.93 | . I also tried running the depth-3 search on board 4, and to my surprise it ended in under a minute! . Board: 4, Depth 3, Time taken: 59.1 | . Note this would not be representative of a generic depth-3 search, because the winning move is found about 1/6 of the way into the full search. . Lists within lists . Running cProfile revealed that having nested lists to encode the board slows things down considerably, so I re-wrote the program so that the board was represented by a single list. I was worried this would take a lot of effort, but fortunately it consisted of replacing board[i][j] with board[6*i + j], and other similar simple changes. This halved the times: . Board: 0, Player: 0, Time taken: 0.407 | Board: 0, Player: 1, Time taken: 0.411 | Board: 1, Player: 0, Time taken: 0.000520 | Board: 1, Player: 1, Time taken: 0.000450 | Board: 2, Player: 0, Time taken: 0.175 | Board: 2, Player: 1, Time taken: 0.161 | Board: 3, Player: 0, Time taken: 0.0645 | Board: 3, Player: 1, Time taken: 0.424 | Board: 4, Player: 0, Time taken: 1.41 | Board: 4, Player: 1, Time taken: 1.41 | Board: 4, Depth 3, Time taken: 30.7 | . Checking if the game is over . Running cProfile again showed that the new bottle-neck was checking if the game had ended. This involved looping through the set of all possible winning lines, and checking to see if Player 0 or Player 1 occupied all the positions in each line. . Originally, I had one for-loop to check if Player 1 won, then another to check if Player 0 won. I changed this to have a single loop, and for each line check if Player 1 or Player 0 won. This resulted in another big chunk of time-saving. . Board: 0, Player: 0, Time taken: 0.292 | Board: 0, Player: 1, Time taken: 0.283 | Board: 1, Player: 0, Time taken: 0.000499 | Board: 1, Player: 1, Time taken: 0.000334 | Board: 2, Player: 0, Time taken: 0.101 | Board: 2, Player: 1, Time taken: 0.107 | Board: 3, Player: 0, Time taken: 0.0359 | Board: 3, Player: 1, Time taken: 0.261 | Board: 4, Player: 0, Time taken: 0.943 | Board: 4, Player: 1, Time taken: 0.915 | Board: 4, Depth 3, Time taken: 21.6 | . Checking cProfile showed that I had halved the time to check if the game had ended, but it was still the biggest bottle neck. . I then tried to re-design the program to cut down further, but to no avail. For example, I tried to group the set of lines into groups that could be ruled out together, e.g. if I know that position (2,2) in the board is empty, that rules out 7 of the lines. It will be interesting to know if there is a more efficient way to check if the game has ended! . Tidying up and fixing a “bug” . My code was becoming untidy (I was not using version control properly, and instead was creating multiple versions of functions in the same file) so I tidied up all the code. While doing this, I discovered that I did not correctly update the prune function during the ‘Lists within lists’ step: I was only pruning boards of at least depth 2, when it could be pruning board of depth 1. I made the necessary tweak, resulting in the following times: . Board: 0, Player: 0, Time taken: 0.0896 | Board: 0, Player: 1, Time taken: 0.0682 | Board: 1, Player: 0, Time taken: 0.000272 | Board: 1, Player: 1, Time taken: 0.000204 | Board: 2, Player: 0, Time taken: 0.100 | Board: 2, Player: 1, Time taken: 0.0539 | Board: 3, Player: 0, Time taken: 0.313 | Board: 3, Player: 1, Time taken: 1.13 | Board: 4, Player: 0, Time taken: 0.204 | Board: 4, Player: 1, Time taken: 0.310 | Board: 4, Depth 3, Time taken: 3.18 | . Woohoo! What big progress. What used to take hours now only takes 3 seconds. . Duplicate board positions . The last thing I wanted to try was dealing with repeat positions. Previously I only skipped these if the same position occurred and they had same parent. But now I wanted to have a way of skipping board positions regardless of where they were in the game-tree. This took many hours to get correct, because my first attempt caused the algorithm to produce sub-optimal moves, and I had no idea why. . The error was that when I pruned a board, I would finalise the board’s value, though the board was not fully analysed. Then, when the board occurred somewhere else in the tree, I would use this incomplete value and miss out all the analysis that was pruned the first time around. . After fixing the bug, the new times are: . Board: 0, Player: 0, Time taken: 0.2888009548187256 | Board: 0, Player: 1, Time taken: 0.15045881271362305 | Board: 1, Player: 0, Time taken: 0.00028705596923828125 | Board: 1, Player: 1, Time taken: 0.0002300739288330078 | Board: 2, Player: 0, Time taken: 0.0822603702545166 | Board: 2, Player: 1, Time taken: 0.03888416290283203 | Board: 3, Player: 0, Time taken: 0.30017614364624023 | Board: 3, Player: 1, Time taken: 0.7593698501586914 | Board: 4, Player: 0, Time taken: 0.3503570556640625 | Board: 4, Player: 1, Time taken: 0.3380570411682129 | Board: 4, Depth 3, Time taken: 3.19 | . The times are not always better, and some are worse. . Next steps and final thoughts . The next step is to introduce neural networks. A brief google search reveals that min-max is not appropriate and that I should have been using reinforcement learning. Doh! In the back of my mind, I was curious as to how the neural network could learn the heuristic function; what would the loss/error be that it would minimise? . Though the optimisation of the min-max algorithm is incomplete (e.g. I do not understand why the latest version is not faster than the previous version), I will end it here. This is because I have already spent a couple of days on this, I have already learnt from this, and it is not necessary for the bigger goal of developing a neural network. . Some final takeaways: . I should have sketched out a plan of the whole project. Though I had basic knowledge of neural networks, I should have researched a bit more and found out that min-max is not appropriate for neural networks. | Be more thorough with testing. It makes spotting bugs easier and quicker. | Seed random number generators. I used used random heuristics (to see effects of pruning), but I did not seed them. This means the times above are not fair comparisons, as some random numbers could have lead to more pruning than others. | Use proper version control. My code got hideous at one point. At least now I have a better sense of the workflow of git. | . The code . The code, at this stage of project, can be found on github. . The boards used for testing . .",
            "url": "https://lovkush-a.github.io/blog/ai/python/2020/06/09/games3.html",
            "relUrl": "/ai/python/2020/06/09/games3.html",
            "date": " • Jun 9, 2020"
        }
        
    
  
    
        ,"post31": {
            "title": "AIs for Games, Part II, Min-max for Pentago",
            "content": "Other posts in series . AIs for Games, Part III, Pruning Min-Max for Pentago . | AIs for Games, Part I, Brute Force TicTacToe . | . The algorithm . In my last post, I said I wanted to code up an alpha-beta pruning algorithm. (See this CS50 Intro to AI lecture for background on tree-based search and alpha-beta pruning). Over the past couple of weeks, I have been thinking about exactly how the algorithm would work and how I would code it up, and it was surprisingly tricky. I therefore decided to just focus on creating an algorithm that would search through a game-tree up to some maximum depth, but in a way that I could add in the pruning. . The algorithm should determine the ‘value’ of the current board state and the move that would achieve that value. A value of 1 means that Player 1 will win (with perfect play) and a value of 0 means that Player 0 will win (with perfect play). A value in the middle indicates which player is more likely to win, as judged by the algorithm. . The general idea of the algorithm is straightforward: . Given a board position, create an initial node and add it to the frontier. | While the initial node does not have a value, pick a node from the frontier and do the following: Check if the game has ended. If so, determine who won, and set the value of the node appropriately. Then update the value of parent nodes appropriately. | Check if the depth of the node is the maximum depth. If so, then estimate the value of the position. For now, I just set this as 0.5, but in future, this will be determined via a neural network. Then update the value of parent nodes appropriately. | Create a list of legal moves and possible board positions arising from this node. Create new nodes and add them to the frontier. | . | Once the initial node has a value, pick a move whose resulting board position has the same value. | The tricky part was the step ‘update the value of the parent nodes appropriately’. It took me some time to flesh out all the details and determine exactly when a parent node should have its value updated. I had to do this in a way so that I could add on the pruning later without having to change the structure of the code. The main ideas were: . Whenever a node has its value determined, the upper or lower bounds of its parent’s node, and only its parent’s node, needs to be updated. . | Whenever all of a node’s children’s values are determined, the node’s value can be determined. This will sometimes lead to some recursive updating of node values. . | . A big sticking point for me was how to decide when to prune a node: it felt like I needed knowledge of uncle/aunt nodes to do this, but following the ideas above, the grandparent node and parent node should contain enough information to decide if a node can be pruned or not. . In the end, I managed to get it altogether. The code, at this stage of the project, can be found on github. . An example . . The image above shows an example winning position for player 1. If it is Player 1’s move, the algorithm finds a winning move using a depth-1 search (play in bottom left, and then rotate bottom left clockwise, giving 5-in-a-row column on left-hand-side). If it is Player 2’s move, the algorithm returns None using a depth-2 search, because no matter what 2 does on this turn, 1 will always win. . The code . The code, at this stage of project, can be found on github. . Areas of improvement . The algorithm is highly inefficient. It takes roughly 10-50 seconds to do depth-2 searches, and on the order of hours for depth-3 search. This is way too long! The number of possible positions after 3 moves is roughly a few million, so that shouldn’t take hours to sort through. . There are many inefficiencies I am aware of and will fix them for my next post. Examples include: . Dealing with repeat positions. Right now, I only avoid positions that repeat if they arise from the same parent. | I currently use a list to track which positions have been visited in the search, to check for repeats. This is less efficient than a set, but I can’t use arrays in sets and all my boards are coded as arrays. I will try changing everything to tuples. | If a winning/losing sequence is found, it will keep on searching. This is not necessarily a bad thing, because we might want to analyse all the lines, but it definitely slows things down. | Not using alpha-beta pruning, yet. | Not doing any time analysis. I will run cProfile to systematically find inefficiencies. | .",
            "url": "https://lovkush-a.github.io/blog/ai/python/2020/06/04/games2.html",
            "relUrl": "/ai/python/2020/06/04/games2.html",
            "date": " • Jun 4, 2020"
        }
        
    
  
    
        ,"post32": {
            "title": "Investigating Credit Card Fraud, Part V, Final Models",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle . | Investigating Credit Card Fraud, Part IV, n_estimators . | Investigating Credit Card Fraud, Part III, Handmade Model . | Investigating Credit Card Fraud, Part II, Removing data . | Investigating Credit Card Fraud, Part I, First Models . | . Forest model, hyper-parameter selection . I tidied up the code from yesterday to allow me to optimise for more than one parameter at once. For each combination of hyper-parameters, I obtained 20 different AUCs (by using five 4-fold cross validations). The results were stored in a pandas dataframe. The code for this is at the bottom of the page. . I then averaged over all the folds and sorted the results. The code for this and the output is below. . auc_forest.max_depth.fillna(value = 0, inplace = True) auc_forest_mean = auc_forest.groupby([&#39;n_estimators&#39;, &#39;max_depth&#39;, &#39;max_features&#39;]).auc.mean() auc_forest_mean.sort_values(ascending = False).head(20) n_estimators max_depth max_features auc 50.0 0.0 10.0 0.774015 50.0 10.0 0.774015 60.0 0.0 10.0 0.772589 50.0 10.0 0.772589 10.0 10.0 0.772573 50.0 10.0 10.0 0.772328 40.0 10.0 10.0 0.771290 80.0 0.0 10.0 0.771108 50.0 10.0 0.771108 40.0 0.0 10.0 0.770744 50.0 10.0 0.770744 50.0 0.0 7.0 0.770522 50.0 7.0 0.770522 80.0 10.0 10.0 0.770487 50.0 10.0 7.0 0.770472 60.0 50.0 7.0 0.770472 0.0 7.0 0.770472 10.0 7.0 0.770025 40.0 50.0 5.0 0.769278 auto 0.769278 . A few things were found by doing this: . The best options for the hyper-parameters are n_estimators = 50, max_depth = None and max_features = 10. | max_depth = None and max_depth = 50 produced the same models. This means that maximum depth achieved without any limits is less than 50. | max_features = auto and max_features = 5 produced the same models. This is obvious in retrospect: auto means taking the square root of the number of features, and we had about 30 features. | . Forest model, final model . Using these hyper-parameters, I created a the final Random Forest model. The precision-recall curve is below: . . For comparison, the very first random forest model is also included. As can be seen, there is an improvement but a seemingly minimal one. Based on examples I have seen elsewhere, these minor improvements are what can be expected from hyper-parameter optimisations. . XGBoost model . I repeated the process above for XGBoost models. The best parameter settings were as follows: . n_estimators max_depth learning_rate auc 50.0 5.0 0.05 0.761125 100.0 5.0 0.02 0.760002 50.0 10.0 0.05 0.759094 15.0 0.05 0.758146 100.0 10.0 0.02 0.757185 15.0 0.02 0.756748 200.0 10.0 0.02 0.747032 15.0 0.02 0.743830 50.0 15.0 0.10 0.742954 10.0 0.10 0.739922 100.0 10.0 0.05 0.737840 15.0 0.05 0.737013 50.0 10.0 0.02 0.729299 15.0 0.02 0.729239 5.0 0.02 0.729049 200.0 5.0 0.02 0.727433 50.0 15.0 0.30 0.726696 5.0 0.20 0.726479 100.0 5.0 0.20 0.724851 15.0 0.30 0.722728 . Using the settings from the top row, I created my final model, whose precision-recall curve is below. I have included the original curve, too. . . !! After doing the optimisations, the model became worse! The AUC decreased by 0.002. The explanation for this must be that removing 99% of the data actually changes the behaviour of the model. . I re-did the process but only removing 90% of the data (recall from Part II that in XGBoost, removing 90% of the data did not decrease performance that much). This time, the optimal settings were as follows: . n_estimators max_depth learning_rate auc 200.0 10.0 0.10 0.816130 5.0 0.10 0.815648 100.0 5.0 0.10 0.807745 10.0 0.10 0.806940 200.0 10.0 0.05 0.805212 5.0 0.05 0.801478 50.0 10.0 0.10 0.797015 5.0 0.10 0.794567 100.0 5.0 0.05 0.793189 10.0 0.05 0.792732 200.0 5.0 0.02 0.785652 10.0 0.02 0.783957 50.0 5.0 0.05 0.779087 10.0 0.05 0.778968 100.0 5.0 0.02 0.776565 10.0 0.02 0.775092 50.0 5.0 0.02 0.761190 10.0 0.02 0.760388 . The optimal parameters changed (thankfully!). I then re-created the final model and this time there was an improvement: . . Next time . My next blog post will be the final one in this series. I will summarise what I have done and what I have learnt. I will also have a look at what others did and see what I can learn from them. . The code . The code is provided for the Random Forest optimisation. The code for XGBoost is similar. . # import modules import numpy as np import pandas as pd from sklearn.model_selection import train_test_split, KFold from sklearn.metrics import precision_recall_curve from sklearn.metrics import auc from matplotlib import pyplot as plt import seaborn as sns from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier import itertools #import data data = pd.read_csv(&quot;creditcard.csv&quot;) y = data.Class X = data.drop([&#39;Class&#39;, &#39;Time&#39;], axis = 1) #create train-valid versus test split Xtv, X_test, ytv, y_test = train_test_split(X,y, random_state=0, test_size=0.2) # create function which takes model and data # returns auc def auc_model(model, Xt, Xv, yt, yv): model.fit(Xt,yt) preds = model.predict_proba(Xv) preds = preds[:,1] precision, recall, _ = precision_recall_curve(yv, preds) auc_current = auc(recall, precision) return auc_current # create options for hyperparameter n_estimators = [40, 50, 60, 80] max_depth = [None, 5, 10, 50] max_features = [&#39;auto&#39;, 3,5,7,10] random_state = range(5) # create frame to store auc data auc_forest = pd.DataFrame({&#39;n_estimators&#39;: [], &#39;max_depth&#39;: [], &#39;max_features&#39;: [], &#39;fold&#39;: [], &#39;auc&#39;: [] }) # loop through hyper parameter space for n, md, mf, rs in itertools.product(n_estimators, max_depth, max_features, random_state): kf = KFold(n_splits = 4, shuffle = True, random_state = rs) model = RandomForestClassifier(n_estimators = n, max_depth = md, max_features = mf, random_state = 0) i=0 for train, valid in kf.split(Xtv): Xt, Xv, yt, yv = Xtv.iloc[train], Xtv.iloc[valid], ytv.iloc[train], ytv.iloc[valid] # remove 99% of the non-fraudulent claims from training data to speed up fitting selection = (Xt.index % 100 == 1) | (yt == 1) Xt_reduced = Xt[selection] yt_reduced = yt[selection] auc_current = auc_model(model, Xt_reduced, Xv, yt_reduced, yv) auc_forest.loc[auc_forest.shape[0]] = [n, md, mf, 4*rs+i, auc_current] i+=1 .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/05/30/creditcard5.html",
            "relUrl": "/python/data%20science/2020/05/30/creditcard5.html",
            "date": " • May 30, 2020"
        }
        
    
  
    
        ,"post33": {
            "title": "Investigating Credit Card Fraud, Part IV, `n_estimators`",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle . | Investigating Credit Card Fraud, Part V, Final Models . | Investigating Credit Card Fraud, Part III, Handmade Model . | Investigating Credit Card Fraud, Part II, Removing data . | Investigating Credit Card Fraud, Part I, First Models . | . First attempt . I used a k-fold cross validation with 4 folds to determine what is a good number of estimators for the Random Forest model. The code to do this is at the bottom. The table below shows the AUC metrics obtained. . Fold n_estimators = 10 n_estimators = 50 n_estimators = 100 n_estimators = 200 n_estimators = 500 . 0 | 0.765 | 0.793 | 0.775 | 0.770 | 0.756 | . 1 | 0.683 | 0.690 | 0.691 | 0.680 | 0.664 | . 2 | 0.766 | 0.783 | 0.781 | 0.784 | 0.774 | . 3 | 0.815 | 0.841 | 0.838 | 0.833 | 0.826 | . From this table, we can see that the AUC depends a lot more on the fold rather than the hyper-parameter. I was surprised at how much the AUC could vary, depending on how the data was chopped up. Nevertheless, it is still clear that the optimal choice for the number of estimators is either 50 or 100. However, it is hard to judge if 50 is definitely better than the default of 100; it is better in 3 out of the 4 folds but maybe this was just a fluke. . I wanted to better understand how the AUC depends on the folds, and make a better decision about which hyper-parameter is better, so I decided to repeat this process many times and see the resulting patterns. . Second attempt . I repeated the first attempt 20 times and stored the results in a pandas dataframe. I then plotted scatterplots and histograms to visualise the patterns. In each of them, I compared the performance against the default of 100 estimators. As always, the code for this is at the bottom. . n_estimators=10 . . The histogram shows that the distribution of AUC values when the number of estimators is 10 is worse than the default values. The scatterplot shows the default setting has a better AUC on the majority of folds - but not every time! . n_estimators=50 . . The histograms almost perfectly overlap! But we do see a little extra blue on the right and extra orange on the left which means n=50 is better. The scatterplot makes this clearer, showing that having 50 estimators produces larger AUC in most of the folds. . n_estimators=200 and n_estimators=500 . . . From these charts, we see that as we increase the number of estimators beyond 100, the model performs worse. Though we can see this in the table in the first attempt, these charts make it much clearer. . Final thoughts . Visualisations are nice! Though the first k-fold validation gave the same conclusions as twenty k-fold validations, the latter is far more convincing and enlightening. In addition to being more certain that n=50 is a superior choice, I have gained knowledge about how much the AUC can vary as the data varies. . Furthermore, the idea of removing data to speed up the fitting (from Part II of the series) really paid off. Generating these charts required 320 fittings altogether. Without removing the data, this would have taken multiple days, so I would never have done it. . Next time, I will complete the hyper-parameter optimisations and present my final models. . Code for first attempt . #create train-valid versus test split Xtv, X_test, ytv, y_test = train_test_split(X,y, random_state=0, test_size=0.2) #create KFold object kf = KFold(n_splits = 4, shuffle = True, random_state = 0) #create function to determine auc given the data def auc_model(model, Xt, Xv, yt, yv): model.fit(Xt,yt) preds = model.predict_proba(Xv) preds = preds[:,1] precision, recall, _ = precision_recall_curve(yv, preds) auc_current = auc(recall, precision) return auc_current # create list of n_estimators for RandomForest n_estimators = [10, 50, 100, 200, 500] # create variable to store aucs aucs = np.zeros([5,4]) # loop through hyper-parameter values and folds i=0 for n_estimator in n_estimators: j = 0 model = RandomForestClassifier(n_estimators = n_estimator, random_state = 0) for train, valid in kf.split(Xtv): Xt, Xv, yt, yv = Xtv.iloc[train], Xtv.iloc[valid], ytv.iloc[train], ytv.iloc[valid] # remove 99% of the non-fraudulent claims from training data to speed up fitting selection = (Xt.index % 100 == 1) | (yt == 1) Xt_reduced = Xt[selection] yt_reduced = yt[selection] auc_current = auc_model(model, Xt_reduced, Xv, yt_reduced, yv) aucs[i][j] = auc_current j += 1 i += 1 . Code for second attempt . # create list of n_estimators for RandomForest n_estimators = [10, 50, 100, 200, 500] # create variables to store auc data aucs = np.zeros([5,4]) auc_df = pd.DataFrame({&#39;n_estimators_&#39;+str(n_estimators[i]): [] for i in range(len(n_estimators))}) # create 20 different KFolds, so we get 80 models for each value of hyperparameter for random_state in range(20): kf = KFold(n_splits = 4, shuffle = True, random_state = random_state) i=0 for n_estimator in n_estimators: j = 0 model = RandomForestClassifier(n_estimators = n_estimator, random_state = 0) for train, valid in kf.split(Xtv): Xt, Xv, yt, yv = Xtv.iloc[train], Xtv.iloc[valid], ytv.iloc[train], ytv.iloc[valid] # remove 99% of the non-fraudulent claims from training data to speed up fitting selection = (Xt.index % 100 == 1) | (yt == 1) Xt_reduced = Xt[selection] yt_reduced = yt[selection] auc_current = auc_model(model, Xt_reduced, Xv, yt_reduced, yv) aucs[i][j] = auc_current j += 1 i += 1 # update dataframe auc_df with latest batch of aucs for j in range(4): auc_df.loc[auc_df.shape[0]] = aucs[:,j] .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/05/29/creditcard4.html",
            "relUrl": "/python/data%20science/2020/05/29/creditcard4.html",
            "date": " • May 29, 2020"
        }
        
    
  
    
        ,"post34": {
            "title": "Bacon numbers via Recursive SQL",
            "content": "SQL in Python . I ran the SQL commands in Python using the module sqlite3. I used this article to help me set it all up. Below is the code to run queries on the database movies.db in python. . import sqlite3 def run_sql(query): connection = sqlite3.connect(&#39;movies.db&#39;) cursor = connection.cursor() cursor.execute(query) results = cursor.fetchall() cursor.close() connection.close() return results . Understanding the database . The data is in the form of a database, movies.db, and was obtained from the online course CS50, which in turn was obtained from imdb. To find out the structure of the tables in the database, I ran the following code (obtained from stackoverflow). . def produce_schema(): sql = &quot;SELECT name FROM sqlite_master WHERE type = &#39;table&#39;;&quot; tables = run_sql(sql) schema = {} for table in tables: sql = f&quot;SELECT sql FROM sqlite_master WHERE type = &#39;table&#39; and name = &#39;{table[0]}&#39;&quot; results = run_sql(sql) print(results[0][0]) schema[table[0]] = results[0][0] return schema produce_schema() . For the task of determining bacon numbers, the relevant tables and columns are: . people, with columns id and name. Each person has a unique id. There are 1044499 people in the table. | stars, with columns movie_id and person_id. Each row tells you that the actor with id person_id starred in the movie with id movie_id. | . Kevin Bacon has an id of 102, found by running the query SELECT id FROM people WHERE name = &#39;Kevin Bacon&#39; . The recursive query . To understand recursion in SQL, I recommend this guide which explains recursion and how to use recursion in SQL, and then the SQLite documentation to better understand some implementation details and what options are available to you. . The query I created to produce the table of bacon numbers is: . WITH RECURSIVE costars(id1, id2) AS ( SELECT stars1.person_id, stars2.person_id FROM stars AS stars1 JOIN stars AS stars2 ON stars1.movie_id = stars2.movie_id ), bacon(id, num) AS ( VALUES(102, 0) UNION SELECT id2, num+1 FROM bacon JOIN costars ON bacon.id = costars.id1 WHERE num &lt; 13 ) SELECT bacon.id, name, MIN(num) FROM bacon JOIN people ON bacon.id = people.id GROUP BY bacon.id ORDER BY num . The table costars lists all pairs of actors that co-starred in a movie. This is obtained by doing a self-join of the stars table. | The table bacon is the main table. id is the id of the actor and num is the bacon number. It starts off with the entry (102,0). 102 is Kevin Bacon’s id and 0 is Kevin Bacon’s Bacon number. | Then for any entry (id, num) already in the table, we add a new row (id2, num+1), whenever id2 co-starred with id. | num &lt; 13 indicates we will only have a maximum bacon number of 13. Without this manual limit, the recursive query would never end. This is because the underlying data is not acyclic: e.g. if a is a co-star with b, then b is a co-star of a. The number 13 was chosen via trial-and-error. The resulting table does not change if I increase the limit further, which implies that the maximum bacon number is 13. | . | In the end, I select the relevant data from this recursive construction. Because each actor can appear many times in this construction, I use GROUP BY to ensure each actor appears only once. I use MIN(num) to select each actor’s earliest appearance. | . The two main problems with this query are that: . It is inefficient. There is huge redundancy as actors appear many times in the recursive construction. I do not think there is a way of avoiding this within SQL. | I have to know what the maximum Bacon number is for the query to produce a complete list. I found this using trial-and-error. | . Bacon number of 13 . By running a simple query, I find there are two people with the maximum bacon number of 13, Javier Ordonez and Kimberly Peters. Using the program created for this CS50 AI project, I could find the path from these actors to Kevin Bacon. As expected, it takes 13 steps (always satisfying to see two different programs being consistent!) and they are below. Note they have the same path. . Javier Ordonez/Kimberly Peters and Amanda Brass starred in PRND | Amanda Brass and Michael Bayouth starred in Park Reverse Neutral Drive, PRND (Director’s cut) | Michael Bayouth and Brandy Bourdeaux starred in Grease Trek | Brandy Bourdeaux and Kim Beuché starred in Murder Inside of Me | Kim Beuché and Ed Baccari starred in Island, Alicia | Ed Baccari and Aida Angotti starred in Late Watch | Aida Angotti and Lamont Copeland starred in Bottom Out | Lamont Copeland and Ashley Marie Arnold starred in Eye Was Blind | Ashley Marie Arnold and Sid Bernstein starred in The Rodnees: We Mod Like Dat! | Sid Bernstein and Chuck Berry starred in The Beatles: The Lost Concert | Chuck Berry and Eric Clapton starred in Chuck Berry Hail! Hail! Rock ‘n’ Roll | Eric Clapton and Tom Cruise starred in Close Up | Tom Cruise and Kevin Bacon starred in A Few Good Men |",
            "url": "https://lovkush-a.github.io/blog/sql/python/2020/05/24/recursion_sql.html",
            "relUrl": "/sql/python/2020/05/24/recursion_sql.html",
            "date": " • May 24, 2020"
        }
        
    
  
    
        ,"post35": {
            "title": "AIs for Games, Part I, Brute Force TicTacToe",
            "content": "Other posts in series . AIs for Games, Part III, Pruning Min-Max for Pentago . | AIs for Games, Part II, Min-max for Pentago . | . The AI in Action . Below are screenshots illustrating the AI in action. . 1) I run the program, select the AI to play first, and respond to its first move. . . 2) The AI plays in the top left, forcing me to play in the middle left. . . 3) The end is nigh. The AI creates two threats at once. I stop one of them… . . 4) But I cannot stop both. . . AI: 1, Humans: 0. . Comments . This program is highly inefficient. E.g it will search through every possible move, even if it has already found a winning sequence. My focus was on getting something working, rather than trying to optimise it. . Next steps . Coding up more complicated games. I have already done this for Pentago (see github). | Coding up the alpha-beta pruning algorithm | Coding up a neural network to create a heuristic function | . Code . The code for this is available on github. .",
            "url": "https://lovkush-a.github.io/blog/ai/python/2020/05/20/games1.html",
            "relUrl": "/ai/python/2020/05/20/games1.html",
            "date": " • May 20, 2020"
        }
        
    
  
    
        ,"post36": {
            "title": "Investigating Credit Card Fraud, Part III, Handmade Model",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle . | Investigating Credit Card Fraud, Part V, Final Models . | Investigating Credit Card Fraud, Part IV, n_estimators . | Investigating Credit Card Fraud, Part II, Removing data . | Investigating Credit Card Fraud, Part I, First Models . | . The hand-made model . Given the training data, for each feature, I compute the mean of that feature amongst all fraudulent transactions and the non-fraudulent transactions. For example, the mean value of the transaction was $124 for fraudulent transactions and $88 for non-fraudulent transactions. . Then, given some unseen transaction, I ask how many of its features are closer to the fraudulent mean or non-fraudulent mean. For example, if the value of a transaction is $115, then it is closer to the fraudulent mean of $124, so this is evidence the transaction is fraudulent. . And that’s it! This is not sophisticated at all, but I am interested to see how much information is contained in the means, and more importantly, how much information is gained by using more sophisticated models like Random Forests and XGBoosts. . Results . Below is the precision-recall graph for the model. . . The AUC score is not great, only 0.616, but the model manages to identify ~70% of fraudulent claims with a precision of 0.8, which is not too bad! . At the end of the previous post, I said I hoped to achieve a score of 0.7. To do this, I just flattened the above precision-recall curve, by grouping together all the ‘high probability’ transactions (see code for explicit details). See the new curve: . . An AUC of 0.68! Not quite 0.7, but not too shabby. . Final thoughts . There are several ways one could easily improve this model. One example is that I weighted each of the features equally, but some features ought to be more weighted than others. Another example is that I did not consider how much closer a value was to the fraudulent mean compared to the non-fraudulent mean. There are many more possibilities. However, all of these are just the initial steps to creating one of the standard models - this hand-made model is just a (bad) Random Forest after all! . Doing this hands-on modelling has been quite satisfying. Though Random Forests and XGBoosts produce much better models, they did not actually teach me anything. Of course, one big reason for this is that I do not yet know how to extract information from those models - I have only learnt how to create them. Even so, there is something immediately tangible in the simplicity of this hand-made model: 70% of fraudulent transactions follow the average trends of previous fraudulent claims. Though I am prepared to be wrong, I doubt the insights from XGBoosts will be as easy to formulate as that! . The code . # import modules import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_curve from sklearn.metrics import auc from matplotlib import pyplot as plt # import data and create train_test split data = pd.read_csv(&quot;creditcard.csv&quot;) y = data.Class X = data.drop([&#39;Class&#39;], axis = 1) Xt, Xv, yt, yv = train_test_split(X,y, random_state=0) # calculate means for fraudulent and non-fraudulent claims for training data data_t = pd.concat([Xt, yt], axis = 1) means = data_t.groupby(&#39;Class&#39;).mean() means_mid_point = (means.loc[0] + means.loc[1])/2 # create function to create predictions def make_prob(row): prob = 0 for col in row.index: if (row[col] &gt; means_mid_point[col]) and (means.loc[1, col] &gt; means.loc[0, col]): prob += 1 elif (row[col] &lt; means_mid_point[col]) and (means.loc[1, col] &lt; means.loc[0, col]): prob += 1 return prob/30 # create predictions, calculate AUC and plot PRC preds = Xv.apply(make_prob, axis = 1) precision, recall, _ = precision_recall_curve(yv, preds) auc_handmade = auc(recall, precision) plt.plot(recall, precision, marker = &#39;.&#39;) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) plt.title(f&#39;Hand-made model. AUC = {auc_handmade:.3f}&#39;) plt.savefig(&#39;creditcard_3_handmade&#39;) # tweak predictions to &#39;flatten the curve&#39; preds2 = [] for pred in preds: if pred &gt; 0.7: preds2.append(0.8) else: preds2.append(pred) .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/05/19/creditcard3.html",
            "relUrl": "/python/data%20science/2020/05/19/creditcard3.html",
            "date": " • May 19, 2020"
        }
        
    
  
    
        ,"post37": {
            "title": "Investigating Credit Card Fraud, Part II, Removing data",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle . | Investigating Credit Card Fraud, Part V, Final Models . | Investigating Credit Card Fraud, Part IV, n_estimators . | Investigating Credit Card Fraud, Part III, Handmade Model . | Investigating Credit Card Fraud, Part I, First Models . | . My thinking and plan . When doing the initial investigations, I noticed it took some time for the fitting, in particular for the random forest models to be fit. I want to do some hyper-parameter optimisations, but do not want to wait hours for it. Therefore, I wanted to reduce the time it takes. . I figured that 10% of the non-fraudulent data should contain most of the patterns that 100% of the non-fraudulent data does, and presumably having smaller datasets reduced the run time. . To reduce the datasets, I first split the data using train_test_split as normal. Then, I kept only those non-fraudulent entries whose index had final digit 0 - so I only have 10% remaining. . To better understand the effect removing data has, I tried removing different amounts of data, from 50% to 99%. The code for all this is at the bottom of the page. . Results for Random Forests . The charts below show what happened as I varied how much data was removed. . . As I hoped, the time taken for the fitting to take place reduces as the dataset is made smaller. (In fact, time taken is linear with size of dataset. I don’t know if this is surprising or not, but I imagine it is clear if one knows implementation details of the algorithms). Also as I predicted, the effectiveness does not drop considerably by removing data. . The charts below show some of the resulting AUC curves, so we can see where the drop in performance occurs. . . We can see that removing non-fraudulent data has resulted in reduced precision, with no visible drop in recall. This makes sense: I did not remove any of the fraudulent entries, so it looks like the models were still able extract the same information about them. . This is encouraging. In the context of credit card fraud, recall is more important than precision: the cost of fraud is greater than cost of annoying customers by mis-labelling their transactions as fraudulent. . Results for XGBoost . I ran the process on XGBoost models too. The charts are below. . . The results are similar to those for the random forest. The compute time is linear with the amount of data kept, and performance does not drop much either. Surprisingly, the performance is almost the same with only 10% of the data: only a 0.007 drop in the AUC! It looks like XGBoost is more ‘data-efficient’ than Random Forest: to get good performance, XGBoost requires less data than Random Forests. . Next steps . The next steps will be to do some hyper-parameter optimisations. But before that, like mentioned in Part 1, I want to better understand the data by creating a crude hand-made model. It will be interesting to see how it compares! My hope is to get an AUC of 0.7. . The code . Below is the code to produce the XGBoost models and charts. The code for Random Forest is similar. . # import modules import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_curve from sklearn.metrics import auc from matplotlib import pyplot as plt from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier from datetime import datetime # import data and create train_test split data = pd.read_csv(&quot;creditcard.csv&quot;) y = data.Class X = data.drop([&#39;Class&#39;], axis = 1) Xt, Xv, yt, yv = train_test_split(X,y, random_state=0) # create function which takes model and data # returns auc, time taken, and saves plot. def auc_model(model, title, saveas, Xt, Xv, yt, yv): t0 = datetime.now() model.fit(Xt,yt) t1 = datetime.now() time = t1 - t0 time = time.total_seconds() preds = model.predict_proba(Xv) preds = preds[:,1] precision, recall, _ = precision_recall_curve(yv, preds) auc_current = auc(recall, precision) plt.figure() plt.plot(recall, precision, marker=&#39;.&#39;, label=&#39;basic&#39;) plt.xlabel(&#39;Recall&#39;) plt.ylabel(&#39;Precision&#39;) plt.title(title + f&#39;. AUC={auc_current:.3f}&#39;) plt.savefig(saveas + &#39;.png&#39;) return time, auc_current # create multiply xgb models with varying amount of data removed model_xgb = XGBClassifier() fraction_kept = [1,0.5,0.2,0.1,0.05,0.02,0.01] times = [] aucs = [] for f in fraction_kept: selection = (Xt.index % (1/f) == 0) | (yt == 1) Xt_reduced = Xt[selection] yt_reduced = yt[selection] title = f&#39;XGB Model. Keeping {100*f:.0f}% of non-fraudulent data&#39; saveas = f&#39;creditcard_2_xgb_{100*f:.0f}&#39; time_new, auc_new = auc_model(model_xgb, title, saveas, Xt_reduced, Xv, yt_reduced, yv) times.append(time_new) aucs.append(auc_new) # plot charts to show effect of changing fraction of non-frauduluent data removed plt.figure() plt.plot(fraction_kept, times, marker=&#39;.&#39;) plt.xlabel(&#39;Fraction of non-fraudulent data kept&#39;) plt.ylabel(&#39;Time to fit the model, seconds&#39;) plt.title(&#39;XGB. Fraction of non-fraudulent data kept vs time for fitting&#39;) plt.savefig(&#39;creditcard_2_xgb_times&#39;) plt.figure() plt.plot(fraction_kept, aucs, marker=&#39;.&#39;) plt.xlabel(&#39;Fraction of non-fraudulent data kept&#39;) plt.ylabel(&#39;AUC of model&#39;) plt.title(&#39;XGB. Fraction of non-fraudulent data kept vs AUC&#39;) plt.savefig(&#39;creditcard_2_xgb_aucs&#39;) .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/05/16/creditcard2.html",
            "relUrl": "/python/data%20science/2020/05/16/creditcard2.html",
            "date": " • May 16, 2020"
        }
        
    
  
    
        ,"post38": {
            "title": "Trouble with Jekyll",
            "content": "For reasons unknown to me, Jekyll stopped working, and Googling around did not reveal a fix. I decided to create a fresh directory, create a new Conda environment, and install everything again. However, this did not work! . This was strange - it all worked the first time around without a hitch. After much Googling and several failed attempts, I just took a break and decided to come back to it tomorrow (now today). I was agitated and frustrated. I was worried this blog would die out before it even got going! . Today, the first thing I tried was installing Jekyll outside of any environment, i.e., without creating a Conda environment. This failed because “You don&#39;t have write permissions for the /Library/Ruby/Gems/2.3.0 directory”. I googled and this stackoverflow page explained that I should not try to over-ride these permissions, but instead use chruby and ruby-install. . Googling those lead me to this little guide. I followed the steps and it all seemed to work as expected. (Note, I am uncomfortable manually modifying hidden files like .zshrc. Is this normal?). . Now came the moment of truth. I enter gem install jekyll bundler into terminal. . … . And it works! Phew! OK, now it’s actually time to practice some data science. .",
            "url": "https://lovkush-a.github.io/blog/other%20it/2020/05/14/jekyll.html",
            "relUrl": "/other%20it/2020/05/14/jekyll.html",
            "date": " • May 14, 2020"
        }
        
    
  
    
        ,"post39": {
            "title": "Investigating Credit Card Fraud, Part I, First Models",
            "content": "Other posts in series . Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle . | Investigating Credit Card Fraud, Part V, Final Models . | Investigating Credit Card Fraud, Part IV, n_estimators . | Investigating Credit Card Fraud, Part III, Handmade Model . | Investigating Credit Card Fraud, Part II, Removing data . | . Quick Personal Background . I have been studying programming and data science using various resources. The main resources I have used so far, for data science specifically, are the Kaggle Courses. I have completed: . Intro to Machine Learning | Intermediate Machine Learning | Pandas | Data Visualisation | . Following the advice I have read in numerous places, I decided I should try to do some data science of my own - not just follow some exercises. I skimmed through the Kaggle datasets, and this dataset on credit card fraud caught my fancy, so I jumped right in! . First steps . In the description, it said that there are 284807 transactions with only 492 labelled as fraudulent. The task is to create a model to predict which transactions are fraudulent. The description also said that because the percentage of fraudulent cases is so small, it is best to use “AUPRC”, Area Under the Precision-Recall Curve, to evaluate the model. I had not heard of this yet so I did some Googling. . I found this blog post by Jason Brownlee, which explained what AUPRC is with some examples. This was helpful and I used the same packages he did. . Before going onto AUPRC and my models, I should say that there was no need to do any data cleaning for this dataset. It had already been cleaned and the data has been anonymised via a PCA. Again, I have not studied this yet, but a brief skim of this wikipedia article on PCAs gives me some basic insight. It looks neat actually - it replaces the original data with new data which captures the variation in the old data but in such a way that the features in the new data have minimal correlation between them. My intuition is that the less correlation there is between features, the better the models works. PCA is definitely something I need to look into more at some point! . AUPRC . I will try to summarise the idea behind AUPRC. . The framework. We have features X and we are trying to predict y. In this case, y says whether the transaction is fraudulent or not. If y=0 then the transaction is not fraudulent, and if y=1 then it is. | The models produce values of y between 0 and 1, representing the probability of a transaction being fraudulent. | To decide if a transaction is fraudulent or not, we need to also specify a threshold probability, p. If y&gt;p, we label that transaction as fraudulent. | . | Precision and Recall Precision is the fraction of transactions that are labelled as fraudulent that actually are fraudulent. I somehow find it easier to think about 1-Precision, which measures how many false-positives we have. | Recall is the fraction of actually fraudulent transactions the model manages to label as fraudulent. Again, I find it easier to think about 1-Recall, which measures how many false-negatives we have. | A perfect model has a precision and a recall of 1. This is not possible and we need to make a trade-off between them. This trade-off is achieved by varying the threshold probability p. | . | The Precision Recall curve When p=1, we are saying all transactions are not fraudulent (because you need to assign a probability greater than 1 in order it to be considered fraudulent, which is not possible). Therefore, there are no false positives so we have a precision of 1, but we have not found any of the fraudulent transactions, so the recall is 0. | When p=0, we are saying all transactions are fraudulent. Therefore, all the fraudulent transactions are found so we have a recall of 1, but we have a huge number of false positives, so a precision almost equal to 0. | As we vary p from 1 to 0, we move from the the coordinate (0,1) to (1,0), and we are hoping to get as close to the coordinate (1,1) as possible. | . | The Area Under the Precision-Recall Curve The closer we get to (1,1), the larger the area under the curve is. | A perfect model would reach (1,1) and achieve an area of 1. | The worst model, where you assign probabilities at random, will produce a straight line connecting (0,1) to (1,0), giving an area of 0.5. | Thus the AUPRC is a measure of the model, with a value between 0.5 and 1. | . | Enough text, time for pictures . Below you will find the precision-recall charts for the various models that were created, along with their AUPRC metric. . Basic model . I started by creating a basic model, where it just assigns a probability of 0.1% for any transaction to be fraudulent. . . As you can see from the chart and the AUPRC of 0.501, this model is poor. No surprises here, which I suppose is a good thing. . Logistic Regression . I have not studied logistic regression yet so I do not actually know what it does differently to linear regression. I decided to still use it because it is the model Jason Brownlee used in their example, and I wanted to follow their example before exploring on my own. . . Huzzah! My first non-trivial AUC curve. The model can identify 60% of the fraudulent cases without too many false positives (roughly 20%). The precision falls dramatically if you try to increase the recall. . Decision Tree . Next I tried a Decision Tree model. (This is the first model taught in Kaggle). . . Curve looks very simple, but it has actually extracted some information. It can identify ~75% of the fraudulent cases with a precision of roughly 0.75. I would argue this is better than the logistic regression, because the cost of fraud is greater than the cost of mis-identifying something as fraud. . Random Forest . Next I tried a Random Forest model. (This is the second model taught in Kaggle). . . Woh! I was surprised by how good this is. 80% of the fraudulent cases identified with a precision of 95%! Lets see if the infamous XGBoost can do better. . XGBoost . Drum roll please… . . Looks very similar to the random forest model. I do not know if this is surprising or not - hopefully I will get more intuition for these kind of things with more practice. . Next steps . There are various things I would like to try. . Exploring the data a bit and creating a crude handmade model. Something like find the average of Xi in fraudulent cases and in non-fraudulent cases. Then the probability of being fraudulent is determined by whether you are closer to the fraudulent means than the non-fraudulent means. | Hyper-parameter optimisations. I used only default settings for all the models. I just wanted to get something made and published before trying to mess around with settings. | Seeing what happens if I randomly delete 90% of the fraudulent cases from the training. My prediction is there will not be significant loss in information but there should be significant time savings. (The random forest model took a few minutes to run.) This will make hyper-parameter training quicker. | Exploring the models themselves. What are the probabilities produced by the models? What patterns have the models found? | After I have done my own investigations, find out what other people did. | . The code . Below is the code to produce the XGBoost model. The code for other models is identical but with ‘XGB’ replaced as appropriate. (At some point in future, I will use for loops to loop through the models.) . # import modules import numpy as np import pandas as pd from sklearn.model_selection import train_test_split from sklearn.metrics import precision_recall_curve from sklearn.metrics import auc from matplotlib import pyplot from sklearn.linear_model import LogisticRegression from sklearn.tree import DecisionTreeClassifier from sklearn.ensemble import RandomForestClassifier from xgboost import XGBClassifier # create test validation split y = data.Class X = data.drop([&#39;Class&#39;], axis = 1) Xt, Xv, yt, yv = train_test_split(X,y, random_state=0) # create model and predictions model_xgb = XGBClassifier(random_state = 0) model_xgb.fit(Xt, yt) predictions_xgb = model_xgb.predict_proba(Xv) predictions_xgb = predictions_xgb[:,1] # calculate precision, recall and AUC xgb_precision, xgb_recall, _ = precision_recall_curve(yv, predictions_xgb) xgb_auc = auc(xgb_recall, xgb_precision) # plot precision recall curve pyplot.plot(xgb_recall, xgb_precision, marker=&#39;.&#39;, label=&#39;basic&#39;) pyplot.xlabel(&#39;Recall&#39;) pyplot.ylabel(&#39;Precision&#39;) pyplot.title(f&#39;XGBoost model. AUC ={xgb_auc:.3f}&#39;) pyplot.savefig(&#39;creditcard_1_xgb.png&#39;) .",
            "url": "https://lovkush-a.github.io/blog/python/data%20science/2020/05/14/creditcard1.html",
            "relUrl": "/python/data%20science/2020/05/14/creditcard1.html",
            "date": " • May 14, 2020"
        }
        
    
  
    
        ,"post40": {
            "title": "Making this blog",
            "content": "I decided to create a blog after reading David Robinson’s advice. I had read many times before that I ought to create a blog/portfolio, but somehow this article was the tipping point. I recommend reading it! . | I got a basic understanding of Jekyll and Github Pages by following Jonathan McGlone’s guide. It has minimal pre-requisites and is accessible to beginners. I did not follow all the steps, but it was a good starting point. It was easy to get something up and running, and it recommends resources to move forwards. . | I followed Jekyll’s’ step-by-step tutorial. This is excellent. It is clear and it builds things up logically. A few minor details: . I created a Conda environment in which I installed Ruby and Jekyll. | I cloned the GitHub repository from Step 2 into this enviroment, and adapted the steps as appropriate (e.g. copying the CSS from Step 2 into the SASS file). | I do not set up ‘authors’, as I am going to be the only author on this blog! | . | As with most of these things, the upfront cost is high and it requires patience to get through it. But now that I have done it, I am extremely pleased. Having created my personal website using a hodge-podge of HTML, CSS and PHP, I can appreciate how much smoother Jekyll makes everything. It is refreshing. . Goodbye, angled-brackets! You shan’t be missed. .",
            "url": "https://lovkush-a.github.io/blog/other%20it/2020/05/13/blog.html",
            "relUrl": "/other%20it/2020/05/13/blog.html",
            "date": " • May 13, 2020"
        }
        
    
  
    
        ,"post41": {
            "title": "First blog post",
            "content": "This is my first blog post! :D .",
            "url": "https://lovkush-a.github.io/blog/2020/05/12/first.html",
            "relUrl": "/2020/05/12/first.html",
            "date": " • May 12, 2020"
        }
        
    
  
    
        ,"post42": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . #collapse-hide import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . #collapse-show cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # single-value selection over [Major_Genre, MPAA_Rating] pairs # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(movies).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(movies).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=alt.Y(&#39;IMDB_Rating:Q&#39;, axis=alt.Axis(minExtent=30)), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=600, height=400 ) . Example 3: More Tooltips . # select a point for which to provide details-on-demand label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=700, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; df = pd.read_json(movies) # display table with pandas df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://lovkush-a.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post43": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://lovkush-a.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "My name is Lovkush Agarwal. I recently decided to change careers and become a data scientist. Following David Robinson’s’ advice, I decided to create this blog. . To find out more about me, check out my personal website. . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://lovkush-a.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  

  
  

  
      ,"page12": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://lovkush-a.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}