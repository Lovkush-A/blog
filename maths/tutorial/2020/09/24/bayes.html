<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>An intuitive but unknown version of Bayes’ Theorem | Lovkush Agarwal</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="An intuitive but unknown version of Bayes’ Theorem" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds." />
<meta property="og:description" content="In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds." />
<link rel="canonical" href="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html" />
<meta property="og:url" content="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html" />
<meta property="og:site_name" content="Lovkush Agarwal" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-24T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"An intuitive but unknown version of Bayes’ Theorem","description":"In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds.","datePublished":"2020-09-24T00:00:00-05:00","dateModified":"2020-09-24T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html"},"url":"https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://lovkush-a.github.io/blog/feed.xml" title="Lovkush Agarwal" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-29327017-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>





<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>An intuitive but unknown version of Bayes’ Theorem | Lovkush Agarwal</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="An intuitive but unknown version of Bayes’ Theorem" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds." />
<meta property="og:description" content="In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds." />
<link rel="canonical" href="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html" />
<meta property="og:url" content="https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html" />
<meta property="og:site_name" content="Lovkush Agarwal" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-24T00:00:00-05:00" />
<script type="application/ld+json">
{"headline":"An intuitive but unknown version of Bayes’ Theorem","description":"In the 80000 Hours’ interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye’s Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds.","datePublished":"2020-09-24T00:00:00-05:00","dateModified":"2020-09-24T00:00:00-05:00","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html"},"url":"https://lovkush-a.github.io/blog/maths/tutorial/2020/09/24/bayes.html","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://lovkush-a.github.io/blog/feed.xml" title="Lovkush Agarwal" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-29327017-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>


    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.css" integrity="sha384-zB1R0rpPzHqg7Kpt0Aljp8JPLqbXI3bhnPWROx27a9N0Ll6ZP/+DiW/UqRcLbRjq" crossorigin="anonymous">
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML"> </script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/katex.min.js" integrity="sha384-y23I5Q6l+B6vatafAwxRu/0oK/79VlbSz7Q9aiSZUvyWYIYsd+qj+o24G5ZU2zJz" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/katex@0.11.1/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Lovkush Agarwal</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">An intuitive but unknown version of Bayes&#39; Theorem</h1><p class="page-description">In the 80000 Hours' interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye's Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-24T00:00:00-05:00" itemprop="datePublished">
        Sep 24, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      12 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#maths">maths</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#tutorial">tutorial</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#algebraic">Algebraic</a></li>
<li class="toc-entry toc-h2"><a href="#changing-the-universe--some-algebra">‘Changing the universe’ + some algebra</a></li>
<li class="toc-entry toc-h2"><a href="#adjusting-a-first-estimate">Adjusting a first estimate</a></li>
<li class="toc-entry toc-h2"><a href="#bayes-via-odds-the-unknown-version-of-bayes-theorem">Bayes via odds: the unknown version of Bayes’ Theorem</a>
<ul>
<li class="toc-entry toc-h3"><a href="#odds">Odds</a></li>
<li class="toc-entry toc-h3"><a href="#example-1-disease-given-a-positive-test-result">Example 1. Disease given a positive test result</a></li>
<li class="toc-entry toc-h3"><a href="#example-2-the-monty-hall-problem">Example 2. The Monty-Hall Problem</a></li>
<li class="toc-entry toc-h3"><a href="#exercises">Exercises</a></li>
<li class="toc-entry toc-h3"><a href="#pros-and-cons">Pros and Cons</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#final-remarks">Final remarks</a></li>
</ul><h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>One of the big lessons I have learnt is that no matter how well (you think) you understand something, there is almost always a deeper or clearer level of understanding available. Even for trivially simple things, like addition. Today, I will describe my experience with this phenomena in the context of Baye’s Theorem, by going through four different ways I have of conceptualising it. The final way  is surprisingly simple but largely unknown; I learnt about it from this <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/#bayesian-updating">80000 Hours’ interview of Spencer Greenberg</a>.</p>

<p>Recall that Bayes Theorem states that:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A | B) = \frac{P(A) P(B|A)}{P(B)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">A</span><span class="mclose mtight">)</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mord mtight">∣</span><span class="mord mathdefault mtight">A</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>

<h2 id="algebraic">
<a class="anchor" href="#algebraic" aria-hidden="true"><span class="octicon octicon-link"></span></a>Algebraic</h2>
<p>For a long time, the main way I thought about Baye’s Theorem was that it was just a consequence of some algebraic re-arrangement:</p>

<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo>∩</mo><mi>A</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo><mspace linebreak="newline"></mspace><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(B \cap A) = P(A \cap B) \\
P(B) P(A|B) = P(A) P(B|A) \\
P(A|B) = \frac{P(A) P(B|A)}{P(B)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">∩</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mclose">)</span></span><span class="mspace newline"></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:2.363em;vertical-align:-0.936em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mclose">)</span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mord">∣</span><span class="mord mathdefault">A</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.936em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></span>

<p>Pros:</p>
<ul>
  <li>It is a derivation of the formula</li>
  <li>It is easy to follow this argument</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Provides no insight or understanding</li>
</ul>

<p>Unfortunately, this is how I treated a lot of mathematics during my teenage and undergraduate years. The purpose of a proof is to establish the truth of something, and once you know it is true, you are free to use it. Somehow, I never stepped back and asked myself if there was more understanding available.</p>

<h2 id="changing-the-universe--some-algebra">
<a class="anchor" href="#changing-the-universe--some-algebra" aria-hidden="true"><span class="octicon octicon-link"></span></a>‘Changing the universe’ + some algebra</h2>
<p>Often when dealing with probabilities, you have to determine all the possibilities of the situation (‘the universe of possibilities’) and then determine which of those corresponds to the event you are interested in.</p>

<p>For example, to determine the probability of getting two heads when you toss a fair coin twice, you might say that there are four total options (that are all equally likely), and one of those is what we are interested in, so the probability is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>4</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{4}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">4</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</p>

<p>How does this relate to Bayes? Well, the way I think about conditional probability is that when calculating the probability of A given B, I am changing the universe so that the possibilities are precisely those that correspond to B. Once I am in this restricted universe, I continue reasoning as normal.</p>

<p>For example, what is the probability of getting two heads given that at least one of them (but we don’t know which) is heads. In this case, the universe of possibilities is reduced to three options (we have ruled out the option of TT), so the probability is now <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><mn>3</mn></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{3}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.190108em;vertical-align:-0.345em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.845108em;"><span style="top:-2.6550000000000002em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">3</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.394em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.345em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span>.</p>

<p>Converting this to algebra, we get:
<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mi mathvariant="normal">∣</mi><mi>B</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo>∩</mo><mi>B</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>A</mi><mo stretchy="false">)</mo><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mi mathvariant="normal">∣</mi><mi>A</mi><mo stretchy="false">)</mo></mrow><mrow><mi>P</mi><mo stretchy="false">(</mo><mi>B</mi><mo stretchy="false">)</mo></mrow></mfrac></mrow><annotation encoding="application/x-tex">P(A|B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) P(B|A)}{P(B)}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathdefault" style="margin-right:0.13889em;">P</span><span class="mopen">(</span><span class="mord mathdefault">A</span><span class="mord">∣</span><span class="mord mathdefault" style="margin-right:0.05017em;">B</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">A</span><span class="mbin mtight">∩</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1.53em;vertical-align:-0.52em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.01em;"><span style="top:-2.655em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mclose mtight">)</span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.485em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight">A</span><span class="mclose mtight">)</span><span class="mord mathdefault mtight" style="margin-right:0.13889em;">P</span><span class="mopen mtight">(</span><span class="mord mathdefault mtight" style="margin-right:0.05017em;">B</span><span class="mord mtight">∣</span><span class="mord mathdefault mtight">A</span><span class="mclose mtight">)</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.52em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span></span></span></span></p>

<p>Pros:</p>
<ul>
  <li>It is a derivation of the formula</li>
  <li>Slightly more insight than the original, e.g. we divide by $P(B)$ because $B$ has become the ‘universe of possibilities’.</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Still not particularly insightful</li>
</ul>

<h2 id="adjusting-a-first-estimate">
<a class="anchor" href="#adjusting-a-first-estimate" aria-hidden="true"><span class="octicon octicon-link"></span></a>Adjusting a first estimate</h2>
<p>A third way I have of thinking about Baye’s Theorem (which if I remember correctly, only arose after being exposed to the fourth way below) starts with the idea that a reasonable default belief or first estimate is:</p>

<table>
  <tbody>
    <tr>
      <td>$$P(A</td>
      <td>B) = P(A)$$.</td>
    </tr>
  </tbody>
</table>

<p>Why is this a reasonable first estimate?</p>
<ul>
  <li>The probability of $A$ happening given some information ought to be related to the probability of $A$ happening without any information.</li>
  <li>If $B$ has no influence on $A$ whatsoever, then this first estimate <em>is</em> exactly correct. (And this is the basis of the formal definition of two events being independent.)</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>Now that we have a first estimate, we need adjust it to get the exact probability. This adjustment factor is the role of the term $$\frac{P(B</td>
      <td>A)}{P(B)}$$. Why is this a sensible adjustment factor? Let us see when we increase or decrease our first estimate.</td>
    </tr>
  </tbody>
</table>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>If $$P(B</td>
          <td>A) &gt; P(B)$$, then the adjustment factor is bigger than 1, so we increase our first estimate. This makes sense, or at least feels intuitive: if $B$ is more likely when $A$ occurs than when $A$ does not occur, then we should increase our estimate of $A$ occuring.</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>If $$P(B</td>
          <td>A) &lt; P(B)$$, then the adjustment factor is smaller than 1, so we decrease our first estimate. The makes sense, following similar reasoning to the point above.</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<table>
  <tbody>
    <tr>
      <td>I hope that makes sense - please let me know if it does not, it is somewhat vague and fuzzy. The strange thing about this is that this is the whole point of ‘priors’ and ‘posteriors’ in Bayesian updating. Yet somehow, I never made the intuitive leap that $$P(A</td>
      <td>B)$$ ought to be $P(A)$ with some kind of adjustment. I cannot remember what I thought when I first learnt about Bayesian stats at university, but my guess is that I treated it more mechanically: “If we have these various bits of information, which for formality sake we label with fancy terms like ‘prior distribution’ and ‘likelihood function’, then we can use Baye’s Theorem to calculate this new thing which we label ‘posterior distribution’”.</td>
    </tr>
  </tbody>
</table>

<p>Pros:</p>
<ul>
  <li>Provides insight into what Bayes’ Theorem is actually saying</li>
  <li>Corresponds to Bayesian statistics / Bayesian updating</li>
</ul>

<p>Cons:</p>
<ul>
  <li>Does not provide a derivation of the formula</li>
</ul>

<h2 id="bayes-via-odds-the-unknown-version-of-bayes-theorem">
<a class="anchor" href="#bayes-via-odds-the-unknown-version-of-bayes-theorem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Bayes via odds: the unknown version of Bayes’ Theorem</h2>
<p>There are two main ideas in this version of Bayes’ Theorem.</p>
<ul>
  <li>Replacing probabilities with odds</li>
  <li>Adjusting Bayes’ Theorem to odds</li>
</ul>

<p>Instead of giving a general derivation or discussion, I think the easiest way to illustrate and show-off this version of Bayes’ Theorem is by doing walking through some explicit examples. Before that though, we should first get used to thinking in terms of odds.</p>

<h3 id="odds">
<a class="anchor" href="#odds" aria-hidden="true"><span class="octicon octicon-link"></span></a>Odds</h3>
<p>When describing uncertainty, the usual way is to list all the possible options along with their probabilities. With odds, we instead assign <em>relative probabilities</em>, and collectively these relative probabilities are called the odds.</p>

<ul>
  <li>Example. Instead of saying there is a probability of getting 0.5 of getting H and 0.5 of getting T, we would say the odds are 1:1 of getting H and T.</li>
  <li>Example. Instead of saying the probabilities are 0.25, 0.25 and 0.5 of getting two heads, two tails or one of each, we would say the odds are 1:1:2.</li>
</ul>

<p>(Note, I do not actually know what the official phrasing is. I have just come up with something that feels OK and whose meaning is hopefully clear.)</p>

<p>As a little exercise to check your understanding, determine how you would convert between probabilities and odds.</p>

<p>No really, I recommend you do this. The best way to learn is by actively doing something with the ideas, rather than passively reading.</p>

<p>Anyway, here is how I would describe the conversions:</p>
<ul>
  <li>Given some odds, you determine the probabilities by dividing all the relative probabilities by the sum of the relative probabilities. This process is known as <em>normalising</em> and the sum of the relative probabilities is the <em>normalising factor</em> or <em>normalising constant</em>.</li>
  <li>Given some probabilities, there are infinitely many different odds you could create. Say we had probabilities $p_1, p_2$ and $p_3$, then the odds would be any multiple of these three numbers: $ap_1:ap_2:ap_3$. Of course, you should pick $a$ to create the most intuitive list of relative probabilities.</li>
</ul>

<p>Do not worry if this feels strange. That is to be expected with a different way of looking at something that we are so used to looking at in a certain way.</p>

<h3 id="example-1-disease-given-a-positive-test-result">
<a class="anchor" href="#example-1-disease-given-a-positive-test-result" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example 1. Disease given a positive test result</h3>
<p>This is a famous question that demonstrates how bad our intuitions are when it comes to reasoning about uncertainty and probabilities. Here are the assumptions:</p>
<ul>
  <li>Without any other information, there is a 1 in a 1000 chance somebody has the disease, independent of everybody else.</li>
  <li>There is a test for the disease which is 99% effective. Explicitly, 99% of people who have the disease will get a positive test result, and 99% of people who do not have the disease will get a negative test result.</li>
</ul>

<p>(Do not think about how one could know these facts. E.g. how can you know the effectiveness of a test if there is not already a test which is 100% effective?)</p>

<p>The question: you take the test and get a positive result. What is the probability that you have the disease?</p>

<p>The most common instinctive answer is 99%, because we know the test is 99% effective. However, the error here is getting mixed up between the probability of having the disease given that you have a positive test versus the probability of getting a positive test result given that you have the disease.</p>

<p>Before continuing, try to answer the question using standard tools and Bayes’ Theorem as usual.</p>

<p>…</p>

<p>…</p>

<p>OK, I assume you have tried it. Or at least, that you have done it before. If it was me, I would probably draw a probability tree and then do the various calculations.</p>

<p>Now, here is how to answer this question using the magical alternative Bayes’ via odds.</p>

<ol>
  <li>Determine the odds without any information.
    <ul>
      <li>They are 1:999 of having the disease versus not having the disease. (Why is it 999 and not 1000?)</li>
    </ul>
  </li>
  <li>Determine the probabilities of getting a positive test result, conditioned on the possibilities from Step 1.
    <ul>
      <li>If I have the disease, I have 0.99 chance of getting positive result.</li>
      <li>If I do not have the disease, it is a 0.01 chance.</li>
    </ul>
  </li>
  <li>Multiply the corresponding numbers from Steps 1 and 2 together. Re-scale so numbers are nice.
    <ul>
      <li>From Step 1 we had 1:999. From Step 2 we had 0.99:0.01</li>
      <li>Multiplying them together gives 0.99:9.99</li>
      <li>Re-scaling gives 99:999</li>
    </ul>
  </li>
  <li>Marvel at the fact we’re done. Those final numbers you worked out are the posterior odds!</li>
</ol>

<p>!!! This is amazing! I still find it surreal how straightforward the odds perspective makes the whole process. It is basically magic. (Though, if you did do the question using traditional means, you should see how all the numbers and calculations match up).</p>

<h3 id="example-2-the-monty-hall-problem">
<a class="anchor" href="#example-2-the-monty-hall-problem" aria-hidden="true"><span class="octicon octicon-link"></span></a>Example 2. The Monty-Hall Problem</h3>
<p>Another famous question. I assume you know about it, but here is a brief description of the set-up. You are on a game show. There are three doors. Behind two of the doors are goats and behind the third door is a prize (which you value more than a goat). You initially guess that the prize is behind Door 1. The gameshow host is nice, and reveals that Door 3 has a goat behind it. You are then offered to switch your choice from Door 1 to Door 2.</p>

<p>Question: Should you stay on Door 1, switch to Door 2, or does it make no difference (statistically speaking)?</p>

<p>This question is a real brain-burner because, counter-intuitively, the answer is that you should switch. The most common thought is that once Door 3 is ruled out, there is a 50:50 chance of the prize being behind Door 1 or Door 2, so it makes no difference if you switch or not.</p>

<p>Here, I will illustrate how using odds, we can gain some understanding of this situation.</p>

<ol>
  <li>Determine the odds without any information (i.e. before Door 3 was revealed).
    <ul>
      <li>The odds are 1:1:1 for the prize being behind Doors 1, 2 and 3 respectively.</li>
    </ul>
  </li>
  <li>Determine the probabilities of Door 3 being revealed, conditioned on the events in Step 1.
    <ul>
      <li>If the prize is behind Door 1, then there is a 50% chance of Door 3 being opened by the host. (The host picks randomly if they have a choice).</li>
      <li>If the prize is behind Door 2, then there is a 100% chance of Door 3 being opened by the host. (The host will never open the door you originally guessed.)</li>
      <li>It the prize is behind Door 3, then there is 0% chance of Door 3 being opened by the host. (The host always reveals a goat)</li>
    </ul>
  </li>
  <li>Multiply the corresponding values to generate the posterior odds.
    <ul>
      <li>0.5:1:0</li>
      <li>Re-scaling we get, 1:2:0.</li>
    </ul>
  </li>
</ol>

<p>The prize is twice as likely to behind Door 2 as it is Door 1, so you should switch.</p>

<p>Note, I do not think this is the most intuitive explanation of the Monty-Hall Problem. But I hope it is an illustration of the incredible ease of this odds-based Bayes’ Theorem.</p>

<h3 id="exercises">
<a class="anchor" href="#exercises" aria-hidden="true"><span class="octicon octicon-link"></span></a>Exercises</h3>
<p>As I said above, the best way to learn is be actively engaging with the ideas, rather than passively reading. Hence, here are some exercises to try out.</p>
<ol>
  <li>Formulate a statement of Bayes’ Theorem in this odds framework. Then prove it.</li>
  <li>A bag contains 1000 coins - 999 are fair coins but one of them is dodgy and has two heads. You take one out randomly, toss it five times-in-a-row, and get heads each time. What is the probability you have the dodgy coin? How many heads in a row would you have to observe so that you think it is more likely you have a dodgy coin than not?</li>
  <li>You meet somebody. They tell you they have two kids and that at least one of them is born on a Tuesday. What is the probability the other is born on Friday?</li>
</ol>

<h3 id="pros-and-cons">
<a class="anchor" href="#pros-and-cons" aria-hidden="true"><span class="octicon octicon-link"></span></a>Pros and Cons</h3>
<p>Pros</p>
<ul>
  <li>Easy to use</li>
  <li>Better matches how Bayesian reasoning is used in real-life. You want to update all probabilities, not just a single one.</li>
  <li>The ease of use makes explicit exactly how the new information changes the relative probabilities.</li>
  <li>It reveals that $P(B)$ in the standard formulation has no information, it is just a normalising constant.</li>
</ul>

<p>Cons</p>
<ul>
  <li>It is not a derivation.</li>
  <li>It requires an unfamiliar change in perspective.</li>
</ul>

<h2 id="final-remarks">
<a class="anchor" href="#final-remarks" aria-hidden="true"><span class="octicon octicon-link"></span></a>Final remarks</h2>
<p>I hope you gained some new insights into Bayes’ Theorem. If you have any other perspectives on it, please let me know! As I said at the start, it is always surprising how things that you feel you understand contain hidden layers and deeper insights.</p>

<p>Lastly, I recommend you listen to the <a href="https://80000hours.org/podcast/episodes/spencer-greenberg-bayesian-updating/#bayesian-updating">80000 Hours’ interview of Spencer Greenberg</a>. First, you will hear Spencer’s description and perspectives on this odds framework. Second, you will learn a whole bunch of other insightful things: Spencer researches thinking and rationality and tries to develop software tools that take make use of his insights. For example, Spencer discusses their research on when people are over- or under-confident.</p>

  </div><a class="u-url" href="/blog/maths/tutorial/2020/09/24/bayes.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog for my data science learning and projects</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/lovkush-a" title="lovkush-a"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/lovkushatleeds" title="lovkushatleeds"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
