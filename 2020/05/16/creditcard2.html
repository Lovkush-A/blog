<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Investigating Credit Card Fraud, Part II | Lovkush Agarwal</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Investigating Credit Card Fraud, Part II" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models." />
<meta property="og:description" content="I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models." />
<link rel="canonical" href="https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html" />
<meta property="og:url" content="https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html" />
<meta property="og:site_name" content="Lovkush Agarwal" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-16T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models.","@type":"BlogPosting","headline":"Investigating Credit Card Fraud, Part II","dateModified":"2020-05-16T00:00:00-05:00","datePublished":"2020-05-16T00:00:00-05:00","url":"https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://lovkush-a.github.io/blog/feed.xml" title="Lovkush Agarwal" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-29327017-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>





<!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Investigating Credit Card Fraud, Part II | Lovkush Agarwal</title>
<meta name="generator" content="Jekyll v4.0.0" />
<meta property="og:title" content="Investigating Credit Card Fraud, Part II" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models." />
<meta property="og:description" content="I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models." />
<link rel="canonical" href="https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html" />
<meta property="og:url" content="https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html" />
<meta property="og:site_name" content="Lovkush Agarwal" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-05-16T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"I carry on my investigation into the Kaggle Credit Card Fraud dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models.","@type":"BlogPosting","headline":"Investigating Credit Card Fraud, Part II","dateModified":"2020-05-16T00:00:00-05:00","datePublished":"2020-05-16T00:00:00-05:00","url":"https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html","mainEntityOfPage":{"@type":"WebPage","@id":"https://lovkush-a.github.io/blog/2020/05/16/creditcard2.html"},"@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

<link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css"><link type="application/atom+xml" rel="alternate" href="https://lovkush-a.github.io/blog/feed.xml" title="Lovkush Agarwal" /><!-- the google_analytics_id gets auto inserted from the config file -->



<script>(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)})(window,document,'script','//www.google-analytics.com/analytics.js','ga');ga('create','UA-29327017-2','auto');ga('require','displayfeatures');ga('send','pageview');</script>



<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head><body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">Lovkush Agarwal</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">About Me</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Investigating Credit Card Fraud, Part II</h1><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-05-16T00:00:00-05:00" itemprop="datePublished">
        May 16, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>I carry on my investigation into the <a href="https://www.kaggle.com/mlg-ulb/creditcardfraud">Kaggle Credit Card Fraud</a> dataset by seeing what happens if I remove entries from the dataset. The hope is that it reduces the time needed to create the models without drastically reducing the effectiveness of the models.</p>

<h2 id="other-posts-in-series">Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/python/data%20science/2020/06/25/creditcard6.html">Credit Card Fraud, Part VI, Summary and Lessons from Kaggle</a></p>
  </li>
  <li>
    <p><a href="/blog/2020/05/30/creditcard5.html">Investigating Credit Card Fraud, Part V</a></p>
  </li>
  <li>
    <p><a href="/blog/2020/05/29/creditcard4.html">Investigating Credit Card Fraud, Part IV</a></p>
  </li>
  <li>
    <p><a href="/blog/2020/05/19/creditcard3.html">Investigating Credit Card Fraud, Part III</a></p>
  </li>
  <li>
    <p><a href="/blog/2020/05/14/creditcard1.html">Investigating Credit Card Fraud, Part I</a></p>
  </li>
</ul>

<h2 id="my-thinking-and-plan">My thinking and plan</h2>
<p>When doing the initial investigations, I noticed it took some time for the fitting, in particular for the random forest models to be fit.  I want to do some hyper-parameter optimisations, but do not want to wait hours for it.  Therefore, I wanted to reduce the time it takes.</p>

<p>I figured that 10% of the non-fraudulent data should contain most of the patterns that 100% of the non-fraudulent data does, and presumably having smaller datasets reduced the run time.</p>

<p>To reduce the datasets, I first split the data using train_test_split as normal. Then, I kept only those non-fraudulent entries whose index had final digit 0 - so I only have 10% remaining.</p>

<p>To better understand the effect removing data has, I tried removing different amounts of data, from 50% to 99%.  The code for all this is at the bottom of the page.</p>

<h2 id="results-for-random-forests">Results for Random Forests</h2>
<p>The charts below show what happened as I varied how much data was removed.</p>

<p><img src="/blog/images/creditcard_2_forest_times.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_aucs.png" alt="image" /></p>

<p>As I hoped, the time taken for the fitting to take place reduces as the dataset is made smaller. (In fact, time taken is linear with size of dataset. I don’t know if this is surprising or not, but I imagine it is clear if one knows implementation details of the algorithms). Also as I predicted, the effectiveness does not drop considerably by removing data.</p>

<p>The charts below show some of the resulting AUC curves, so we can see where the drop in performance occurs.</p>

<p><img src="/blog/images/creditcard_2_forest_100.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_50.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_20.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_10.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_5.png" alt="image" />
<img src="/blog/images/creditcard_2_forest_1.png" alt="image" /></p>

<p>We can see that removing non-fraudulent data has resulted in reduced precision, with no visible drop in recall.  This makes sense: I did not remove any of the fraudulent entries, so it looks like the models were still able extract the same information about them.</p>

<p>This is encouraging. In the context of credit card fraud, recall is more important than precision: the cost of fraud is greater than cost of annoying customers by mis-labelling their transactions as fraudulent.</p>

<h2 id="results-for-xgboost">Results for XGBoost</h2>
<p>I ran the process on XGBoost models too. The charts are below.</p>

<p><img src="/blog/images/creditcard_2_xgb_times.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_aucs.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_100.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_50.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_20.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_10.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_5.png" alt="image" />
<img src="/blog/images/creditcard_2_xgb_1.png" alt="image" /></p>

<p>The results are similar to those for the random forest. The compute time is linear with the amount of data kept, and performance does not drop much either. Surprisingly, the performance is almost the same with only 10% of the data: only a 0.007 drop in the AUC! It looks like XGBoost is more ‘data-efficient’ than Random Forest: to get good performance, XGBoost requires less data than Random Forests.</p>

<h2 id="next-steps">Next steps</h2>
<p>The next steps will be to do some hyper-parameter optimisations. But before that, like mentioned in Part 1, I want to better understand the data by creating a crude hand-made model. It will be interesting to see how it compares! My hope is to get an AUC of 0.7.</p>

<h2 id="the-code">The code</h2>
<p>Below is the code to produce the XGBoost models and charts. The code for Random Forest is similar.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code># import modules
import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc

from matplotlib import pyplot as plt

from sklearn.ensemble import RandomForestClassifier
from xgboost import XGBClassifier

from datetime import datetime



# import data and create train_test split
data = pd.read_csv("creditcard.csv")
y = data.Class
X = data.drop(['Class'], axis = 1)
Xt, Xv, yt, yv = train_test_split(X,y, random_state=0)



# create function which takes model and data
# returns auc, time taken, and saves plot.

def auc_model(model, title, saveas, Xt, Xv, yt, yv):
    t0 = datetime.now()
    model.fit(Xt,yt)
    t1 = datetime.now()
    
    time = t1 - t0
    time = time.total_seconds()
    
    preds = model.predict_proba(Xv)
    preds = preds[:,1]
    
    precision, recall, _ = precision_recall_curve(yv, preds)
    auc_current = auc(recall, precision)
    
    plt.figure()
    plt.plot(recall, precision, marker='.', label='basic')
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.title(title + f'. AUC={auc_current:.3f}')
    plt.savefig(saveas + '.png')
    
    return time, auc_current



# create multiply xgb models with varying amount of data removed

model_xgb = XGBClassifier()

fraction_kept = [1,0.5,0.2,0.1,0.05,0.02,0.01]
times = []
aucs = []

for f in fraction_kept:
    selection = (Xt.index % (1/f) == 0) | (yt == 1)
    Xt_reduced = Xt[selection]
    yt_reduced = yt[selection]
    title = f'XGB Model. Keeping {100*f:.0f}% of non-fraudulent data'
    saveas = f'creditcard_2_xgb_{100*f:.0f}'
    
    time_new, auc_new = auc_model(model_xgb, title, saveas, Xt_reduced, Xv, yt_reduced, yv)
    times.append(time_new)
    aucs.append(auc_new)



# plot charts to show effect of changing fraction of non-frauduluent data removed
plt.figure()
plt.plot(fraction_kept, times, marker='.')
plt.xlabel('Fraction of non-fraudulent data kept')
plt.ylabel('Time to fit the model, seconds')
plt.title('XGB. Fraction of non-fraudulent data kept vs time for fitting')
plt.savefig('creditcard_2_xgb_times')

plt.figure()
plt.plot(fraction_kept, aucs, marker='.')
plt.xlabel('Fraction of non-fraudulent data kept')
plt.ylabel('AUC of model')
plt.title('XGB. Fraction of non-fraudulent data kept vs AUC')
plt.savefig('creditcard_2_xgb_aucs')
</code></pre></div></div>


  </div><a class="u-url" href="/blog/2020/05/16/creditcard2.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A blog for my data science learning and projects</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/lovkush-a" title="lovkush-a"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/lovkushatleeds" title="lovkushatleeds"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
