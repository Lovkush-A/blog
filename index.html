<div class="home"><h1 id="welcome">Welcome!</h1>

<p>My name is Lovkush Agarwal. I recently decided to change careers and become a
data scientist. Following <a href="http://varianceexplained.org/r/start-blog/">David Robinson’s’</a>
advice, I decided to create this blog, to record my progress, learning and projects.</p>

<h1 id="posts">Posts</h1>



  

  <!-- Hide posts if front matter flag hide:true -->
  
  

  <!-- Sort posts by rank, then date -->
  
  
  

 
  

   <!-- Assemble final sorted posts array -->
  
  <ul class="post-list"><li><h3>
    <a class="post-link" href="/blog/data%20science/2021/10/12/squash_elo.html">
    Using data to improve professional squash rankings
    </a>
</h3><p class="post-meta-description">Improving the ratings of professional squash players using data and the ELO rating system</p><p class="post-meta">Oct 12, 2021</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/2021/04/17/trees_variants.html">
    Similarity trees and NaN trees
    </a>
</h3><p class="post-meta-description">I summarise the two main ideas in Sathe and Aggarwal's paper Similarity Forests.</p><p class="post-meta">Apr 17, 2021</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/causality/tutorial/2021/02/21/collider.html">
    Examples of collider bias
    </a>
</h3><p class="post-meta-description">Collider bias can completely skew research findings. I describe some examples that highlight how this non-obvious bias arises.</p><p class="post-meta">Feb 21, 2021</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/data%20viz/2021/01/01/art.html">
    Using Data Science to Create Art
    </a>
</h3><p class="post-meta-description">While trying to find the best clustering of some text data, I unintentionally stumbled upon some visually striking plots, which I think are highly aesthetic and artistic.</p><p class="post-meta">Jan 1, 2021</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/communication/2020/12/23/demoday.html">
    Presentations. Turning good slides into great slides
    </a>
</h3><p class="post-meta-description">I spent a solid week improving my Demo Day talk with the help of several people at Faculty. In this post, I detail the changes that were made and big lessons I learnt.</p><p class="post-meta">Dec 23, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/nlp/regex/2020/12/20/regex.html">
    A surprising bug caused by regex
    </a>
</h3><p class="post-meta-description">My program to process hundreds of documents would mysteriously stop on a particular document. After a couple of hours of checking all my code, I managed to isolate the problem to a particular regex search. I describe what I learnt in this blog post.</p><p class="post-meta">Dec 20, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/python/data%20science/data%20viz/2020/11/01/squash3.html">
    Squash rankings, Part III, All hail Bokeh!
    </a>
</h3><p class="post-meta-description">Bokeh is amazing! I learnt about it earlier this week and I want to illustrate its prowess by remaking plots from Part II series using Bokeh.</p><p class="post-meta">Nov 1, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/python/2020/10/18/l1l2reg2.html">
    Visualising L1 and L2 regularisation, Part II, Lessons learnt from an experienced programmer
    </a>
</h3><p class="post-meta-description">The process I used to make the animations was inefficient and not programmatic. I could not work out how to adapt the matplotlib animation tools to my situation so I asked for help. Here I describe what I learnt from the help that I received.</p><p class="post-meta">Oct 18, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/python/2020/10/11/l1l2reg.html">
    Visualising L1 and L2 regularisation
    </a>
</h3><p class="post-meta-description">I create various charts to help visualise the difference between L1 and L2 regularisation. The pattern is clear and L1 regularisation does tend to force parameters to zero.</p><p class="post-meta">Oct 11, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/python/2020/10/01/sgd4.html">
    Stochastic Gradient Descent, Part IV, Experimenting with sinusoidal case
    </a>
</h3><p class="post-meta-description">I end this series by describing some experiments I did with the sinusoidal case, before I realised that the learning rate was too big. Spoiler alert: turns out my first instincts from Part I were correct all along...</p><p class="post-meta">Oct 1, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/python/data%20science/2020/09/28/squash2.html">
    Squash rankings, Part II, dimension reduction and clustering
    </a>
</h3><p class="post-meta-description">I finally dip my toes into some dimension reduction and clustering algorithms, by visualising the data I scraped in Part I of this series.</p><p class="post-meta">Sep 28, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/maths/tutorial/2020/09/24/bayes.html">
    An intuitive but unknown version of Bayes&#39; Theorem
    </a>
</h3><p class="post-meta-description">In the 80000 Hours' interview of Spencer Greenberg, Spencer describes a surprisingly simple yet largely unknown version of Baye's Theorem via odds instead of probabilities. In this post, I will describe the various ways I have conceptualised Bayes Theorem, ending with the interpretation that Spencer describes using odds.</p><p class="post-meta">Sep 24, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/python/scraping/2020/09/17/squash1.html">
    Squash rankings, Part I, Scraping wikipedia and data analysis
    </a>
</h3><p class="post-meta-description">I practice some web-scraping and pandas manipulation by scraping squash ranking data from Wikipedia.</p><p class="post-meta">Sep 17, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/neural%20network/python/2020/09/17/sgd3.html">
    Stochastic Gradient Descent, Part III, Fitting linear, quadratic and sinusoidal data using a neural network and **S**GD
    </a>
</h3><p class="post-meta-description">I add the stochasticity in Stochastic Gradient Descent, by using mini-batches. In my previous post, I was hoping this would solve my local minimum with sinusoidal data. To my dismay, it did not help. However, I discover what the problem was all along.</p><p class="post-meta">Sep 17, 2020</p>
</li><li><h3>
    <a class="post-link" href="/blog/data%20science/neural%20network/python/2020/09/11/sgd2.html">
    Stochastic Gradient Descent, Part II, Fitting linear, quadratic and sinusoidal data using a neural network and GD
    </a>
</h3><p class="post-meta-description">I continue my project to visualise and understand gradient descent. This time I try to fit a neural network to linear, quadratic and sinusoidal data.</p><p class="post-meta">Sep 11, 2020</p>
</li></ul>

    
      <div class="pager">
        <ul class="pagination">
          <li><div class="pager-edge">•</div></li>
          <li><div class="current-page">1</div></li>
          <li><a href="/blog/page2/" class="next-page">2</a></li>
        </ul>
      </div></div>