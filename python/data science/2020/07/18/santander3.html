<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Santander Dataset, Part III, Learning from others</h1><p class="page-description">I end this series by describing what I learnt by reading other people's kernals on Kaggle.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-07-18T00:00:00-05:00" itemprop="datePublished">
        Jul 18, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#other-posts-in-series">Other posts in series</a></li>
<li class="toc-entry toc-h2"><a href="#introduction">Introduction</a></li>
<li class="toc-entry toc-h2"><a href="#read-the-instructions">Read the instructions!</a></li>
<li class="toc-entry toc-h2"><a href="#eda">EDA</a></li>
<li class="toc-entry toc-h2"><a href="#two-other-algorithms">Two other algorithms</a></li>
<li class="toc-entry toc-h2"><a href="#creating-a-model-for-each-feature-separately">Creating a model for each feature separately</a></li>
<li class="toc-entry toc-h2"><a href="#using-frequency-as-new-feature">Using frequency as new feature</a></li>
<li class="toc-entry toc-h2"><a href="#visualising-a-tree-based-algorithm">Visualising a tree-based algorithm</a></li>
<li class="toc-entry toc-h2"><a href="#a-remarkable-discovery">A remarkable discovery</a></li>
<li class="toc-entry toc-h2"><a href="#conclusion">Conclusion</a></li>
</ul><h2 id="other-posts-in-series">
<a class="anchor" href="#other-posts-in-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/python/data%20science/2020/07/13/santander2.html">Santander Dataset, Part II, Feature Selection</a></p>
  </li>
  <li>
    <p><a href="/blog/python/data%20science/2020/07/01/santander1.html">Santander Dataset, Part I</a></p>
  </li>
</ul>

<h2 id="introduction">
<a class="anchor" href="#introduction" aria-hidden="true"><span class="octicon octicon-link"></span></a>Introduction</h2>
<p>I did some hyper-parameter optimisations, but there was nothing particularly noteworthy. I got some incremental improvements using GridSearch and that’s about it. After that, I had a look at what other people on Kaggle did and see what I can learn from them.</p>

<h2 id="read-the-instructions">
<a class="anchor" href="#read-the-instructions" aria-hidden="true"><span class="octicon octicon-link"></span></a>Read the instructions!</h2>
<p>For some reason, I thought that the metric for this contest was accuracy? I suspect it is because I had read this line from the instructions: “For each Id in the test set, you must make a binary prediction of the target variable.” However, other people were submitting probabilities, not just binary predictions. So, I tried submitting the raw probabilities from my model, and the score jumped up from around 0.77 to 0.86! Something is a amiss. Have I misunderstood accuracy.  I re-read the instructions and find that I somehow missed this line: “Submissions are evaluated on area under the ROC curve between the predicted probability and the observed target.” That makes more sense. But if they are using AUROC, then why do they ask for a binary prediction?!</p>

<h2 id="eda">
<a class="anchor" href="#eda" aria-hidden="true"><span class="octicon octicon-link"></span></a>EDA</h2>
<p>This is something that I should have done, having noticed other people doing it for the <a href="/blog/blog/python/data%20science/2020/06/25/creditcard6.html">credit card dataset</a>, but forgot to do. I will make sure to do this for next time! (I did very minimal exploration, but there is clearly more I could do).</p>

<h2 id="two-other-algorithms">
<a class="anchor" href="#two-other-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Two other algorithms</h2>
<p>There are two more algorithms to add to my toolbox: Naive Bayes and LightGBM.  Naive Bayes is intuitive and I am surprised I have not encountered it already. LightGBM seems to be a faster alternative to XGBoost, and this seems to be the most popular algorithm used in this challenge.</p>

<h2 id="creating-a-model-for-each-feature-separately">
<a class="anchor" href="#creating-a-model-for-each-feature-separately" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating a model for each feature separately</h2>
<p>This idea seems to be first described in this <a href="https://www.kaggle.com/ymatioun/santander-model-one-feature-at-a-time">example</a> and <a href="https://www.kaggle.com/cdeotte/modified-naive-bayes-santander-0-899">here</a>. The intuition for why this works is that the features are seemingly independent, so you can combine the predictions made from considering each feature, one at a time. I can’t remember which example it is, but somebody else calculated all the pairwise correlations between features and found they were all very small.</p>

<h2 id="using-frequency-as-new-feature">
<a class="anchor" href="#using-frequency-as-new-feature" aria-hidden="true"><span class="octicon octicon-link"></span></a>Using frequency as new feature</h2>
<p>This idea was referred to as the magic feature (e.g. <a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920">here</a>): for each feature ‘F’, add a new feature ‘F_freq’ which is the frequency of the first feature ‘F’. It is not intuitive to me why this should improve the performance of the model.</p>

<h2 id="visualising-a-tree-based-algorithm">
<a class="anchor" href="#visualising-a-tree-based-algorithm" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualising a tree-based algorithm</h2>
<p>In this <a href="https://www.kaggle.com/cdeotte/200-magical-models-santander-0-920">example</a>, there are excellent visuals demonstrating how LightGBM is making its predictions. In particular, there are excellent visuals that demonstrate how including the magic frequency feature improves the models.</p>

<h2 id="a-remarkable-discovery">
<a class="anchor" href="#a-remarkable-discovery" aria-hidden="true"><span class="octicon octicon-link"></span></a>A remarkable discovery</h2>
<p><a href="https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split">YaG320</a> made an incredible discovery: by looking at the frequencies that different values occur, they concluded that there must be synethic data and then separated out the real data from the synthetic. The reasoning was clever: by noticing that there were fewer unique values in the test data than in the training data, they suspected that the test data started out as some real data and then augmented with some synthetic data. Furthermore, the synthetic data will only use values that occured in the real data. To sniff out the synthetic data, you have to ask yourself: are any of its feature values unique? If yes, then it cannot be synthetic (as synthetic only uses values that already occured in the real data), and if not, then it is very likely synthetic (not certain but very likely). YaG320 wrote code to implement this idea, and found that exactly half the code was real and half was synthetic. Impressive detective work! By removing the fake synthetic data from the construction of their models, people were able to improve their models.</p>

<p>It is unlikely this exact idea will be useful for me in any future projects I will do. However, it highlights the power and thrill of data science. By looking at a bunch of numbers (and with a healthy dose of creativity) one can make deductions that would otherwise be completely hidden.</p>

<h2 id="conclusion">
<a class="anchor" href="#conclusion" aria-hidden="true"><span class="octicon octicon-link"></span></a>Conclusion</h2>
<p>There is a lot for me still to learn! Trying to analyse these Kaggle datasets and then comparing my approach to others seems to be an excellent way to learn and I will be sure to continue it. However, I need to practice some data cleaning/data scraping, so I will start some projects in that vain soon.</p>

  </div><a class="u-url" href="/blog/python/data%20science/2020/07/18/santander3.html" hidden></a>
</article>