<article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Investigating Credit Card Fraud, Part VI, Summary and Lessons from Kaggle</h1><p class="page-description">I end this project by summarising what I did and summarising what I learnt by having a look at other people's examples on Kaggle.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-06-25T00:00:00-05:00" itemprop="datePublished">
        Jun 25, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      7 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#python">python</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#data science">data science</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#other-posts-in-series">Other posts in series</a></li>
<li class="toc-entry toc-h2"><a href="#summaries">Summaries</a>
<ul>
<li class="toc-entry toc-h3"><a href="#part-i">Part I</a></li>
<li class="toc-entry toc-h3"><a href="#part-ii">Part II</a></li>
<li class="toc-entry toc-h3"><a href="#part-iii">Part III</a></li>
<li class="toc-entry toc-h3"><a href="#part-iv">Part IV</a></li>
<li class="toc-entry toc-h3"><a href="#part-v">Part V</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#lessons-learnt-from-kaggle">Lessons learnt from Kaggle</a>
<ul>
<li class="toc-entry toc-h3"><a href="#auroc-versus-auprc">AUROC versus AUPRC</a></li>
<li class="toc-entry toc-h3"><a href="#under--and-over-sampling">Under- and over-sampling</a></li>
<li class="toc-entry toc-h3"><a href="#removing-anomalous-data">Removing anomalous data</a></li>
<li class="toc-entry toc-h3"><a href="#dimensionality-reduction-and-clustering">Dimensionality reduction and clustering</a></li>
<li class="toc-entry toc-h3"><a href="#normalising-data">Normalising data</a></li>
<li class="toc-entry toc-h3"><a href="#outlier-detection-algorithms">Outlier detection algorithms</a></li>
<li class="toc-entry toc-h3"><a href="#auto-encoders-and-latent-representation">Auto-encoders and latent representation</a></li>
<li class="toc-entry toc-h3"><a href="#visualising-the-features">Visualising the features</a></li>
<li class="toc-entry toc-h3"><a href="#gbm-vs-xgboost-vs-lightgbm">GBM vs xgboost vs lightGBM</a></li>
</ul>
</li>
</ul><h2 id="other-posts-in-series">
<a class="anchor" href="#other-posts-in-series" aria-hidden="true"><span class="octicon octicon-link"></span></a>Other posts in series</h2>

<ul>
  <li>
    <p><a href="/blog/python/data%20science/2020/05/30/creditcard5.html">Investigating Credit Card Fraud, Part V, Final Models</a></p>
  </li>
  <li>
    <p><a href="/blog/python/data%20science/2020/05/29/creditcard4.html">Investigating Credit Card Fraud, Part IV, <code class="highlighter-rouge">n_estimators</code></a></p>
  </li>
  <li>
    <p><a href="/blog/python/data%20science/2020/05/19/creditcard3.html">Investigating Credit Card Fraud, Part III, Handmade Model</a></p>
  </li>
  <li>
    <p><a href="/blog/python/data%20science/2020/05/16/creditcard2.html">Investigating Credit Card Fraud, Part II, Removing data</a></p>
  </li>
  <li>
    <p><a href="/blog/python/data%20science/2020/05/14/creditcard1.html">Investigating Credit Card Fraud, Part I, First Models</a></p>
  </li>
</ul>

<h2 id="summaries">
<a class="anchor" href="#summaries" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summaries</h2>
<h3 id="part-i">
<a class="anchor" href="#part-i" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part I</h3>
<p>In Part I, I described the framework and created the first set of models using default settings. I tried logistic regression, decision tree, random forest and xgboost models, and they respectively achieved an AUPRC of 0.616, 0.746, 0.842 and 0.856. Since then, I have learnt about more models and if I were to do this project again, I would also have included a support vector machine model and a k-nearest-neighbour model.</p>

<p><img src="/blog/images/creditcard_1_logistic.png" alt="">
<img src="/blog/images/creditcard_1_tree.png" alt="">
<img src="/blog/images/creditcard_1_forest.png" alt="">
<img src="/blog/images/creditcard_1_xgb.png" alt=""></p>

<h3 id="part-ii">
<a class="anchor" href="#part-ii" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part II</h3>
<p>In Part I, it look some time to fit the models, the forest model in particular, and I wanted to do some hyper-parameter optimisations. I wanted to find out if I could reduce the time taken to fit by removing non-fraudulent claims. The results of the experimentation showed that the time to fit was proportional to the size of the training data set, but the AUPRC did not take a massive hit.  This is good because it means I can do more hyper-parameter optimisations than before.</p>

<p><img src="/blog/images/creditcard_2_forest_aucs.png" alt="">
<img src="/blog/images/creditcard_2_forest_times.png" alt=""></p>

<h3 id="part-iii">
<a class="anchor" href="#part-iii" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part III</h3>
<p>Though the models were able to identify fraudulent transactions, I had gained no understanding. I tried creating a simple model: for each feature, determine whether the value is closer to the fraudulent mean or the non-fraudulent mean. This achieved an AUPRC of 0.682 and was able to identify about 70% of the frauduluent claims. This was satisfying, and better lets me appreciate what is gained by using more sophisticatd models.</p>

<p><img src="/blog/images/creditcard_3_2.png" alt=""></p>

<h3 id="part-iv">
<a class="anchor" href="#part-iv" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part IV</h3>
<p>I started doing some hyper-parameter optimisations on the forest model, and noticed the AUPRC varied a lot between the different folds. I decided to investigate how the AUPRC can vary, to better appreciate what is gained by choosing one hyper-parameter over another. After doing this, I could confidently say that choosing 50 estimators is better than the default of 100 estimators.</p>

<p><img src="/blog/images/creditcard_4_forest_n_est50_hist.png" alt="">
<img src="/blog/images/creditcard_4_forest_n_est50_scatter.png" alt=""></p>

<h3 id="part-v">
<a class="anchor" href="#part-v" aria-hidden="true"><span class="octicon octicon-link"></span></a>Part V</h3>
<p>Here I actually carry out the hyper-parameter optimisations, and train the final models. The random forest’s AUPRC increased from 0.842 to 0.852, and the xgboost’s AUPRC increased from 0.856 to 0.872. Modest gains, and from the few articles I have read, this is to be expected.</p>

<p><img src="/blog/images/creditcard_5_forest.png" alt="">
<img src="/blog/images/creditcard_5_xgb2.png" alt=""></p>

<h2 id="lessons-learnt-from-kaggle">
<a class="anchor" href="#lessons-learnt-from-kaggle" aria-hidden="true"><span class="octicon octicon-link"></span></a>Lessons learnt from Kaggle</h2>
<p>I had a skim through the several most up-voted kernels on Kaggle. Below are the the things I found out by doing so. There is a lot for me to learn!</p>

<h3 id="auroc-versus-auprc">
<a class="anchor" href="#auroc-versus-auprc" aria-hidden="true"><span class="octicon octicon-link"></span></a>AUROC versus AUPRC</h3>
<p>Many of the examples (including the most upvoted <a href="https://www.kaggle.com/janiobachmann/credit-fraud-dealing-with-imbalanced-datasets/notebook">example</a>!) use AUROC instead of AUPRC. The main reason this surprised me is that the description of the dataset recommended using AUPRC; I suppose there was an advantage to not knowing much before hand! The second reason this surprised me is that AUPRC is a more informative measure than AUROC for unbalanced data. I try to explain why.</p>

<p>The PRC and ROC are quite similar. They are both plots that visualise false positives against false negatives.</p>
<ul>
  <li>False negatives are measured in the same way in both plots, namely, using recall/true positive rate. Recall tells you what percentage of truly fraudulent transactions the model successfully labels as fraudulent. (And so 1 - Recall measures how many false negatives we have, as a percentage of truly fraudulent claims.)</li>
  <li>False positive are recorded differently in the two plots.
    <ul>
      <li>In PRC, precision is used. This is the percentage of transactions labelled as fraudulent that actually are fraudulent. Equivalently, 1-PRC is the number of false positives expressed as a percentage of <em>claims labelled as fraudulent</em>.</li>
      <li>In ROC, the false-positive rate is used. This is the number of false positives expressed as a percentage of <em>truly non-fraudulent transactions</em>.</li>
    </ul>
  </li>
</ul>

<p>To make this more concrete, lets put some numbers to this:</p>
<ul>
  <li>Imagine there are 100100 transactions altogether, 100 which are fraudulent and 100000 which are not.</li>
  <li>Suppose a model predicts there are 200 fraudulent claims, and further suppose 50 of these were correct and 150 of these were incorrect.</li>
  <li>For both PRC and ROC, the true positive measurement would be 50%: 50% of the fraudulent claims were found.</li>
  <li>For PRC, the false positive measurement is 75%: 75% of the claims labelled as fraudulent were incorrectly labelled.</li>
  <li>For ROC, the false positive measurement is 0.15%: only 0.15% of the non-fraudulent claims were incorrectly labelled as fraudulent.</li>
</ul>

<p>In short, ROC is much more forgiving of false positives than PRC, when we have highly unbalanced data.</p>

<p>I have also decided to plot PRC and ROC for a couple of the models in this series of posts, so you can visually see the difference. (Note that I have rotated the ROC curve to match up the variables with the PRC curve, so the comparison is easier.)</p>

<p><strong>PRC and ROC for the final XGBoost model</strong>
<img src="/blog/images/creditcard_5_xgb2.png" alt="">
<img src="/blog/images/creditcard_6_xgb_roc2.png" alt=""></p>

<p>ROC makes the model look much better than PRC does. And it is deceiving: one might look at that second chart and say we can identify 90% of fraudulent claims without many false positives.</p>

<p><strong>PRC and ROC for the handmade model</strong>
<img src="/blog/images/creditcard_3_2.png" alt="">
<img src="/blog/images/creditcard_6_handmade.png" alt=""></p>

<p>Here, the effect is far more dramatic and very clearly shows how unfit AUROC is for unbalanced ata.</p>

<h3 id="under--and-over-sampling">
<a class="anchor" href="#under--and-over-sampling" aria-hidden="true"><span class="octicon octicon-link"></span></a>Under- and over-sampling</h3>
<p>It turns out my idea from Part III, to remove non-fraudulent data, has a name: under-sampling. However, it sounds like there is an expectation that under-sampling could actually improve the performance of the models. This is surprising to me; unless you are systematially removing unrepresenative data, how can the model improve with less information?! A quick skim of the wikipedia article suggests I have not completely missed the point: ‘the reasons to use undersampling are mainly practical and related to resource costs’.</p>

<p>Over-sampling looks like an interesting idea, in which you create new artificial data to pad out the under-represented class. Some people on Kaggle used SMOTE, where you take two nearby points, and introduce new points directly in between these two points. Something to keep in mind for future!</p>

<h3 id="removing-anomalous-data">
<a class="anchor" href="#removing-anomalous-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Removing anomalous data</h3>
<p>A simple idea: try to find entries in the training data that are not representative and remove them to avoid skewing the models / to avoid over-fitting. Based on my limited understanding, I think tree-based models are not sensitive to extreme data (in the same way the median is not sensitive to extreme data), so this particular idea is unlikely to have helped me improve the models for this example. However, this is another tool I will keep in mind for future projects.</p>

<h3 id="dimensionality-reduction-and-clustering">
<a class="anchor" href="#dimensionality-reduction-and-clustering" aria-hidden="true"><span class="octicon octicon-link"></span></a>Dimensionality reduction and clustering</h3>
<p>An interesting idea: try to find a mapping of the data into a smaller dimension that preserves the clusters. The algorithm somebody used was t-SNE which is explained in this <a href="https://www.youtube.com/watch?v=NEaUSP4YerM">YouTube video</a>. A couple of other algorithms used were PCA and truncated SVD.  I do not yet understand how I could use this to improve the models (in the example, this was done to give a visual indication of whether frauduluent and non-frauduluent data could be distinguished).</p>

<h3 id="normalising-data">
<a class="anchor" href="#normalising-data" aria-hidden="true"><span class="octicon octicon-link"></span></a>Normalising data</h3>
<p>Useful idea I should always keep in mind! Again, I don’t think this matters for tree-based models, but something I should keep in mind.</p>

<h3 id="outlier-detection-algorithms">
<a class="anchor" href="#outlier-detection-algorithms" aria-hidden="true"><span class="octicon octicon-link"></span></a>Outlier detection algorithms</h3>
<p><a href="https://www.kaggle.com/pavansanagapati/anomaly-detection-credit-card-fraud-analysis/notebook">One person</a> used a bunch of (unsupervised?) learning algorithms: isolation forests, local outlier factor algorithm, SVM-based algorithms. More things for me to learn about!</p>

<h3 id="auto-encoders-and-latent-representation">
<a class="anchor" href="#auto-encoders-and-latent-representation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Auto-encoders and latent representation</h3>
<p><a href="https://www.kaggle.com/shivamb/semi-supervised-classification-using-autoencoders">This person</a> used ‘semi-supervised learning’ via auto-encoders. This was particularly interesting, especially because they had a visual showing how their auto-encoder was better at separating fraudulent and non-fraudulent data than t-SNE. This is definitely something for me to delve deeper into some time, especially because of how visually striking it is.</p>

<h3 id="visualising-the-features">
<a class="anchor" href="#visualising-the-features" aria-hidden="true"><span class="octicon octicon-link"></span></a>Visualising the features</h3>
<p><a href="https://www.kaggle.com/currie32/predicting-fraud-with-tensorflow/notebook">Here</a> and <a href="https://www.kaggle.com/shelars1985/anomaly-detection-using-gaussian-distribution/notebook">here</a> are examples of a nice way of visualising the range of values of each feature for frauduluent and non-frauduluent data. The key thing is that they normalised the histograms, but I am not sure how they did that. Something for me to learn!</p>

<h3 id="gbm-vs-xgboost-vs-lightgbm">
<a class="anchor" href="#gbm-vs-xgboost-vs-lightgbm" aria-hidden="true"><span class="octicon octicon-link"></span></a>GBM vs xgboost vs lightGBM</h3>
<p><a href="https://www.kaggle.com/nschneider/gbm-vs-xgboost-vs-lightgbm/notebook">This kernel</a> compared three algorithms. I quite liked this because it felt historical, and helps me appreciate how the community learns. The person compared the accuracy and time taken for each of the algorithms, and also describes some new settings and options they recently discovered.</p>


  </div><a class="u-url" href="/blog/python/data%20science/2020/06/25/creditcard6.html" hidden></a>
</article>